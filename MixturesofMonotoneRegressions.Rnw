\documentclass[fleqn,10pt]{olplainarticle}
% Use option lineno for line numbers 
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{subfig}
\usepackage{caption}
\usepackage{bm}
\usepackage[toc,page]{appendix}
\usepackage[nogin]{Sweave}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

% import libraries, code, and data
<<import, echo=FALSE, results='hide', message=FALSE, warning=FALSE>>=
library(lattice)
library(flexmix)
library(fdrtool)
library(dplyr)
library(data.table)
library(readr)
library(rnaturalearth)
library(ggnewscale)
library(gtable)
library(ggplot2)
library(RColorBrewer)
library(grid)
library(ggpubr)
library(gganimate)
library(gifski)
library(av)
library(rbenchmark)

# import monotone_mixture code
source("../monotone_mixture/monotone_driver/part_fit.R")
source("../monotone_mixture/monotone_driver/M_driver.R")
source("../monotone_mixture/pseudo_data/data_generator.R")
source("../monotone_mixture/monotone_driver/flex_wrapper.R")

# import GDP/Life expectancy data

lifex <- read.csv("../monotone_mixture/monotone_driver/API_SP.DYN.LE00.IN_DS2_en_csv_v2_1926713.csv", skip = 3)
continent <- read.csv("../monotone_mixture/monotone_driver/Metadata_Country_API_NY.GDP.MKTP.CD_DS2_en_csv_v2_1994746.csv")
gdp2 <- read.csv("../monotone_mixture/monotone_driver/API_NY.GDP.PCAP.CD_DS2_en_csv_v2_1926744.csv", skip=3)
  


lifex$Country.Name <- as.character(lifex$Country.Name)
lifex$Country.Name[which(lifex$Country.Name %in% c(
"Antigua and Barbuda",
"Bahamas, The" ,
"Bosnia and Herzegovina", 
"Brunei Darussalam" ,
"Cabo Verde"  ,
"Cayman Islands", 
"Central African Republic" ,
"Congo, Dem. Rep.", 
"Congo, Rep." ,
"Cote d'Ivoire", 
"Curacao" ,
"Czech Republic", 
"Dominican Republic", 
"Egypt, Arab Rep." ,
"Equatorial Guinea" ,
"Eswatini" ,
"Faroe Islands", 
"French Polynesia", 
"Gambia, The" ,
"Hong Kong SAR, China",
"Iran, Islamic Rep.",
"Korea, Rep." ,
"Kyrgyz Republic", 
"Macao SAR, China", 
"Marshall Islands" ,
"Micronesia, Fed. Sts.", 
"Russian Federation" ,
"Sao Tome and Principe",  
"Sint Maarten (Dutch part)", 
"Slovak Republic" ,
"Solomon Islands" ,
"South Sudan",
"St. Lucia" ,
"St. Vincent and the Grenadines", 
"Syrian Arab Republic",
"Venezuela, RB" ,
"Virgin Islands (U.S.)",
"Yemen, Rep." 
))] <- c( "Antigua and Barb.",
 "Bahamas",
 "Bosnia and Herz." ,
 "Brunei" ,
 "Cape Verde", 
 "Cayman Is." ,
 "Central African Rep.", 
 "Dem. Rep. Congo",
 "Congo",
 "Côte d'Ivoire",
 "Curaçao",
 "Czech Rep.",
 "Dominican Rep.",
"Egypt",
 "Eq. Guinea",
"Swaziland",
"Faeroe Is.",
 "Fr. Polynesia",
"Gambia",
 "Hong Kong",
 "Iran",
 "Korea",
 "Kyrgyzstan",
 "Macao",
 "Marshall Is.",
"Micronesia",
 "Russia",
 "São Tomé and Principe" ,
 "Sint Maarten",
 "Slovakia",
 "Solomon Is.",
 "S. Sudan",
 "Saint Lucia",
 "St. Vin. and Gren.",
 "Syria",
"Venezuela",
 "U.S. Virgin Is.",
 "Yemen"
 )
lifex$Country.Name <- as.factor(lifex$Country.Name)




gdp2$Country.Name <- as.character(gdp2$Country.Name)
gdp2$Country.Name[which(gdp2$Country.Name %in% c(
"Antigua and Barbuda",
"Bahamas, The" ,
"Bosnia and Herzegovina", 
"Brunei Darussalam" ,
"Cabo Verde"  ,
"Cayman Islands", 
"Central African Republic" ,
"Congo, Dem. Rep.", 
"Congo, Rep." ,
"Cote d'Ivoire", 
"Curacao" ,
"Czech Republic", 
"Dominican Republic", 
"Egypt, Arab Rep." ,
"Equatorial Guinea" ,
"Eswatini" ,
"Faroe Islands", 
"French Polynesia", 
"Gambia, The" ,
"Hong Kong SAR, China",
"Iran, Islamic Rep.",
"Korea, Rep." ,
"Kyrgyz Republic", 
"Macao SAR, China", 
"Marshall Islands" ,
"Micronesia, Fed. Sts.", 
"Russian Federation" ,
"Sao Tome and Principe",  
"Sint Maarten (Dutch part)", 
"Slovak Republic" ,
"Solomon Islands" ,
"South Sudan",
"St. Lucia" ,
"St. Vincent and the Grenadines", 
"Syrian Arab Republic",
"Venezuela, RB" ,
"Virgin Islands (U.S.)",
"Yemen, Rep." 
))] <- c( "Antigua and Barb.",
 "Bahamas",
 "Bosnia and Herz." ,
 "Brunei" ,
 "Cape Verde", 
 "Cayman Is." ,
 "Central African Rep.", 
 "Dem. Rep. Congo",
 "Congo",
 "Côte d'Ivoire",
 "Curaçao",
 "Czech Rep.",
 "Dominican Rep.",
"Egypt",
 "Eq. Guinea",
"Swaziland",
"Faeroe Is.",
 "Fr. Polynesia",
"Gambia",
 "Hong Kong",
 "Iran",
 "Korea",
 "Kyrgyzstan",
 "Macao",
 "Marshall Is.",
"Micronesia",
 "Russia",
 "São Tomé and Principe" ,
 "Sint Maarten",
 "Slovakia",
 "Solomon Is.",
 "S. Sudan",
 "Saint Lucia",
 "St. Vin. and Gren.",
 "Syria",
"Venezuela",
 "U.S. Virgin Is.",
 "Yemen"
 )
gdp2$Country.Name <- as.factor(gdp2$Country.Name)
@
% data cleaning
<<clean, echo=FALSE, results='hide', message=FALSE, warning=FALSE>>=

# clean lifex csv
lifex <- lifex[,c(-3, -4, -64, -65, -66)]
names(lifex)[3:61] <- substring(names(lifex)[3:61],2,5)
lifex <- melt(setDT(lifex), id.vars = 1:2, variable.name = "Year")
lifex <- lifex[complete.cases(lifex),]
lifex$Year <- as.integer(as.character(lifex$Year))
names(lifex)[4] <- "LifeExpectancy"
# lifex <- lifex[,-2]

# clean gdp2 csv
gdp2 <- gdp2[,c(-2,-3, -4, -64, -65, -66)]
names(gdp2)[2:60] <- substring(names(gdp2)[2:60],2,5)
gdp2 <- melt(setDT(gdp2), id.vars = 1, variable.name = "Year")
gdp2 <- gdp2[complete.cases(gdp2),]
gdp2$Year <- as.integer(as.character(gdp2$Year))
names(gdp2)[3] <- "GDP"


# merge
le <- merge(lifex, gdp2, by.x = c("Country.Name", "Year"), by.y = c("Country.Name", "Year"))

continent <- merge(le, continent, by.x = c("Country.Code"), by.y = c("Country.Code"))

le <- le[,-3] # remove country code
@

% set argmin and argmax as math operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\title{Mixtures of Partially Linear Models with Monotone Shape Constraints}

\author[1]{Daniel Leibovitz}
\author[2]{Matthias Loffler}
\affil[1]{daniel.leibovitz@uzh.ch}
\affil[2]{matthias.loeffler@stat.math.ethz.ch}

\keywords{Mixture Models, Shape Constraints, Isotonic Regression}

\begin{abstract}
Mixtures of non-parametric monotone regressions are readily applicable to clustering problems where there is prior knowledge about appropriate shape constraints within the resulting model. For example, utility functions in economics, risk-exposure relationships in epidemiology, are both a priori monotonic. The current standard for estimating such models involves fitting a series of non-parametric regression functions without shape constraints using an EM algorithm, as described by Zhang et al. (\cite{zhangetal}), followed by a monotonic estimate given the resulting latent variable classifications. In this paper, we propose to remove redundancy by incorporating the non-parametric monotone regression function estimate into the M-step of the EM algorithm. We demonstrate the effectiveness of the algorithm when applied to both simulated data and real-world data on global life expectancy and GDP from the World Bank.
\end{abstract}

\begin{document}


\flushbottom
\maketitle
\thispagestyle{empty}

\section{Introduction}

The mixture of partially linear regressions with monotone shape constraints takes the following form:

\begin{equation} \label{modstrucbrief}
  Y = 
  \begin{dcases}
    \sum_{h=1}^{p} g_{h1} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{j1} X_{j} \ +\ \epsilon_1, \text{  with probability $\pi_1$; }\\
    \vdots \\
    \sum_{h=1}^{p} g_{hk} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{jk} X_{j} \ +\ \epsilon_k, \text{  with probability $\pi_k$; } \\
  \end{dcases}
\end{equation}

where the model has $K$ components, $\boldsymbol{X} \in \mathbb{R}^p$, $\boldsymbol{W} \in \mathbb{R}^p$, $\beta \in \mathbb{R}^p$, and each function $g_{hk}()$ is assumed monotone. The error $\epsilon$ is normally distributed with mean $0$, and is independent of the covariates $(\boldsymbol{X}, \boldsymbol{Z})$. Such a model has broad applications for clustering of data in domains where monotone relationships are known \emph{a priori}. These domains include, for example, epidemiology, where risk-exposure relationships may be modeled monotonically (\cite{morton}, \cite{carcinogen}); economics, where FILL (CITE); biomedical research, where biochemical kinetics may be monotone over time (\cite{kinetics}). 


A similar type of model has previously been described by Zhang et al. (\cite{zhangetal}), and takes the following form:

\begin{equation} \label{zhangstruc}
  Y = 
  \begin{dcases}
    \sum_{k=1}^{K}\pi_k(\sum_{h=1}^{p} g_{hk} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{jk} X_{j} \ +\ \epsilon_k) \\
  \end{dcases}
\end{equation}


We have identified three drawbacks of in the model and estimator of Zhang et al.:

\begin{enumerate}[noitemsep] 
  \item The model proposed by Zhang et al. cannot be generalized to a semiparametric approach, i.e., one cannot include linear effects in the mixture components. Being able to include linear effects in the mixture components is conducive to two distinct purposes: 
    \begin{enumerate}[noitemsep]
      \item First, it can be used when the data to be clustered has multiple independent variables that are not of primary interest, but which the user would still like to include in the model. As Zhang et al. point out, including such varibles with nonparametric effects can explode the complexity of the algorithm beyond usability (see Section \ref{seccomplex}). Including such variables as linear effects is a safe alternative that keeps the complexity of the algorithm tractable.
      \item Second, users who are mainly concerned with linear effects of components within a mixture model can flexibly control for one or more nuisance variables that are suspected of being monotone in effect.
    \end{enumerate}
  \item The Zhang et al. approach fits mixture components in two, sequential steps, first fitting an unconstrained function and then applying monotone constraints once the components membership has been calculated. This approach introduces a potential bias when components are not clearly identifiable.
  \item The Zhang et al. approach requires a tuning parameter for the fitting of each unconstrained mixture component function, which adds computational complexity.
\end{enumerate}

We propose model \ref{modstrucbrief} as an alternative, more generalized form of the approach suggested by Zhang et al. that avoids the drawbacks mentioned above. Specifically, our model accepts any number of non-parametric monotone or linear effects within each mixture component; it fits the monotone functions within each component in a single step; and it does not require the calibration of any tuning parameters.

The main weakness of our generalized model is that the monotone component of each regression is not a smooth function, as is the case in the approach by Zhang et al., but rather a step-wise function. However, if the user does not require smooth monotone functions, the advantages of the algorithm we propose may outweigh this cost.

The remainder of this paper is organized as follows. In Section 2, we discuss previous research in the domains of mixture models, partial linear models, and isotonic regression. In Section 3, we discuss the components of the proposed model. In Section 4, we describe the theoretical structure of the proposed model (model \ref{modstrucbrief}) as well as the estimation algorithm, followed by the model's asymptotic properties and empirical complexity. In Section 5, we apply the proposed model to simulated data as well as World Bank data on global life expectancy through the end of the 20\textsuperscript{th} century. In the final section, we discuss implications and future work.


\section{Previous Work}

The model proposed in this article draws from several branches of statistical research. In this section, we briefly discuss the history and current state of said research.

\subsection{Regression with shape constraints}

always non-parametric. monotone, concave, convex,
discuss history, estimation methods (splines, inversion,), variance bounds.
discuss monotone regressions in particular. different approaches -- pava, active set, etc.
The addition of a monotonicity shape constraint to any of the models mentioned thus far complicates their estimation, and indeed much has been written on the estimation of monotone nonparametric functions alone. As Zhang et al. do in their mixture model, most approaches to monotonic nonparametric estimation apply a two step algorithm, either estimating a smooth function and applying a monotonic constraint on the resulting function (e.g., Friedman \& Tibshirani (\cite{friedman})), or estimating a monotonic function and then smoothing the resulting estimate (e.g., Cheng \& Lin (\cite{cheng})). Mammen compares the consistency of these approaches in a comprehensive treatment of smooth, monotonic nonparametric regression. Although these algorithms differ in their asymptotic behaviours, all of them require the selection of a tuning parameter (\cite{mammen}).

Monotone regressions alone have seen very diverse applications across research domains. They have been used to estimate disease risk as functions of spatial exposure (CITE), etc. etc.



\subsection{partial linear models}

discuss GAMs

\subsection{Mixture Models}

Model-based clustering models, or mixture models, are a common model for producing clusters with probabalistic or "soft" cluster assignment. 
also talk about inclusion/exclusion constraints.
discuss asymptotic MLE.

Finite mixture models date from the 19\textsuperscript{th} century (CITE), while the more commonly known implementation via the EM algorithm was first introduced in 1977 (Dempster, Laird, Rubin).
Ifinite mixture models: \cite{infinte}

\subsection{Mixtures of Regressions}

From the larger set of mixture models, mixtures of regressions play a useful role in cases where we want to model a dependence structure amongst observed covariates without modelling the distributions of the covariates themselves. 
allows the analysis of such data where the researcher suspects latent categories among the observations. 


\subsection{Mixtures of Nonparametric Regressions}

Both Xiang \& Yao (CITE), and Huang et al. (CITE) have discussed mixtures of nonparametric regressions. The model of Xiang \& Yao estimates mixing proportions and the variance of each component as constants, while allowing the mean of each component to be a nonparametric function of the data. Huang et al., by contrast, propose a model where mixing proportions, mean and variance within each mixture component are all estimated nonparametrically.

Various attempts have been made at generalizing the above approaches to include linear effects, i.e., to model mixtures of partially linear models (PLMs) or generalized additive models (GAMs). Wu \& Liu (CITE) propose a structure for estimating mixtures of PLMs with a univariate nonparametric effect and arbitrary linear effects per component. Most recently, Zhang \& Pan (CITE) extended this model to accept arbitrary nonparametric effects.



\subsection{Mixture Models with Monotone Shape Constraints}

Within the smaller subset of mixtures of non-parametric regressions, we may frequently encounter situations in which we would like to place shape constraints on some, or each, of the components in our mixture model. A common such shape constraint is the monotonicity constraint, which ensures that in the regression model \( E(Y|X) = f(X) \), the function $f(X)$ is either non-increasing or non-decreasing over the range of $X$.

There has been relatively little previous work in the modelling of mixtures of specifically monotone nonparametric regressions, with the publication by Zhang et al. standing out as the only treatment of this particular issue. There has been, however, much research in adjacent models.


\section{An Overview of Contributing Models and Algorithms}

\subsection{Mixture Models and the EM Algorithm}

\subsubsection{General Mixture Models}

Given some number $n$ of observed values $X_1,...,X_n$ general mixture model has the following structure:

\begin{equation} \label{genmix}
  p(x) \ =\ \sum_{k=1}^{K} \pi_k \cdot p_k(x)
\end{equation}

\begin{equation} \label{genmixlatent}
  p(x) \ =\ \sum_{k=1}^{K} \pi_k \cdot p(x | Z_k)
\end{equation}

\subsubsection{Finite Mixtures of Regressions}

At its most basic, the structure of a finite mixture of regressions model can be described without specifying the exact form of either the regression model $Y = f(\cdot|\vec{X}) + \epsilon$ or the distribution of $E(Y|\vec{X})$. Suppose we observe $Y_i,...,Y_n$ and associated $X_i,...,X_n$. We assume that each observed set $(Y_i, \vec{X_i})$ belongs to one of $\{1,...,k\}$ unobserved components, for some positive integer $k$, and we denote this by a vector of probabilities $Z_i$ of length $k$, such that $\sum_{k=1}^{K}Z_{ik} = 1$.

Without loss of generality, we can assume that each specifying the exact form of the regression model $Y \sim f_k(\cdot|\vec{X})$ for component $k$, we can assume some vector of regression model parameters $\Theta$ and write the likelihood of the mixture model as such:

\begin{equation} \label{mixlik}
  L(\pi, \Theta) \ =\ \prod_{i=1}^n \sum_{j=1}^k \pi_j p_j(y_i\ |\ \vec{x_i},\ \Theta_j)
\end{equation}


where $\pi_j$ is the prior probability of component $j$, and $p_j$ is the density of $f(Y)$ at $y_i$ given observed $x_i$ and $\Theta_j$ for component $j$. When the likelihood is maximized and parameters are estimated, the model provides the following:

\begin{enumerate}[noitemsep] 
  \item An $n \ \times\ k$ matrix $\mathcal{L}$ representing the posterior probability of each $(Y_i, \vec{X_i})$ belonging to each of $K$ components. 
  \item A vector $\pi_1,...,\pi_k$ of prior probabilities representing the mixing proportions of each component in the larger mixture model
  \item A set of parameters $\Theta_k$ for each regression component $k$ 
\end{enumerate}

By extension, the fitted mixture model also allows us to compute the marginal density of $Y$ given $X = x$:

\begin{equation} \label{mixmarg}
  p(Y = y) \ =\ \sum_{k=1}^{K}\pi_k p(Y = y\ |\ X = x, \Theta_k)
\end{equation}

We find the asymptotic global maximum [@emalgo] of the likelihood function via the EM algorithm, which is described in section 3.2 (Link).

\subsubsection{The EM Algorithm}

A finite mixture model can be estimated via the EM algorithm for any vector of distributions that have a calculable likelihood, which of course includes regression models.

\subsection{Partially Linear Models and the Backfitting Algorithm}

\subsubsection{Partially Linear Models}

a generalization of GAMs

\subsubsection{The Backfitting Algorithm}

describe backfitting
mgcv as an alternative to backfitting

\subsubsection{Partially Linear Models with Monotone Constraints}

The generalized partial linear model is an additive regression model with some finite combination of linear and non-linear components, which can be denoted thus:


\begin{equation} \label{partlin}
  Y = \sum_{h=1}^{p} g_{h} (X_{h}) \ +\  \sum_{j=1}^{q} \beta_{j} X_{j} \ +\ \epsilon
\end{equation}


where the model has $h$ non-linear covariates and $j$ linear covariates, where each $g_{h}()$ is some nonparametric function of $X_{h}$, and where $\epsilon \sim Normal(0, \sigma^2)$. The parameters $\vec{\beta}$ and the functions $g_1(\cdot),...g_p(\cdot)$ are determined as the minimizers of the quadratic loss function:

\begin{equation} \label{partlinloss}
  \sum_{i=1}^{n} (y_i - \sum_{h=1}^{p} g_{h} (x_{ih}) \ -\  \sum_{j=1}^{q} \beta_{j} x_{ij})^2
\end{equation}


The estimation of the functions $g_1(\cdot),...g_p(\cdot)$ is a problem of isotonic regression, discussed in the next subsection.

The MLE of the entire partial linear model is obtained via a backfitting algorithm suggested by Cheng [@cheng], and involves sequentially updating the linear and non-linear components of the partial linear model in a two-step process (\ref{backfit1}, \ref{backfit2}) until convergence.

\begin{equation} \label{backfit1}
  (I) \ \ \ \ \ \ \ \ \ \{\hat{g}_1,...,\hat{g}_p\} = \underset{g_1:g_p}{\operatorname{argmin}} \sum_{i=1}^n\left(y_i-\sum_{j=1}^{q} \beta_{j} x_{ij}-\sum_{h=1}^{p} g_{h} (x_{ih})\right) \\
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  holding \ \ \ \vec\beta \ \ \ fixed
\end{equation}


\begin{equation} \label{backfit2}
  (II) \ \ \ \ \ \ \ \ \ \hat{\vec\beta} = \underset{\beta}{\operatorname{argmin}} \sum_{i=1}^n\left(y_i-\sum_{h=1}^{p} g_{h} (x_{ih}) - \sum_{j=1}^{q} \beta_{j} x_{ij}\right) \\
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  holding \ \ \ \{g_1,...,g_p\} \ \ \ fixed
\end{equation}


\subsection{Isotonic Regression}

At the most basic level, with univariate $x$ and $y$ and a simple ordering amongst $x$ such that \( x_{1} \leq x_{2} \leq ... \leq x_{n} \) for all \( x_{i} \in X \), isotonic regression determines a non-decreasing function $g(\cdot)$ such that \( g(x_{1}) \leq g(x_{2}) \leq ... \leq g(x_{n}) \) and for which \( g(\cdot) = \argmin_{g} \sum_{i=1}^{n}(g(x_{i}) - x_{i})^2 \). If observations are weighted, the objective function becomes \( g(\cdot) = \argmin_{g} \sum_{i=1}^{n}w_{i}(g(x_{i}) - x_{i})^2 \) for weights $w$. In the so-called antitonic case, the function $g(\cdot)$ is non-increasing such that \( g(x_{1}) \geq g(x_{2}) \geq ... \geq g(x_{n}) \). Without loss of generality, from this point on we discuss only the non-decreasing case.

The solution can be written out in the form of a min-max formula (\cite{jordan}):

(MIN MAX EQUATION)


One well-known way to estimate $g()$ is through the Pool Adjacent Violators Algorithm (PAVA). The PAVA -- for univariate monotone regression (\ref{pava}) -- returns a step-function fit without either having to select a bandwidth or having to set a congergence tolerance parameter. For multivariable monotone regression (\ref{cpav}), we must take a different approach suggested by Bacchetti and called the Cyclic Pool Adjacent Violators Algorithm (CPAV). Within the CPAV, we iterate through each univariate function sequentially and update univariate monotone functions until convergence, returning a sum of step-functions.



\begin{equation} \label{pava}
  Y = g(X) \ +\ \epsilon
\end{equation}


\begin{equation} \label{cpav}
  Y = \sum_{h=1}^{p} g_{h} (X_{h}) \ +\ \epsilon
\end{equation}


% Compare our algo (Algo II) to previous algo (Algo I) here.
% 
% Main difference between algorithms: 
% Algo I has non-parametric priors and non-parametric normal variances which are both functions of X (pi_c(x) and sig_c(x) for component c).
% Algo I provides smooth monotonic estimates.
% Algo I is not readily generalizable to partially linear models
% Algo I uses CV to estimate tuning parameters for kernel density estimation in both mixture on non parametrics AND monotone estimate.
% Steps of Algo I: 
% 1. mixture of nonparametric regressions (nonparametric estimate? bandwidth/lambda?)
% 2. monotone constraint 

\section{Proposed Model}


\subsection{Model Definition}

The model proposed in this article has the following structure:

\begin{equation} \label{modstruc}
  Y = 
  \begin{dcases}
    \sum_{h=1}^{p} g_{h1} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{j1} X_{j} \ +\ \epsilon_1 \text{  with probability $\pi_1$; } \\
    \vdots \\
    \sum_{h=1}^{p} g_{hk} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{jk} X_{j} \ +\ \epsilon_k \text{  with probability $\pi_k$; }\\
  \end{dcases}
\end{equation}

where $\pi_k$ represents the prior probability of mixture component $k$; $g_{hk}(\cdot)$ represents the monotone function of variable $h$ within component $k$; $\beta_{jk}$ represents the linear effect of variable $j$ within mixture component $k$; and $\epsilon_k$ represents the error associated with component $k$. All $\epsilon_k$ are assumed to be normally distributed with mean $0$, and are assumed to be independent of the covariates $(\boldsymbol{X}, \boldsymbol{Z})$ . All $\pi_k$ are assumed to be unknown constants, and all $\pi_k \in (0,1)$ such that $\sum^{K} \pi_k = 1$.

There is no requirement that the $K$ regression functions in model \ref{modstruc} be identical. Specifically, $g_{hk}()$ for any $h \in p$ and any $k \in K$ can be set as monotone non-increasing, monotone non-decreasing, or absent, regardless of the other $g_{hk}$. Likewise, the linear effects $\beta_{j}$, including the intercept $\beta_0$, need not be same across different components $k$.


\subsubsection{Model-Based Prediction}

In typical mixture model frameworks, the model estimate permits the calculation of unconditioned joint distributions of the data and marginal distributions of any given variable. By extension, such typical models permit the generation of new pseudo-observations that conform with the model structure. The mixture of regressions model does not permit this type of generativity unless the distribution of the covariates is known \emph{a priori}, since the distribution of the covariates is not estimated as a part of the model. 

Similarly, whereas typical mixture models can classify new observations by summing over the product of the mixture prior and the density at the new observed data (\ref{typ_mix}), a mixture of regressions model must classify with a bayesian approach of summing over the likelihood given the observed data (\ref{reg_mix}).

\begin{equation} \label{typ_mix}
  p(Y = y) \ =\ \sum_{k=1}^{K}\pi_k p_k(\vec{Y} = \vec{y}\ | \Theta_k)
\end{equation}

\begin{equation} \label{reg_mix}
  p(Y = y | X = x) \ =\ \sum_{k=1}^{K}\frac{\pi_k p_k(y \ | \ X = x, \Theta_k)}{\int(f(x|theta)f(theta)d\theta)}p_k(Y = y\ |\ X = x, \Theta_k)
\end{equation}

This is simply the application of Bayes' theorem (\ref{bayes}), where the distributions of the components $k$ are multiplied by the posterior given $X = x$ rather than the uninformed prior $\pi_k$. Practically, it is not necessary to calculate the integral in the denominator of the posterior analytically; rather, the sum of the posteriors are normalized to sum to $1$.

\begin{equation} \label{bayes}
  p(\theta | X) \ =\ \frac{p(X|\theta)p(\theta)}{\int_{\theta}p(X|\theta)p(\theta)}
\end{equation}


% It should be noted that the model is NOT generative, that is, there is no distribution assumed for $X$ and therefore no reasonably probabalistic way to determine a marginal distribution of $Y$ without an observed $X$.

\subsection{Model Estimation}

The proposed model is obtained from a series of nested, iterative algorithms, described below. Algorithm 1 describes the EM algorithm for fitting mixture priors and observation posteriors. Algorithm 2 describes the weighted partial linear regression for the fitting of each component within each M-step of the EM algorithm. Algorithm 3 describes the weighted, cyclic pool adjacent violators algorithm for cases where there is more than one monotone function fit within a single partial linear regression. Algorithm 4 describes the weighted pool adjacent violators algorithm for fitting a single monotone regression. In all cases, convergence thresholds are set by the user.

\begin{algorithm}[H]
\SetAlgoLined
\KwData{\;
  $x$ — an \(n \times p\) matrix (independent variables with no shape constraint)\;
  $z$ — an \(n \times q\) matrix (independent variables with monotone shape constraint)\;
  $y$ — an \(n \times 1\) matrix (dependent variable)\;
  $k$ — a positive integer representing the number of categories of latent variable L\;
}
\KwResult{\;
  \( \mathcal{L} \) — an \(n \times k\) matrix representing the posterior probability of observation \(i = 1,...,n\) belonging to latent category \(j = 1,...,k\). Additionally, for all \(i = 1,...,n\) and \(j = 1,...,k\), \( \mathcal{L}_{ij} \) is a real number in the range \([0,1]\), and \( \sum_{j=1}^{k} \mathcal{L}_{ij} = 1 \) \;
  $\vec{\pi}$ — a vector $\pi_1,...,\pi_k$ of prior probabilities representing the mixing proportions of each component in the larger mixture model \;
  $\vec{\Theta}$ — a set of parameters $\Theta_k$ for each regression component $k$ \;
}
Set iteration index $d \leftarrow 1$ \;
 \For{\( i \in 1,...n \)}{
 With uniform probability across $k$, assign one of the elements of \( [\mathcal{L}_{i1}, ..., \mathcal{L}_{ik}] \) to 1 and all other to 0, such that \( \mathcal{L}_{i} = [0,..., 1, ...,0] \) \;}
 \While{algorithm is not converged}{
  % M-step:\;
  \For{\(j \in 1,...,k\)}
  {Set prior mixture proportion \( \displaystyle{\pi_{j}^{(d)} \leftarrow \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}_{ij}^{(d-1)}} \) \;
Set weighted partial linear model regression parameters such that \( \displaystyle{[\hat{\beta}_{j}, \hat{g}_{j}]^{(d)} \leftarrow \argmin_{\beta, g} \sum_{i=1}^{n}\mathcal{L}_{ij}^{(d-1)}(y - x\beta_{j} - g_{j}(z))^{2} } \) (See Algorithm 2, WPLR)\;}  

  % E-step:\;
  \For{\(i \in 1,...,n\)}{
    \For{\(j \in 1,...,k\)}
    {Set \( \mathcal{L}_{ij}^{(d)} \leftarrow \pi_{j}^{(d)}p(y_{i} | x_{i}, \beta_{j}^{(d)}, g_{j}^{(d)}(z_{i})) \), where \(p(y_{i} | x_{i}, \beta_{j}^{(d)}, g_{j}^{(d)}(z_{i}))\) is the density of a $Normal$ distribution with \(\mu = x_{i}\beta_{j}^{(d)} + g_{j}^{(d)}(z_{i})\) and \(\sigma = \sqrt{\frac{\sum w_{i}r^{2}/\bar{w}}{n-rk(X)}}\) }
    % sigma = sqrt(sum(wates * (resids)^2 / mean(wates))/ (nrow(x)-qr(x)$rank))
  Normalize the posterior probabilities \( [\mathcal{L}_{i1},...,\mathcal{L}_{ik}]^{(d)} \) such that \( \displaystyle{\sum_{j=1}^{k}\mathcal{L}_{ij}^{(d)} = 1} \)\;
  }
  
  $d = d + 1$\;
  
 }
 \caption{EM algorithm for Finite Mixtures of Regressions}
\end{algorithm}


\begin{algorithm}[H]
\SetAlgoLined
\KwData{\;
  $x$ — an \(n \times p\) matrix (independent variables with no shape constraint)\;
  $z$ — an \(n \times q\) matrix (independent variables with monotone shape constraint)\;
  $y$ — an \(n \times 1\) matrix (dependent variable)\;
  $w$ — an \(n \times 1\) matrix (observation weights)\;
}
\KwResult{\( \hat{\beta},\hat{g_{1}},...,\hat{g_{q}} \) such that \( \displaystyle{[\hat{\beta},\hat{g_{1}},...,\hat{g_{q}}] = \argmin_{\beta, g_{1},...,g_{q}} \sum_{i=1}^{n} w_{i}(y_{i} - \sum_{h=1}^{q}g_{h}(z_{ih}) - x_{i}\beta)^{2}} \)}

 Set iteration index $b \leftarrow 1$\;
 Set \( \displaystyle{\hat{\beta}^{(0)} \leftarrow \beta_{x}} \), where \( \displaystyle{[\beta_{x}, \beta_{z}] = \argmin_{\beta_{x}, \beta_{z}} \sum_{i=1}^{n} w_{i}(y_{i} - z_{i}\beta_{z} - x_{i}\beta_{x})^{2}}\)\;
 \While{algorithm is not converged}{
  \eIf{$z$ is univariate}
  {Set \( \displaystyle{\hat{g}^{(b)} \leftarrow \argmin_{g} \sum_{i=1}^{n} w_{i}([y_{i} - x_{i}\beta^{(b-1)}] - g(z_{i}))^{2}} \) holding \( \beta^{(b-1)} \) fixed. (See Algorithm 4, PAVA)}
  {Set \( \displaystyle{\sum_{h=1}^{q}\hat{g}_{h}^{(b)} \leftarrow \argmin_{g_{1},...,g_{q}} \sum_{i=1}^{n} w_{i}([y_{i} - x_{i}\beta^{(b-1)}] - \sum_{h=1}^{q}g_{h}(z_{ih}))^{2}} \) holding \( \beta^{(b-1)} \) fixed. (See Algorithm 3, CPAV)}
  Set \( \displaystyle{\hat{\beta}^{(b)} = \argmin_{\beta} \sum_{i=1}^{n} w_{i}([y_{i} - g^{(b)}(z_{i})] - x_{i}\beta)^{2}} \) holding \( g^{(b)} \) fixed.\;
  $b = b + 1$\;
  
 }
 \caption{Weighted Partial Linear Regression}
\end{algorithm}



\begin{algorithm}[H]
\SetAlgoLined
\KwData{\;
  $x$ — an \(n \times q\) matrix (independent variables)\;
  $z$ — an \(n \times 1\) vector (observation weights)\;
  $y$ — an \(n \times 1\) vector (dependent variable)\;
}
\KwResult{A set of non-decreasing functions \(\hat{f_1},...\hat{f_q}\) such that \( \displaystyle{[\hat{f_1},...\hat{f_q}] = \argmin_{f_1,...f_q]} \sum_{i=1}^{n} w_{i}(y_{i} - \sum_{h=1}^{q}f_h(x_{ih}))^2}  \)}

 
 Set iteration index $m \leftarrow 1$ \;
 \While{algorithm is not converged}{
  \For{\(h \in 1,...,q\)}{
  Set \( \displaystyle{\hat{f_h} \leftarrow \argmin_{f_h} \sum_{i=1}^{n} w_{i}([y_{i} - \sum_{\substack{j=1 \\ j\neq h}}^{q}f_j(x_{ih})] - f_h(x_{ih}))^2}  \) holding all $f_j(), j \neq h$ fixed (See Algorithm 4, PAVA) \;
  }
  
  $m = m + 1$\;
  
 }
 \caption{Weighted Cyclic Pool Adjacent Violators Algorithm}
\end{algorithm}




\begin{algorithm}[H]
\SetAlgoLined
\KwData{\;
  $x$ — an \(n \times 1\) vector (independent variable)\;
  $w$ — an \(n \times 1\) vector (observation weights)\;
  $y$ — an \(n \times 1\) vector (dependent variable)\;
}
\KwResult{A non-decreasing function \( \displaystyle{\hat{f}(\cdot) = \argmin_{f} \sum_{i=1}^{n} w_{i}(y_{i} - f(x_{i}))^2}  \)}
 
 Set iteration index $l \leftarrow 0$ \;
 Set blocks $r \leftarrow 1, ..., B$ where at $ l = 0$, $B = n$ \;
 Set $f^{(l=0)}(x_i) \leftarrow y_i$ \;
 Set initial block membership $f^{(l=0)}(x_i) \in r_i$ \;
 \While{any $f_{r}^{l}(x) \geq f_{r+1}^{l}(x)$}{
  \If{$f_{r}^{l}(x) > f_{r+1}^{l}(x)$}
  {Merge blocks $r$ and $r + 1$}
  Solve $f_{r}^{(l)}()$ for block $r$ as the weighted mean, i.e, \(\displaystyle{f_{r}() = \frac{1}{\sum_{i=1}^{n}w_i}\sum_{i=1}^{n} w_i(y_i)} \) for all $x \in r$\;
  
  $l = l + 1$\;
  
 }
 \caption{Weighted Pool Adjacent Violators Algorithm}
\end{algorithm}

%% Original algorithm without separation of component algorithms.
% We are given
% \begin{itemize}
%   \item[]	$x$ — an \(n \times p\) matrix (independent variables with no shape constraint)
%   \item[]	$z$ — an \(n \times q\) matrix (independent variables with monotone shape constraint)
%   \item[]	$y$ — an \(n \times 1\) matrix (dependent variable)
%   \item[]	$k$ — a positive integer representing the number of categories of latent variable L
%   \item[] \( \mathcal{L} \) — an \(n \times k\) matrix representing the posterior probability of observation \(i = 1,...,n\) belonging to latent category \(j = 1,...,k\). Additionally, for all \(i = 1,...,n\) and \(j = 1,...,k\), \( \mathcal{L}_{ij} \) is a real number in the range \([0,1]\), and \( \sum_{j=1}^{k} \mathcal{L}_{ij} = 1 \)
% \end{itemize}
% 
% \begin{enumerate}
%   \item For each \( i \) = 1,...n, set the vector \( \mathcal{L}_{i} \) = \( [\mathcal{L}_{i1},...,\mathcal{L}_{ik}] \) as an instance of a multinomial distribution with k=k and n=1. I.e., randomly assign one of the vector elements of \( [\mathcal{L}_{i1}, ..., \mathcal{L}_{ik}] \) to 1 and all other lik to 0, such that each \( \mathcal{L}_{i} = [0,..., 1, ...,0] \) where 1 is at a random index.
%   \item In each iteration $(d)$ until convergence:
%     \begin{enumerate}
%       \item M-step
%         \begin{enumerate}
%           \item Calculate prior mixture proportions \([p_{1},...,p_{k}]^{(d)}\) such that \( \displaystyle{p_{k}^{(d)} = \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}_{ik}^{(d-1)}} \)
%           \item Calculate weighted partial linear model regression parameters; For each \( j = 1,..., k \), estimate \( \displaystyle{[\hat{\beta}_{j}, \hat{g}_{j}]^{(d)} = \argmin_{\beta, g} \sum_{i=1}^{n}\mathcal{L}_{ij}^{(d-1)}(y - x\beta_{j} - g_{j}(z))^{2} } \) by iterating the following steps until convergence. For each iteration $b$:
%           \begin{enumerate}
%             \item Weighted PAVA/CPAV: If $z$ is univariate, i.e., an $n \times 1$ matrix, use PAVA to estimate \( \displaystyle{\hat{g}_{j}^{(b)} = \argmin_{g} \sum_{i=1}^{n}\mathcal{L}_{ij}^{(d-1)}([y - x\beta_{j}^{(b-1)}] - g_{j}(z))^{2}} \) holding \( \beta_{j}^{(b-1)} \) fixed. If $z$ is multivariate, use CPAV algorithm*.
%             \item Weighted Linear Model: Estimate \( \displaystyle{\hat{\beta}_{j}^{(b)} = \argmin_{\beta} \sum_{i=1}^{n}\mathcal{L}_{ij}^{(d-1)}([y - g_{j}^{(b)}(z)] - x\beta_{j})^{2}} \) holding \( g_{j}^{(b)} \) fixed.
%           \end{enumerate}
%         \end{enumerate}
%       \item E-step
%         \begin{enumerate}
%           \item Given $k$ regression functions \( [E(Y | X; Z) = X\beta_{k} + g_{k}(Z)]^{(d)} \), for \( i = 1,...,n \), calculate \( [\mathcal{L}_{i1},...,\mathcal{L}_{ik}]^{(d)} \) as \( [p_{1}^{(d)}P(y_{i} | x_{i}, \beta_{1}^{(d)}, g_{1}^{(d)}(z_{i})),...,p_{k}^{(d)}P(y_{i} | x_{i}, \beta_{k}^{(d)}, g_{k}^{(d)}(z_{i}))] \)
%           \item For each \( i = 1,...,n \), normalize the posterior probabilities \( [\mathcal{L}_{i1},...,\mathcal{L}_{ik}]^{(d)} \) such that \( \displaystyle{\sum_{j=1}^{k}\mathcal{L}_{ij}^{(d)} = 1} \)
%         \end{enumerate}
%     \end{enumerate}
% \end{enumerate}

\subsection{Asymptotic Properties of Model and Estimator}

Discuss the error estimates of parameters.


\subsection{Confidence Intervals via Bootstrapping}

\subsubsection{Ordinary Bootstrap}

\subsubsection{Conditional Bootstrap}


The obvious thing to do would be to take the bootstrapped parameters and functions as observations, and run yet another mixture model on these parameters to determine a probabalistic and bayesian interpretation of the distributions of parameters. The interpetation of such a hypothetical model would be difficult, and we leave this task for future research.

\subsection{Computational Complexity of Estimator} \label{seccomplex}

A common concern amongst users of this algorithm will be the speed of the estimator, and by extension, the computational complexity of the estimator. It is not possible to specify exact $O(\cdot)$ notation given that the number of iterations within each optimization step of the algorithm is problem-dependent. However, given the generalized nature of the algorithm, we can compare its complexity for different types and numbers of features within the component regression models, and we can support these comparisons with timed applications on pseudo-data.

TIMED COMPLEXITY: here.

<<complexity_benchmarking, eval=FALSE, echo=FALSE>>=

# data with 4 latent categories

################
X <- cbind(
  runif(1000, -5, 5),
  runif(1000, -10, 10),
  runif(1000, -100, 100),
  runif(1000, -100, 100),
  runif(1000, -100, 100)
)
################

# print benchmarks (all excluding intercept)
benchmark(
"Univariate monotone without linear effects" = {
  Y1 <- (X[1:250,1])+3 + rnorm(250, 0, 3) # component 1
  Y2 <- (X[251:500,1])^3 + rnorm(250, 0, 4) # component 2
  Y3 <- 2*((X[501:750,1])+5) + rnorm(250, 0, 3) # component 3
  Y4 <- 2*((X[751:1000,1])-5) + rnorm(250, 0, 4) # component 4
  df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
  names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
  ###
  m1 <- flexmix(Y ~ X1 -1, data = df_3, k = 4, 
                model = mono_reg(mon_inc_names = "X1"))
},
"Bivariate monotone without linear effects" = {
  Y1 <- (X[1:250,1])+3 + 1.5*X[1:250,2] + 
    rnorm(250, 0, 3) # component 1
  Y2 <- (X[251:500,1])^3 + 3*X[251:500,2] + 
    rnorm(250, 0, 4) # component 2
  Y3 <- 2*((X[501:750,1])+5) + 5*X[501:750,2] + 
    rnorm(250, 0, 3) # component 3
  Y4 <- 2*((X[751:1000,1])-5) + 10*X[751:1000,2] + 
    rnorm(250, 0, 4) # component 4
  df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
  names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
  ###
  m2 <- flexmix(Y ~ X1 + X2 -1, data = df_3, k = 4, 
                model = mono_reg(mon_inc_names = c("X1", "X2")))
},
"Univariate monotone with linear effects" = {
  Y1 <- (X[1:250,1])+3 + 1.5*X[1:250,2] - 1.5*X[1:250,3] -
    1*X[1:250,4] + X[1:250,5] + rnorm(250, 0, 3) # component 1
  Y2 <- (X[251:500,1])^3 + 3*X[251:500,2] + 2*X[251:500,3] -
    2*X[251:500,4] + 2*X[251:500,5] + 
    rnorm(250, 0, 4) # component 2
  Y3 <- 2*((X[501:750,1])+5) + 5*X[501:750,2] - 1*X[501:750,3] +
    2*X[501:750,4] + 4*X[501:750,5] + 
    rnorm(250, 0, 3) # component 3
  Y4 <- 2*((X[751:1000,1])-5) + 10*X[751:1000,2] -
    3*X[751:1000,3] - 3*X[751:1000,4] + 3*X[751:1000,5] + 
    rnorm(250, 0, 4) # component 4
  df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
  names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
  ###
  m3 <- flexmix(Y ~ . -1, data = df_3, k = 4, 
                model = mono_reg(mon_inc_names = "X1"))
},
"Bivariate monotone with linear effects" = {
  Y1 <- (X[1:250,1])+3 + 1.5*X[1:250,2] - 1.5*X[1:250,3] -
    1*X[1:250,4] + X[1:250,5] + rnorm(250, 0, 3) # component 1
  Y2 <- (X[251:500,1])^3 + 3*X[251:500,2] + 2*X[251:500,3] -
    2*X[251:500,4] + 2*X[251:500,5] + 
    rnorm(250, 0, 4) # component 2
  Y3 <- 2*((X[501:750,1])+5) + 5*X[501:750,2] - 1*X[501:750,3] +
    2*X[501:750,4] + 4*X[501:750,5] + 
    rnorm(250, 0, 3) # component 3
  Y4 <- 2*((X[751:1000,1])-5) + 10*X[751:1000,2] - 
    3*X[751:1000,3] - 3*X[751:1000,4] + 3*X[751:1000,5] + 
    rnorm(250, 0, 4) # component 4
  df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
  names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
  ###
  m4 <- flexmix(Y ~ . -1, data = df_3, k = 4, 
                model = mono_reg(mon_inc_names = c("X1", "X2")))
},
replications = 3,
columns = c("test", "replications", "elapsed", "relative", "sys.self")
)

@

In the benchmarking table above (LINK), one can see that -- for a model with 4 latent components -- adding a second monotone nonparametric effect within each component multiplies the computation time approximately 50. This is an indication of the heavy cost of increasing even slightly the dimensionality of the non-parametric estimation within each component model.

By comparison, adding linear effects within the component models comes essentially for free. In fact, the estimation of models with univariate monotone effects and 4 linear effects is \emph{faster} than the complementary model without linear effects. 

\section{Model Applications}
\subsection{Simulated Data}

In this section, we demonstrate the application of the proposed model by fitting it to randomly generated pseudo-data. We begin by modeling 1000 observations generated from 2 latent categories with the following underlying structure:

\begin{align*}
  Y_{1} &= 50 + X^3 + \epsilon_1 \\
  Y_{2} &= -50 + 0.04 \cdot X^5 + 30 \cdot X + \epsilon_2 \\
\end{align*}

where 

\begin{align*}
  \epsilon_1 &\sim N(0,30) \\
  \epsilon_2 &\sim N(0,20) \\
\end{align*}

and

\begin{align*}
  \pi_1 &= 0.65 \\
  \pi_2 &= 0.35 \\
\end{align*}

and

\begin{align*}
  X &\sim Uniform(-10,10) \\
\end{align*}


We proceed to estimate a mixture of univariate regressions (\ref{m2}), with the number of components known \emph{a priori} as 2. The fitted model includes only monotone non-decreasing function of covariate $X$, and no intercept. The regression models are identical for each of the 2 components.

\begin{equation} \label{m2}
  Y = \sum_{k=1}^{2}\pi_k (g_{k} (X) \ +\  \epsilon_k)
\end{equation}

% TODO complete boostrap implementation. Have flexmix bootstrap return model with monotone fit(s) as well as posteriors for a bootstrapped rootogram
\begin{minipage}{0.8\textwidth}
<<pseudo_mixture_monovar, fig=TRUE, warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=

# data with 2 latent categories
################
pi1 <- 0.65
pi2 <- 0.35
n <- 1000

Xa <- runif(n*pi1,-10,10) #seq(-10,10, length.out=1001)
Xb <- runif(n*pi2,-10,10) #seq(-10,10, length.out=1001)


Y1t <- 50 + Xa^3 
Y1 <- Y1t + rnorm(n*pi1, 0, 30) # component 1
Y2t <- -50 + 0.04*(Xb)^5 + 30*Xb
Y2 <- Y2t + rnorm(n*pi2, 0, 20) # component 2
cat <- c(rep.int(1, n*pi1), rep.int(2,n*pi2))
# plot(X,Y1, type="p", cex=0.05)
# lines(X,Y2,type="p", cex=0.05)

df_2 <- data.frame(c(Y1, Y2), c(Xa,Xb), c(Y1t, Y2t), cat)
names(df_2) <- c("Y", "X", "Yt", "cat")
################


# build model
# TODO mono_reg model is FORCING a linear component here. It should fit a purely nonparametric model when it sees that the monotone indices are all variables minus the intercept.
m2 <- flexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X"))

# m2boot <- boot(initFlexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X")), R=10, verbose=1, model=TRUE, initialize_solution=TRUE)

# plot fitted model
plot(m2, ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=1)
@
% TODO describe the confidence intervals of the bootstrapped rootogram
\captionof{figure}[Abbrviated Caption]{The rootogram of the two-component mixture model shows the distribution of posterior probabilities with reference to the binary latent variable, for all observations used to fit the model. The model indicates higher confidence in the identification of clusters and the classification of individual observations when the observations accumulate near the limits of the rootogram, at 0 and 1. Conversely, greater mass at the center of the rootogram represents observations that are less confidently classified.}
\end{minipage}

\begin{minipage}{0.8\textwidth} 
<<pseudo_mixture_monovar_b, fig=TRUE, warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=
plot(m2, ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=2) + 
  geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
  geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")

@
% TODO add description/explanation of how the model has difficulty estimating the functions at/after the points where the functions cross one another.
\captionof{figure}[Abbrviated Caption]{The estimated monotone functions of the two-component mixture model, with overlaid, dotted black lines representing the true functions. The confidence intervals are generated by an ordinary (non-parametric) bootstrap.}
\end{minipage}

As can be seen from figure (\ref{pseudo_mixture_monovar_b}), the algorithm is more uncertain of the monotone regression shapes where the true data generating functions overlap. 


Alternately, we can fit mixtures with alternate specifications for each component. In the next demonstration, we model data generated from 2 latent categories with contrary monotonic effects, i.e., one with a monotone non-increasing true function and one with a monotone non-decreasing true function. The data has the following underlying structure:

\begin{align*}
  Y_{1} &= X^3 + \epsilon_1 \\
  Y_{2} &= 100 + 0.02 \cdot X^5 + \epsilon_2 \\
\end{align*}

where 

\begin{align*}
  \epsilon_1 &\sim N(0,30) \\
  \epsilon_2 &\sim N(0,20) \\
\end{align*}

and

\begin{align*}
  \pi_1 &= 0.7 \\
  \pi_2 &= 0.3 \\
\end{align*}

and

\begin{align*}
  X &\sim Uniform(-10,10) \\
\end{align*}


We proceed to estimate a mixture of univariate regressions (\ref{m2a}), with the number of components known \emph{a priori} as 2. The fitted model includes only monotone non-decreasing function of covariate $X$, and no intercept. The regression models are as follows for each of the 2 components:

\begin{equation} \label{m2a}
  Y = 
  \begin{dcases}
    \pi_1 (g_{1} (X) \ +\  \epsilon_1)
    \pi_2 (g_{2} (X) \ +\  \epsilon_2)
  \end{dcases}
\end{equation}

where $g_{1}$ is non-decreasing, and $g_{2}$ is non-increasing.

\begin{minipage}{0.8\textwidth}
<<pseudo_mixture_monovar_contrary, fig=TRUE, warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=

# TODO currently, you cannot specify different models for different components. the larger monoreg mixture object has inc/dec attributes, which it shouldn't -- these should belong to the components.


# data with 2 latent categories
################
pi1 <- 0.7
pi2 <- 0.3
n <- 1000

Xa <- runif(n*pi1,-10,10) #seq(-10,10, length.out=1001)
Xb <- runif(n*pi2,-10,10) #seq(-10,10, length.out=1001)

Y1t <- 50 + -Xa^3 
Y1 <- Y1t + rnorm(n*pi1, 0, 30) # component 1
Y2t <- -50 + 0.04*(Xb)^5 + 30*Xb
Y2 <- Y2t + rnorm(n*pi2, 0, 20) # component 2
cat <- c(rep.int(1, n*pi1), rep.int(2,n*pi2))
# plot(X,Y1, type="p", cex=0.05)
# lines(X,Y2,type="p", cex=0.05)

df_2 <- data.frame(c(Y1, Y2), c(X,X), c(Y1t, Y2t), cat)
names(df_2) <- c("Y", "X", "Yt", "cat")
################


# build model
# TODO mono_reg model is FORCING a linear component here. It should fit a purely nonparametric model when it sees that the monotone indices are all variables minus the intercept.
m2a <- flexmix(Y ~ X-1, data = df_2, k = 2, model = list(mono_reg(mon_inc_names = "X"), mono_reg(mon_dec_names = "X")) )

# m2boot <- boot(initFlexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X")), R=10, verbose=1, model=TRUE, initialize_solution=TRUE)

# plot fitted model
plot(m2a, ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=1)
@
% TODO describe the confidence intervals of the bootstrapped rootogram
\captionof{figure}[Abbrviated Caption]{The rootogram of the two-component mixture model shows the distribution of posterior probabilities with reference to the binary latent variable, for all observations used to fit the model. The model indicates higher confidence in the identification of clusters and the classification of individual observations when the observations accumulate near the limits of the rootogram, at 0 and 1. Conversely, greater mass at the center of the rootogram represents observations that are less confidently classified.}
\end{minipage}

\begin{minipage}{0.8\textwidth}
<<pseudo_mixture_monovar_contraryb, fig=TRUE, warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=
plot(m2a, ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=2) + 
  geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
  geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")

@

\captionof{figure}[Abbrviated Caption]{The estimated monotone functions of the two-component mixture model, with overlaid, dotted black lines representing the true functions. The confidence intervals are generated by an ordinary (non-parametric) bootstrap.}
\end{minipage}





We continue the demonstration with the inclusion of linear effects. For the next model, we generate pseudo-data from 4 latent categories with the following underlying structure:


\begin{align*}
  Y_{1} &= 10 + X_1^3 + 1.5\cdot X_2 - 1.5\cdot X_3 - X_4 + X_5 + \epsilon_1 \\
  Y_{2} &= -10 + 25 \cdot X_1 + 3\cdot X_2 + 2\cdot X_3 - 2\cdot X_4 + 2\cdot X_5 + \epsilon_2 \\
  Y_{3} &= -4 + 1.5 \cdot (X_1^3) - 2\cdot X_2 - X_3 + 2\cdot X_4 + 4\cdot X_5 + \epsilon_3 \\
  Y_{4} &= 4 + 0.1 \cdot (X_1^5) - 3\cdot X_2 - 3\cdot X_3 - 3\cdot X_4 + 3\cdot X_5 + \epsilon_4 \\
\end{align*}

where 

\begin{equation*}
\begin{aligned}[c]
  \epsilon_1 &\sim N(0,3) \\
  \epsilon_2 &\sim N(0,10) \\
  \epsilon_3 &\sim N(0,3) \\
  \epsilon_4 &\sim N(0,7) \\
\end{aligned}
\begin{aligned}[c]
  X_1 &\sim Uniform(-5,5) \\
  X_2 &\sim Uniform(-10,10) \\
  X_3 &\sim Uniform(-100,100) \\
  X_4 &\sim Uniform(-100,100) \\
  X_5 &\sim Uniform(-100,100) \\
\end{aligned}
\end{equation*}


We proceed to estimate a mixture of partial linear regressions (\ref{m3}), with the number of components known \emph{a priori} as 4. The fitted model includes one monotone non-decreasing function of covariate $X_1$, an intercept, and a linear effect for each of $X_2,...,X_5$. The regression models are identical for each of the 4 components.

\begin{equation} \label{m3}
  Y = \sum_{k=1}^{4}\pi_k (g_{k} (X_1) \ +\  \beta_{0,k} \ +\ \beta_{1,k}\cdot X_2 \ +\ \beta_{2,k}\cdot X_3 \ +\ \beta_{3,k}\cdot X_4 \ +\ \beta_{4,k}\cdot X_5 \ +\ \epsilon_k)
\end{equation}

\begin{minipage}{0.8\textwidth}
<<pseudo_mixture, warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=

# data with 4 latent categories
################
X <- cbind(
  runif(4000, -5, 5),
  runif(4000, -10, 10),
  runif(4000, -100, 100),
  runif(4000, -100, 100),
  runif(4000, -100, 100)
)


Y1 <- 10 + (X[1:1000,1])^3 + 1.5*X[1:1000,2] - 1.5*X[1:1000,3] - 1*X[1:1000,4] + X[1:1000,5] + rnorm(1000, 0, 3) # component 1
Y2 <- -10 + 40*(X[1001:2000,1]) + 3*X[1001:2000,2] + 2*X[1001:2000,3] - 2*X[1001:2000,4] + 2*X[1001:2000,5] + rnorm(1000, 0, 10) # component 2
Y3 <- -4 + 2*((X[2001:3000,1])^3) - 2*X[2001:3000,2] - 1*X[2001:3000,3] + 2*X[2001:3000,4] + 4*X[2001:3000,5] + rnorm(1000, 0, 3) # component 3
Y4 <- 4 + 0.1*((X[3001:4000,1])^5) - 3*X[3001:4000,2] - 3*X[3001:4000,3] - 3*X[3001:4000,4] + 3*X[3001:4000,5] + rnorm(1000, 0, 7) # component 4

df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
################

X <- seq(-5, 5, length.out=4000)



Y1 <- (X)^3  # component 1
Y2 <- 40*(X)  # component 2
Y3 <- 2*((X)^3)  # component 3
Y4 <- 0.1*((X)^5)  # component 4

df_t <- data.frame(Y1, Y2, Y3, Y4, X)
names(df_t) <- c("Y1","Y2","Y3","Y4", "X1")

###############


# build model
m3 <- flexmix(Y ~ ., data = df_3, k = 4, model = mono_reg(mon_inc_names = "X1"), control = list(minprior = 0.1))

# m3boot <- boot(initFlexmix(Y ~ ., data = df_3, k = 4, model = mono_reg(mon_inc_names = "X1")), R=10, initialize_solution=TRUE, verbose=1)

# plot fitted model
plot(m3, ylim=c(-100,100), palet="Dark2", root_scale="sqrt", subplot=1) 

@
\captionof{figure}[Abbrviated Caption]{Here we see the rootogram and mo}
\end{minipage}

\begin{minipage}{0.8\textwidth}
<<pseudo_mixture_b, warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=

plot(m3, ylim=c(-100,100), palet="Dark2", root_scale="sqrt", subplot=2) + 
  geom_line(data = df_t, aes(X1, Y1), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y2), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y3), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y4), linetype="dotted") 

@
\captionof{figure}[Abbrviated Caption]{Estimated monotone functions from within each component of the}
\end{minipage}

% TODO plot the forest-plot of linear-effect estimates and confidence intervals. How do we plot them while accounting for label switching?

\subsection{Algorithm Comparisons with Simulated Data}

In such instances, the proposed algorithm is demonstrably tighter around the true functions than the algorithm of Zhang et al. 

(INCLUDE COMPARISON PLOTS HERE).



\subsection{Real Data: Global Life Expectancy }

In this section, we apply the mixture of monotone regressions to global life expectancy data. Consider data on GDP per Person and Life Expectancy of all countries between the years 1960 and 2018, drawn from the free online resources of the World Bank [@worldbank]. Specifically, the data consists of $n$ observations $(y_1, \vec{x_1}),...,(y_n,\vec{x_n})$, where $Y$ represents Life Expectancy and the vector $\vec{X}$ represents both GDP and Year. 

Moreover, this data has two properties that are very common in real world data:
\begin{enumerate}[noitemsep] 
  \item Missing Data: 
  \item \emph{A priori} Groupings: This data contains multiple observations per country, but we expect that our model will constrain countries to be clustered together (LINK constraints section). 
\end{enumerate}

On a first pass visualization of this data, we find a mostly linear relationship between Life Expectancy \& Year (1), and a mostly logarithmic relationship between Life Expectancy \& GDP (2). 

<<motivation, echo=F, fig.align='center', fig.height=3, message=FALSE>>=


# exploratory vizualization

continent %>%
  filter(Region != "") %>%
  group_by(Region, Year) %>%
  summarise(mean_LE = mean(LifeExpectancy)) %>%
ggplot(mapping = aes(x = Year, y = mean_LE, color = as.factor(Region))) + 
  geom_line() +
  theme_minimal() +
  labs(title = "Life Expectancy by Global Region",
       color="Regions") +
  ylab("Life Expectancy (years)") +
  xlab("Year")

continent %>%
  filter(Region != "") %>%
  group_by(Region, Year) %>%
  summarise(mean_GDP = log(mean(GDP)), mean_LE = mean(LifeExpectancy)) %>%
ggplot(mapping = aes(x = mean_GDP, y = mean_LE, color = as.factor(Region))) + 
  geom_line() +
  theme_minimal() +
  labs(title = "GDP per Capita by Global Region (log scale)",
       color="Regions") +
  ylab("Life Expectancy (years)") +
  xlab("GDP per Capita (current USD, log scale)")
  
  
@


For the purposes of demonstration, we choose to model this data without log-transforming the GDP data in order to preserve its highly non-linear relationship with Life Expectancy. We proceed by building two step models: with and without an intercept, and each with `GDP` as the monotone covariate. Each model is in fact a series of 21 mixture models, 3 for each of $k = 1,...,7$. These models have the form of \ref{m6} and \ref{m7} respectively.

\begin{equation} \label{m6}
  Y = \sum_{k=1}^{K}\pi_k (g_{k} (GDP) \ +\  \beta_{k}\cdot Year \ +\ \epsilon_k)
\end{equation}

\begin{equation} \label{m7}
  Y = \sum_{k=1}^{K}\pi_k (g_{k} (GDP) \ +\  \beta_{0,k} \ +\ \beta_{1,k}\cdot Year \ +\ \epsilon_k)
\end{equation}

For each series, we plot the AIC and BIC per $k$, the rootogram of the model with the lowest AIC, and the fitted monotone functions for the model with the lowest AIC.


[PLOT MODELS AIC/BIC AND MONOTONE COMPS HERE]
<<le_mod6, eval=F, echo=FALSE, results=F>>=

m6 <- stepFlexmix(LifeExpectancy ~ .-1-Country.Name|Country.Name, data = le, k = 1:4, model = mono_reg(mon_inc_names = "GDP")) # step model | no intercept | grouped | monotone GDP

@

<<le_mod7, eval=F, echo=FALSE, results=F>>=

m7 <- stepFlexmix(LifeExpectancy ~ .-Country.Name|Country.Name, data = le, k = 1:4, model = mono_reg(mon_inc_names = "GDP")) # step model | with intercept | grouped | monotone GDP

@





\begin{minipage}{0.8\textwidth}
<<le_mod_plot1, eval=F, echo=F, message=F, fig.align='center', fig.height=3>>=

# m6
plot(m6, main="AIC / BIC / ICL in Model 6 by Number of Components")
num <- which.max(apply(m6@logLiks, 1, function(x) mean(x)))
 # format data for adding fitted Life Expectancy curves by country
plot_data <- le
plot_data$Cluster <- factor(m6@models[[num]]@cluster)
plot_data$fitle <- plot_data$LifeExpectancy - (sapply(plot_data$Cluster, function(x) m6@models[[num]]@components[[x]][[1]]@parameters$coef)*plot_data$Year)
plot(m6@models[[num]], log=T, subplot=2) + # overlay monotone plots with plots of individual countries
  geom_line(data = plot_data, mapping = aes(GDP, fitle, group=Country.Name, color=Cluster), alpha=0.1) +
  scale_color_brewer(palette="Dark2") +
  theme_bw() + 
  scale_x_log10() # set to log scale
# TODO 3-D plot?

@
\captionof{figure}[Abbrviated Caption]{The }
\end{minipage}

<<le_mod_plot2, eval=F, echo=F, message=F, fig.align='center', fig.height=3>>=

############
# m7
plot(m7, main="AIC / BIC / ICL in Model 7 by Number of Components")
num <- which.max(apply(m7@logLiks, 1, function(x) mean(x)))
 # format data for adding fitted Life Expectancy curves by country
plot_data <- le
plot_data$Cluster <- factor(m7@models[[num]]@cluster)
plot_data$fitle <- plot_data$LifeExpectancy - apply(t(sapply(plot_data$Cluster, function(x) m7@models[[num]]@components[[x]][[1]]@parameters$coef))*cbind(rep.int(1, dim(plot_data)[2]), plot_data$Year), 1, sum) # jesus... subtract from life expectancy the SUM of the intercept and the GDP coeffecient (of the correct cluster) x the correct country-year GDP
plot(m7@models[[num]], log=T, subplot=2) + # overlay monotone plots with plots of individual countries
  geom_line(data = plot_data, mapping = aes(GDP, fitle, group=Country.Name, color=Cluster), alpha=0.1) +
  scale_color_brewer(palette="Dark2") +
  theme_bw() +
  scale_x_log10() # set to log scale
@

 

Finally, we plot the distribution of clusters within the lowest-AIC model of each step-model series, projected onto a world map. In these world map plots, the colors of each cluster span a spectrum from white to full-color, representing the strength of the posterior and the confidence of the model in placing a given country within a given cluster. The world-maps indicate what the rootograms had previously indicated, namely that the resulting models are extremely confident about the clustering of nearly all countries.

<<worldmaps1, eval=F, echo=F, message=F, fig.align='center', fig.height=4>>=

world <- ne_countries(scale = "medium", returnclass = "sf")

mod6 <- m6@models[[which.min(AIC(m6))]]


maxpost <- apply(mod6@posterior$scaled, 1, function(x) max(x))
clustdat <- tibble(le$Country.Name, mod6@cluster, maxpost)
names(clustdat) <- c("name", "cluster", "posterior")
plot_data <- merge(world, clustdat, by = "name", all.x = TRUE)

gg <- ggplot()
pal <- brewer.pal(length(mod6@components), "Dark2")

for(j in 1:length(mod6@components)){
  gg <- gg + geom_sf(data = plot_data[plot_data$cluster==j,], aes(fill = posterior)) + 
  scale_fill_gradient2(paste("Cluster", j), limits=c(0,1), low = "white", high = pal[j]) +
  theme(legend.direction = "horizontal") +
 
  new_scale("fill")    
}

gg <- gg + theme_minimal() + 
  theme(legend.direction = "horizontal") + 
  ggtitle("Map of Country Clusters in m6")


# TODO extract legend grob, edit, and replace with title. If this doesnt work, we resort to adding a text annotation
g <- ggplotGrob(gg) # Get the ggplot grob
leg <- g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] # Get the legend
title_grob <- textGrob("Posterior", gp = gpar(fontsize = 12))
table_grob <- gtable_add_rows(leg, heights = grobHeight(title_grob) + unit(5,'mm'), pos = 0)
table_grob <- gtable_add_grob(table_grob, title_grob, 1, 1, 1, ncol(table_grob), clip = "off")
g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] <- table_grob # replace original grob with edited grob

ggpubr::as_ggplot(g) + theme_minimal() # cast back to ggplot, and print


@

<<worldmaps2, eval=F, echo=F, message=F, fig.align='center', fig.height=4>>=

##################
world <- ne_countries(scale = "medium", returnclass = "sf")

mod7 <- m7@models[[which.min(AIC(m7))]]


maxpost <- apply(mod7@posterior$scaled, 1, function(x) max(x))
clustdat <- tibble(le$Country.Name, mod7@cluster, maxpost)
names(clustdat) <- c("name", "cluster", "posterior")
plot_data <- merge(world, clustdat, by = "name", all.x = TRUE)

gg <- ggplot()
pal <- brewer.pal(length(mod7@components), "Dark2")

for(j in 1:length(mod7@components)){
  gg <- gg + geom_sf(data = plot_data[plot_data$cluster==j,], aes(fill = posterior)) +
  scale_fill_gradient2(paste("Cluster", j), limits=c(0,1), low = "white", high = pal[j]) +
  theme(legend.direction = "horizontal") +

  new_scale("fill")
}

gg <- gg + theme_minimal() +
  theme(legend.direction = "horizontal") +
  ggtitle("Map of Country Clusters in m7")

# TODO extract legend grob, edit, and replace with title. If this doesnt work, we resort to adding a text annotation
g <- ggplotGrob(gg) # Get the ggplot grob
leg <- g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] # Get the legend
title_grob <- textGrob("Posterior", gp = gpar(fontsize = 12))
table_grob <- gtable_add_rows(leg, heights = grobHeight(title_grob) + unit(5,'mm'), pos = 0)
table_grob <- gtable_add_grob(table_grob, title_grob, 1, 1, 1, ncol(table_grob), clip = "off")
g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] <- table_grob # replace original grob with edited grob

ggpubr::as_ggplot(g) + theme_minimal() # cast back to ggplot, and print


@




<<predictions, eval=F, echo=F, message=F, fig.align='center'>>=
# TODO demonstrate predictions of new observed X
# TODO predictions should give: a posterior distribution of cluster belonging, and
# TODO a marginal distribution of Y given X

# TODO in order to demonstrate this 


@


\section{Discussion}

\subsection{Applications}

\subsection{Weaknesses}

\subsection{Future Developments}


\section*{Acknowledgments}

Here I acknowledge all my homies.

\bibliography{mono}



\begin{appendices}
\chapter{R Code}


All results in this paper were produced by an extension of Flexmix (CITE), a flexible implementation of generalized mixture models written by FLEISCH and GRUN. The package provides a framework for implementing specific types of mixture models based on a central, universal framework. It is thus intended to allow users to elaborate a specific estimator or "driver" for the modeling of mixture components, while the more abstract behaviours are managed by the Flexmix code. More specifically, Flexmix implements:

\begin{enumerate}
  \item The universal functions of the EM algorithm for assigning prior values $\pi_k$ to each of the mixture components and posterior values $\mathcal{L}_i$ to each of the observations upon each iteration of the EM algorithm;
  \item Threshold constants for the convergence of the EM algorithm;
  \item Restrictions on the model estimate, e.g., restricting components to have an estimated prior above a certain threshhold;
  \item Ordinary- and parametric-bootstrap methods for the estimation of confidence intervals with respect to each component.
\end{enumerate}

Conversely, the user must provide a model estimator and a model object with a log-likelihood method, logLik() (FONT), which returns the density of an observation given a component's parameters. In the case of mixtures of regressions, this additionally implies a prediction function, predict() (FONT), which gives an expected value of an observation's dependent variable given the observed independent variables.

WHAT ELSE DOES FLEXMIX DO FOR US?

The code for the extension of Flexmix for modelling mixtures of partially-linear monotone regressions is included below. The first code block implements the partial linear model with monotone shape constraints; the second code block integrates the partial linear model with the Flexmix framework; the third code block provides additional functions for the visualization of results; the fourth code block implements the "conditional bootstrap" described in section (LINK).


<<part_fit, eval=FALSE>>=
# Define function for fitting a partial linear model with arbitrary monotone-constrained component

# import libraries
library(gridExtra)
library(dplyr)


cpav <- function(x_mat, y, weights, inc_index=NULL, dec_index=NULL, max_iters_cpav=NULL, max_delta_cpav=NULL){
  
  joint_ind <- c(inc_index, dec_index)
  
  if(!is.matrix(x_mat)) stop("x_mat is not of class matrix, and will be rejected by lm.wfit")
  if(any(weights == 0)) stop("monoreg(), and therefore cpav(), cannot take weights of 0!")
  if(length(y) != length(weights) | length(y) != dim(x_mat)[1]) stop("The dimension of the inputs is not 
                                        equal to the dimension of the weights")
  
  # if there is only 1 monotone component, apply ordinary monotone regression
  if(length(joint_ind) == 1){
    if(length(inc_index) == 1){ # the component is monotone increasing
      return( # cast the monoreg object as a matrix, with all attributes as rows in the first column
        matrix(suppressWarnings(monoreg(x = x_mat[,inc_index], y = y, w = weights)), 
               dimnames = list(c("x", "y", "w", "yf", "type", "call")))
      )
    }
    else{ # the component is monotone decreasing
      return( # cast the monoreg object as a matrix, with all attributes as rows in the first column
        
        matrix(suppressWarnings(monoreg(x = x_mat[,dec_index], y = y, w = weights, type = "antitonic")), 
               dimnames = list(c("x", "y", "w", "yf", "type", "call")))
      )
    }
  }
  
  else{ # the monotone components are multiple, so continue with cyclic algorithm
    
    # fit ordinary lm on x_mat and y
    start_betas <- coef(lm.wfit(x=x_mat[,joint_ind], y=y, w=weights))
    
    # set initial monotone reg estimates by calling each monoreg() against y - lm.predict(all other vars)
    
    mr_fits <- sapply(1:length(joint_ind), function(i) 
      if(joint_ind[i] %in% inc_index){
        # I apologize to anyone trying to read this line, but think: the columns of the x_matrix
        # indicated by join_ind, except the value of joint_ind at the ith place in joint_ind
        suppressWarnings(monoreg(x = x_mat[,joint_ind[i]], 
                y = (y - (as.matrix(x_mat[,joint_ind[-i]]) %*% start_betas[-i]) ), w = weights, type = "isotonic"))
        }
      else if(joint_ind[i] %in% dec_index){
        suppressWarnings(monoreg(x = x_mat[,joint_ind[i]], 
                y = (y - (as.matrix(x_mat[,joint_ind[-i]]) %*% start_betas[-i]) ), w = weights, type = "antitonic"))
      })
    
    # iterate through mr_fits. each column of mr_fits (e.g., mr_fits[,1]) is a monoreg fitted object,
    # and its attributes can be called (e.g., mr_fits[,1]$yf)
    iters <- 0
    delta <- 0.5
    if(is.null(max_iters_cpav)){
      max_iters_cpav <- 100
    }
    if(is.null(max_delta_cpav)){
      max_delta_cpav <- 0.0000001
    }
    
    while(abs(delta) > max_delta_cpav & iters < max_iters_cpav){
      old_SS <- mean( (y - get_pred(mr_fits, x_mat[,joint_ind]))^2 )
      
      for(i in 1:length(joint_ind)){
        if(joint_ind[i] %in% inc_index){
          # I apologize to anyone trying to read this line, but think: the columns of the x_matrix
          # indicated by join_ind, except the value of joint_ind at the ith place in joint_ind
          mr_fits[,i] <- suppressWarnings(monoreg(x = x_mat[,joint_ind[i]],
                                 y = (y - get_pred(mr_fits[,-i], x_mat[,joint_ind[-i]]) ), w = weights, type = "isotonic"))
        }
        else if(joint_ind[i] %in% dec_index){
          mr_fits[,i] <- suppressWarnings(monoreg(x = x_mat[,joint_ind[i]],
                                 y = (y - get_pred(mr_fits[,-i], x_mat[,joint_ind[-i]]) ), w = weights, type = "antitonic"))
        }
        
      }

      new_SS <- mean( (y - get_pred(mr_fits, x_mat[,joint_ind]))^2 )
      delta <- (old_SS - new_SS)/old_SS
      iters <- iters + 1
    }
    
    return(mr_fits)
  }
}


# first, define function for obtaining f(x_new) for monotone regression f()
# get_pred returns a vector of length = nrows(xvals), ie, a value for each observation of xvals
get_pred <- function(mr_obj, xvals){
  xvals <- as.matrix(xvals)
  mr_obj <- as.matrix(mr_obj)
  
  if(dim(mr_obj)[2] != dim(xvals)[2]) stop("get_pred() must take an X-matrix with as many columns
                                            as monoreg() objects")
  
  apply(sapply(1:ncol(xvals), function(j)
         mr_obj[,j]$yf[sapply(xvals[,j], function(z)
           ifelse( z < mr_obj[,j]$x[1], 1,
            ifelse(z >= tail(mr_obj[,j]$x, n=1), length(mr_obj[,j]$x), 
                   which.min(mr_obj[,j]$x <= z)-1 )))]
         ), 1, function(h) sum(h))

}


# define partial linear regression of y on x with weights w
# inputs are: x, y, wates, mon_inc_index, mon_dec_index, max_iter
part_fit <- function(x, y, wates = NULL, mon_inc_index=NULL, mon_dec_index=NULL, max_iter=NULL, 
                     component = NULL, na.rm=T, mono_inc_names = NULL, mon_dec_names = NULL, ...){

  # cast x to matrix
  x <- as.matrix(x)
  
  # set default weights
  if(is.null(wates)) wates <- rep(1, length(y))
  
  # remove incomplete cases
  if(T){ # for now, there is no alternative to na.rm=T.  All incomplete cases are removed.
    cc <- complete.cases(y) & complete.cases(x) & complete.cases(wates)
    y <- y[cc]
    x <- x[cc,]
    wates <- wates[cc]
    cc <- NULL
  }
  
  x <- as.matrix(x) # cast again. hacky but necessary?
  
  
  # make sure y and wates is not multivariate
  if(length(y) != dim(x)[1] | length(y) != length(wates)) stop("Inputs are not of the same dimension!")
  
  # take monotone indices of previous component
  if(!is.null(component)){
    inc_ind <- component$mon_inc_index
    dec_ind <- component$mon_dec_index
  }
  else{
    # assume that monotone variable is first column in x and increasing, unless specified otherwise
    if(!is.null(mon_inc_index)){
      inc_ind <- mon_inc_index
    } 
    else{
      inc_ind <- 1
    }
    if(!is.null(mon_dec_index)){
      dec_ind <- mon_dec_index
    } 
    else{
      dec_ind <- NULL
    }
  }
 
  # throw warning if there are duplicates in inc_ind or dec_ind, and then remove
  if(anyDuplicated(inc_ind) | anyDuplicated(dec_ind)){
    warning("There are duplicate index instructions; Duplicates are being removed.")
    inc_ind <- unique(inc_ind)
    dec_ind <- unique(dec_ind)
  }
  
  # throw error if indices overlap
  if(length(intersect(inc_ind, dec_ind)) > 0) stop("At least one variable was marked as BOTH
                                               monotone increasing and monotone decreasing.")
  
  # throw error if indices are not integers
  if(!is.null(inc_ind)){
    if(any(inc_ind != as.integer(inc_ind))) stop("Monotone increasing indices are not integers.")
  }
  if(!is.null(dec_ind)){
    if(any(dec_ind != as.integer(dec_ind))) stop("Monotone decreasing indices are not integers.")
  }
  
  # throw error if indices are not positive 
  if(any(c(inc_ind, dec_ind) < 1)) stop("all monotone component indices must be positive")
  
  # throw error if the number of indices exceeds columns of x
  if(length(c(inc_ind, dec_ind)) > ncol(x)) stop("Number of proposed monotonic relationships exceeds columns of x.")
  
  # If there is an intercept but no other linear effects, stop
  if((length(c(inc_ind, dec_ind))+1) == ncol(x) & "(Intercept)" %in% colnames(x)){
    stop("For identifiability purposes, you cannot build a part_fit with only an intercept as a linear component.")
  }
  
  # option for fit with no linear independent components and one or multiple monotone components:
  if(length(c(inc_ind, dec_ind)) == ncol(x)){
    
    yhat <- cpav(x_mat = as.matrix(x[wates != 0,]), y = y[wates != 0], weights = wates[wates != 0], 
                 inc_index = inc_ind, dec_index = dec_ind)

    # get residuals of model
    resids <- y - get_pred(yhat, x[,c(inc_ind, dec_ind)])
    
    # mod must have: coef attribute, sigma attribute, cov attribute, df attribute, ..., and 
    # may have mon_inc_index and mon_dec_index attributes
    mod <- list(coef = NULL, fitted_pava = NULL, sigma = NULL, df = NULL,
                mon_inc_index = NULL, mon_dec_index = NULL, iterations = NULL, 
                mono_inc_names = NULL, mon_dec_names = NULL)
  
    mod$coef <- NULL
    mod$fitted_pava <- yhat
    mod$mon_inc_index <- inc_ind
    mod$mon_dec_index <- dec_ind
    mod$sigma <- sqrt(sum(wates * (resids)^2 /
                                     mean(wates))/ (nrow(x)-qr(x)$rank))
    mod$df <- ncol(x)+1
    
    class(mod) <- "part_fit"
    
    return(mod)
  }
  else{
    # for starting values, fit a regular lm
    fit <- lm.wfit(x=x, y=y, w=wates)
    betas <- coef(fit)[-c(inc_ind, dec_ind)]
  
    # set maximum iterations for convergence
    if(!is.null(max_iter) & !is.list(max_iter)){
      if(max_iter < 1) stop("max_iter must be positive")
      maxiter <- max_iter
    }
    else{ 
      maxiter <- 10000
    }
      
    # set while loop initial values
    iter <- 0
    delta <- 10
    # iterate between pava and linear model
    while(delta > 1e-6 & iter < maxiter){ # works well enough with delta > 1e-12. Trying 1e-6
  
      yhat <- cpav(x_mat = as.matrix(x[wates != 0,]), y = (y[wates != 0] - (as.matrix(x[wates != 0,-c(inc_ind, dec_ind)]) %*% betas)), 
                   weights = wates[wates != 0], inc_index = inc_ind, dec_index = dec_ind)

      old_betas <- betas    # save old betas for distance calculation
      # to retrieve old ordering of y for fitted values, we use y[match(x, sorted_x)]
      betas <- coef(lm.wfit(x=as.matrix(x[,-c(inc_ind, dec_ind)]), y= (y - get_pred(yhat, x[,c(inc_ind, dec_ind)]) ), w=wates))

      # get euclidian distance between betas transformed into unit vectors
      delta <- dist(rbind( as.vector(betas)/norm(as.vector(betas), type ="2"), 
                           as.vector(old_betas)/norm(as.vector(old_betas), type="2")
                           ))
  
      iter <- iter + 1    # iterate maxiter
    }
  }
  
  # get residuals of model
  resids <- y - (get_pred(yhat, x[,c(inc_ind, dec_ind)]) + (as.matrix(x[,-c(inc_ind, dec_ind)]) %*% betas))

  # mod must have: coef attribute, sigma attribute, cov attribute, df attribute, ..., and 
  # may have mon_inc_index and mon_dec_index attributes
  mod <- list(coef = NULL, fitted_pava = NULL, sigma = NULL, df = NULL,
              mon_inc_index = NULL, mon_dec_index = NULL, iterations = NULL, 
              mono_inc_names = NULL, mon_dec_names = NULL)
  
  mod$coef <- betas
  mod$fitted_pava <- yhat
  mod$iterations <- iter
  mod$mon_inc_index <- inc_ind
  mod$mon_dec_index <- dec_ind
  mod$sigma <- sqrt(sum(wates * (resids)^2 / mean(wates))/ (nrow(x)-qr(x)$rank))
  mod$df <- ncol(x)+1

  class(mod) <- "part_fit"
  
  return(mod)
}


# write plot method for objects returned from part_fit()

append_suffix <- function(num){
  suff <- case_when(num %in% c(11,12,13) ~ "th",
                    num %% 10 == 1 ~ 'st',
                    num %% 10 == 2 ~ 'nd',
                    num %% 10 == 3 ~'rd',
                    TRUE ~ "th")
  paste0(num, suff)
}

plot.part_fit <- function(z){
  if(dim(as.matrix(z$fitted_pava))[2] > 1){
    temp <- list()
    for(i in 1:dim(as.matrix(z$fitted_pava))[2]){
      temp[[i]] <- ggplotGrob(ggplot() +
        geom_line(aes(x = z$fitted_pava[,i]$x, y = z$fitted_pava[,i]$yf)) + 
        theme_bw() +
        labs(title = paste(append_suffix(i), " Monotone Regression"),
             x = "X",
             y = "Y"))
    }
    return(grid.arrange(grobs=temp, ncol=1))
  }
  else{
    temp <- ggplot() +
      geom_line(aes(x = z$fitted_pava[,1]$x, y = z$fitted_pava[,1]$yf)) + 
      theme_bw() +
      labs(title = "Monotone Regression",
           x = "X",
           y = "Y")
    return(temp)
  }
}

@


<<M_driver, eval=FALSE>>=

# The M-step of the EM Algorithm. Meshes with Flexmix Package.

# allow slots defined for numeric to accept NULL
setClassUnion("numericOrNULL",members=c("numeric", "NULL"))
setClassUnion("characterOrNULL", members = c("character", "NULL"))
setOldClass("monoreg")
setClassUnion("matrixOrMonoreg", members = c("matrix", "monoreg"))

# Define new classes
setClass(
  "FLX_monoreg_component",
  contains="FLXcomponent",
  # allow mon_index to take either numeric or NULL
  slots=c(mon_inc_index="numericOrNULL", 
          mon_dec_index="numericOrNULL",
          mon_obj="matrix",
          mon_inc_names="characterOrNULL",
          mon_dec_names="characterOrNULL"
          )
) 

# Define FLXM_monoreg 
setClass("FLXM_monoreg",
         # TODO what does FLXM_monoreg need to inherit?
         contains = "FLXM",
         slots = c(mon_inc_index="numericOrNULL", 
                   mon_dec_index="numericOrNULL",
                   mon_inc_names="characterOrNULL",
                   mon_dec_names="characterOrNULL"))






# definition of monotone regression model.
mono_reg <- function (formula = .~., mon_inc_names = NULL, 
                      mon_dec_names = NULL, mon_inc_index=NULL, mon_dec_index=NULL, ...) {

  # only names or indices can be indicated, not both
  if((!is.null(mon_inc_names)|!is.null(mon_dec_names)) &
     (!is.null(mon_inc_index)|!is.null(mon_dec_index))) stop("mono_reg() can accept either monotone
                                                             names or indices can be chosen, but not both.")
  
  retval <- new("FLXM_monoreg", weighted = TRUE,
                formula = formula,
                name = "partially linear monotonic regression",
                mon_inc_index= sort(mon_inc_index),
                mon_dec_index= sort(mon_dec_index), 
                mon_inc_names= mon_inc_names,
                mon_dec_names= mon_dec_names) 
  
  # @defineComponent: Expression or function constructing the object of class FLXcomponent
  # fit must have attributes: coef, sigma, cov, df, ..., and 
  # may have mon_inc_index and mon_dec_index attributes
  # ... all must be defined by fit() function
  retval@defineComponent <- function(fit, ...) {
                  # @logLik: A function(x,y) returning the log-likelihood for observations in matrices x and y
                  logLik <- function(x, y) { 
                    dnorm(y, mean=predict(x, ...), sd=fit$sigma, log=TRUE)
                  }
                  # @predict: A function(x) predicting y given x. 
                  # TODO x must be partitioned into linear and monotone covars
                  predict <- function(x) {
                    inc_ind <- fit$mon_inc_index
                    dec_ind <- fit$mon_dec_index
                    
                    p <-  get_pred(fit$fitted_pava, x[,c(inc_ind, dec_ind)])
                    if(!is.null(fit$coef)){
                      p <- p + (as.matrix(x[,-c(inc_ind, dec_ind)]) %*% fit$coef)
                    }
                    p
                  }
                  # return new FLX_monoreg_component object
                  new("FLX_monoreg_component", parameters =
                        list(coef = fit$coef, sigma = fit$sigma),
                      df = fit$df, logLik = logLik, predict = predict,
                      mon_inc_index = fit$mon_inc_index,
                      mon_dec_index = fit$mon_dec_index,
                      mon_obj = fit$fitted_pava,
                      mon_inc_names = fit$mon_inc_names,
                      mon_dec_names = fit$mon_dec_names)
  }
  
  # @fit: A function(x,y,w) returning an object of class "FLXcomponent"
  retval@fit <- function(x, y, w, component, mon_inc_index = retval@mon_inc_index, 
                         mon_dec_index = retval@mon_dec_index, 
                         mon_inc_names = retval@mon_inc_names,
                         mon_dec_names = retval@mon_dec_names, ...) {
    
                  
                  if(is.null(mon_inc_index) & is.null(mon_dec_index)){
                    
                    # if not all monotone names are in the design matrix, stop & print the name that is missing
                    if(!all(c(mon_inc_names, mon_dec_names) %in% colnames(x))){
                      stop(paste(setdiff(c(mon_inc_names, mon_dec_names), colnames(x)),
                                 "could not be found in the model matrix. Check your spelling."))
                    } 
                    # Discover correct monotone indices
                    if(any(colnames(x) %in% mon_inc_names)){
                      mon_inc_index <- which(colnames(x) %in% mon_inc_names)
                    }
                    if(any(colnames(x) %in% mon_dec_names)){
                      mon_dec_index <- which(colnames(x) %in% mon_dec_names)
                    }
                  }
                  if(is.null(mon_inc_names) & is.null(mon_dec_names)){
                    # Discover correct monotone names
                    mon_inc_names <- colnames(x)[sort(mon_inc_index)]
                    mon_dec_names <- colnames(x)[sort(mon_dec_index)]
                  }
    
                  # if(any(apply(x, 2, function(x) is.factor(x)))) stop("x cannot have factor columns as monotone components")

                  fit <- part_fit(x, y, w, component, mon_inc_index=mon_inc_index, 
                                  mon_dec_index=mon_dec_index, ...)
                  
                  retval@defineComponent(fit, ...)
                  }
  retval 
  }



@


<<flexmix_wrappers, eval=FALSE>>=

# Wrapper functions for Flexmix objects with monoreg components

# import libraries
library(ggplot2)
library(grid)
library(gridExtra)
library(RColorBrewer)


# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}


####

# overwrite method for plot.flexmix
setMethod('plot',  signature(x="flexmix", y="missing"),
          function(x, mark=NULL, markcol=NULL, col=NULL, 
                   eps=1e-4, root=TRUE, ylim=NULL, xlim=NULL, main=NULL, xlab=NULL, ylab=NULL,
                   as.table = TRUE, endpoints = c(-0.04, 1.04), rootogram=F, palet = NULL, 
                   root_scale = "unscaled", subplot=NULL, ...) {
            
            if(is.null(palet)){
              palet <- "Accent"
            }
            
           
  if(is(x@components[[1]][[1]], "FLX_monoreg_component")){ # check that this is a mixture of part_fits
    # assign appropriate names for graph labelling
    if(is.null( c(x@model[[1]]@mon_inc_names, x@model[[1]]@mon_dec_names) )){
      xnames <- sapply(1:dim(x@components[[1]][[1]]@mon_obj)[2], function(x) paste0("X", x))
      mono_names <- c("Y", xnames)
    }
    else{
      mono_names <- c(x@formula[[2]], c(x@model[[1]]@mon_inc_names, x@model[[1]]@mon_dec_names))
    }
    
            # get dimension of monotone components by reading columns of fitted_pava object
            if(dim( x@components[[1]][[1]]@mon_obj )[2] > 1){ 
              np <- list()
              for(i in 1:dim( x@components[[1]][[1]]@mon_obj )[2]){
                holder <- ggplot()
                
                if(length(x@components) == 1){
                    holder <- holder + 
                      geom_line(aes(x = x@components[[1]][[1]]@mon_obj[,i]$x,
                                y = x@components[[1]][[1]]@mon_obj[,i]$yf)) +
                      theme_bw() +
                      labs(title = paste(append_suffix(i), " Monotone Regression"),
                       x = mono_names[i+1],
                       y = mono_names[1])
                    }
                
                if(length(x@components) > 1){
                  
                  monlist <- list()
                  for(b in 1:length(x@components)){
                    monlist[[b]] <- data.frame(x = x@components[[b]][[1]]@mon_obj[,i]$x, 
                                               yf = x@components[[b]][[1]]@mon_obj[,i]$yf)
                  }
                  
                  mondf   <- cbind(Cluster=rep(1:length(x@components), 
                                               sapply(monlist,nrow)),do.call(rbind,monlist))
                  mondf$Cluster <- as.factor(mondf$Cluster)
                  
                  holder <- holder + geom_line(mondf, mapping = aes(x,yf, color=Cluster)) + 
                    scale_color_brewer(palette=palet) +
                    theme_bw() +
                    labs(title = paste(append_suffix(i), " Monotone Regression"),
                         x = mono_names[i+1],
                         y = mono_names[1])
                }
                
                if(!is.null(ylim)){
                  if(length(ylim) != dim( x@components[[1]][[1]]@mon_obj )[2] |
                     length(ylim[[1]]) != 2 ){
                    stop("If you pass a ylim argument, it must have as many element pairs 
                         as the model has monotone components. Try formulating the argument
                         as: ylim = list(c(i,j), c(i,j), ...)")}
                  holder <- holder + ylim(ylim[[i]])
                }
                if(!is.null(xlim)){
                  if(length(xlim) != dim( x@components[[1]][[1]]@mon_obj )[2] |
                     length(xlim[[1]]) != 2 ){
                    stop("If you pass a xlim argument, it must have as many element pairs 
                         as the model has monotone components. Try formulating the argument
                         as: xlim = list(c(i,j), c(i,j), ...)")}
                  holder <- holder + xlim(xlim[[i]])
                }
                if(!is.null(ylab)){
                  if(length(ylab) != dim( x@components[[1]][[1]]@mon_obj )[2]){
                    stop("If you pass a ylab argument, it must have as many elements 
                         as the model has monotone components. Try formulating the argument
                         as: ylab = c(\"first\",\"second\",...)")}
                  holder <- holder + ylab(ylab[[i]])
                }
                if(!is.null(xlab)){
                  if(length(xlab) != dim( x@components[[1]][[1]]@mon_obj )[2]){
                    stop("If you pass a xlab argument, it must have as many elements 
                         as the model has monotone components. Try formulating the argument
                         as: xlab = c(\"first\",\"second\",...)")}
                  holder <- holder + xlab(xlab[[i]])
                }
                if(!is.null(main)){
                  if(length(main) != dim( x@components[[1]][[1]]@mon_obj )[2]){
                    stop("If you pass a main argument, it must have as many elements 
                         as the model has monotone components. Try formulating the argument
                         as: main = c(\"first\",\"second\",...)")}
                  holder <- holder + ggtitle(main[[i]])
                }
                
                
                
                np[[i]] <- holder
              }
              # return(grid.arrange(grobs=np, ncol=1))
              # return(grid.arrange(grobs=np, ncol=1))
            }
            else{
              np <- ggplot()
              
              if(length(x@components) == 1){
                np <- np + geom_line(aes(x = x@components[[1]][[1]]@mon_obj[,1]$x, y = 
                                           x@components[[1]][[1]]@mon_obj[,1]$yf)) + 
                  theme_bw() +
                  labs(title = "Monotone Component",
                     x = mono_names[2],
                     y = mono_names[1]) 
                }
              
              if(length(x@components) > 1){
                
                monlist <- list()
                for(b in 1:length(x@components)){
                  monlist[[b]] <- data.frame(x = x@components[[b]][[1]]@mon_obj[,1]$x, 
                                                   yf = x@components[[b]][[1]]@mon_obj[,1]$yf)
                }
                
                mondf   <- cbind(Cluster=rep(1:length(x@components), 
                                             sapply(monlist,nrow)), do.call(rbind, monlist))
                mondf$Cluster <- as.factor(mondf$Cluster)
                
                np <- np + geom_line(mondf, mapping = aes(x,yf, color=Cluster)) + 
                  scale_color_brewer(palette=palet) +
                  theme_bw() +
                  labs(title = "Monotone Component",
                       x = mono_names[2],
                       y = mono_names[1])
              }
              
              
              if(!is.null(ylim)){
                np <- np + ylim(ylim)
              }
              if(!is.null(xlim)){
                np <- np + xlim(xlim)
              }
              if(!is.null(ylab)){
                np <- np + ylab(ylab)
              }
              if(!is.null(xlab)){
                np <- np + xlab(xlab)
              }
              if(!is.null(main)){
                np <- np + ggtitle(main)
              }
              
              
              
              
              # return(np)
            }
  }
            # plot and append rootogram
            post <- data.frame(x@posterior$scaled) # collect posteriors
            names(post) <- 1:dim(post)[2] # change columns of posteriors to cluster numbers
            post <- melt(setDT(post), measure.vars = c(1:dim(post)[2]), variable.name = "Cluster")
            rg <- ggplot(post, aes(x=value, fill=Cluster)) + # plot rootogram, with color indicating cluster
              geom_histogram(binwidth = 0.05) + 
              scale_fill_brewer(palette=palet) +
              theme_bw() +
              labs(title = "Rootogram",
                   x = "Posteriors",
                   y = "Count")
            
            if(root_scale == "sqrt"){rg <- rg + 
              scale_y_sqrt() +
              labs(title = "Rootogram (square root scale)",
                   x = "Posteriors",
                   y = "Count (square root)")}
            if(root_scale == "log"){rg <- rg + 
              scale_y_log10() + 
              labs(title = "Rootogram (log scale)",
                   x = "Posteriors",
                   y = "Count (log)")}
            
            if(!is.null(subplot)){
              return(list(rg, np)[[subplot[1]]])
            }
            else{
              multiplot(rg, np)
            }
          }          
)

            
            

internal_boot <- function(fm, R=100){
  
  # Check that fm is a flexmix object
  if(!is(fm, "flexmix")) stop("This method is for FlexMix objects of type: partially linear monotonic regression.")
  
  # Check that fm has at least one monotone obj
  if(!"mon_obj" %in% names(attributes(fm@components[[1]][[1]]))) stop("This method is for FlexMix objects of type: partially linear monotonic regression.")
  
  Y <- fm@model[[1]]@y # store independent data
  dat <- fm@model[[1]]@x # store dependent data
  if("(Intercept)" %in% colnames(fm@model[[1]]@x)){
    x <- x[, colnames(m3@model[[1]]@x) %in% "(Intercept)"] # remove intercept
  }
  
  k <- length(m2@components) # Store the number of components
  coefs <- rep(list(),k)
  sigmas <- matrix(ncol = k, nrow = R)
  mon_fits <- rep(list(rep(NA, R)),k)
  
  for(i in 1:k){ # populate each parameter
    iter <- 1
    while(iter < R ){
      # TODO give part_fit an optional formula argument
      # TODO give part_fit a starting coef and g() argument
      current <- part_fit(formula = fm@model[[1]]@fullformula, start = fm@components[[i]][[1]]) 
      
      coefs[[i]][[iter]] <- current$coef
      sigmas[iter, i] <- current$sigma
      mon_fits[[i]][[iter]] <-current$fitted_pava
      
      # assign 
      iter <- iter + 1
    }
    
  }
  
  
}
          

@

\end{appendices}

\end{document}