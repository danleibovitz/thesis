\documentclass[10pt]{olplainarticle}
% Use option lineno for line numbers 
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{subfig}
\usepackage{caption}
\usepackage{bm}
\usepackage{courier}
\usepackage[toc,page]{appendix}
%\usepackage[nogin]{Sweave}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}



\setlength{\parskip}{1em}

% import libraries, code, and data
<<import, echo=FALSE, results='hide', message=FALSE, warning=FALSE>>=
library(lattice)
library(flexmix)
library(fdrtool)
library(dplyr)
library(data.table)
library(readr)
library(rnaturalearth)
library(ggnewscale)
library(gtable)
library(ggplot2)
library(RColorBrewer)
library(grid)
library(ggpubr)
library(gganimate)
library(gifski)
library(av)
library(rbenchmark)
library(rgeos)
library(utile.visuals)

# import monotone_mixture code
source("../monotone_mixture/monotone_driver/part_fit.R")
source("../monotone_mixture/monotone_driver/M_driver.R")
source("../monotone_mixture/pseudo_data/data_generator.R")
source("../monotone_mixture/monotone_driver/flex_wrapper.R")

# import GDP/Life expectancy data

lifex <- read.csv("../monotone_mixture/monotone_driver/API_SP.DYN.LE00.IN_DS2_en_csv_v2_1926713.csv", skip = 3)
continent <- read.csv("../monotone_mixture/monotone_driver/Metadata_Country_API_NY.GDP.MKTP.CD_DS2_en_csv_v2_1994746.csv")
gdp2 <- read.csv("../monotone_mixture/monotone_driver/API_NY.GDP.PCAP.CD_DS2_en_csv_v2_1926744.csv", skip=3)
  


lifex$Country.Name <- as.character(lifex$Country.Name)
lifex$Country.Name[which(lifex$Country.Name %in% c(
"Antigua and Barbuda",
"Bahamas, The" ,
"Bosnia and Herzegovina", 
"Brunei Darussalam" ,
"Cabo Verde"  ,
"Cayman Islands", 
"Central African Republic" ,
"Congo, Dem. Rep.", 
"Congo, Rep." ,
"Cote d'Ivoire", 
"Curacao" ,
"Czech Republic", 
"Dominican Republic", 
"Egypt, Arab Rep." ,
"Equatorial Guinea" ,
"Eswatini" ,
"Faroe Islands", 
"French Polynesia", 
"Gambia, The" ,
"Hong Kong SAR, China",
"Iran, Islamic Rep.",
"Korea, Rep." ,
"Kyrgyz Republic", 
"Macao SAR, China", 
"Marshall Islands" ,
"Micronesia, Fed. Sts.", 
"Russian Federation" ,
"Sao Tome and Principe",  
"Sint Maarten (Dutch part)", 
"Slovak Republic" ,
"Solomon Islands" ,
"South Sudan",
"St. Lucia" ,
"St. Vincent and the Grenadines", 
"Syrian Arab Republic",
"Venezuela, RB" ,
"Virgin Islands (U.S.)",
"Yemen, Rep." 
))] <- c( "Antigua and Barb.",
 "Bahamas",
 "Bosnia and Herz." ,
 "Brunei" ,
 "Cape Verde", 
 "Cayman Is." ,
 "Central African Rep.", 
 "Dem. Rep. Congo",
 "Congo",
 "Côte d'Ivoire",
 "Curaçao",
 "Czech Rep.",
 "Dominican Rep.",
"Egypt",
 "Eq. Guinea",
"Swaziland",
"Faeroe Is.",
 "Fr. Polynesia",
"Gambia",
 "Hong Kong",
 "Iran",
 "Korea",
 "Kyrgyzstan",
 "Macao",
 "Marshall Is.",
"Micronesia",
 "Russia",
 "São Tomé and Principe" ,
 "Sint Maarten",
 "Slovakia",
 "Solomon Is.",
 "S. Sudan",
 "Saint Lucia",
 "St. Vin. and Gren.",
 "Syria",
"Venezuela",
 "U.S. Virgin Is.",
 "Yemen"
 )
lifex$Country.Name <- as.factor(lifex$Country.Name)




gdp2$Country.Name <- as.character(gdp2$Country.Name)
gdp2$Country.Name[which(gdp2$Country.Name %in% c(
"Antigua and Barbuda",
"Bahamas, The" ,
"Bosnia and Herzegovina", 
"Brunei Darussalam" ,
"Cabo Verde"  ,
"Cayman Islands", 
"Central African Republic" ,
"Congo, Dem. Rep.", 
"Congo, Rep." ,
"Cote d'Ivoire", 
"Curacao" ,
"Czech Republic", 
"Dominican Republic", 
"Egypt, Arab Rep." ,
"Equatorial Guinea" ,
"Eswatini" ,
"Faroe Islands", 
"French Polynesia", 
"Gambia, The" ,
"Hong Kong SAR, China",
"Iran, Islamic Rep.",
"Korea, Rep." ,
"Kyrgyz Republic", 
"Macao SAR, China", 
"Marshall Islands" ,
"Micronesia, Fed. Sts.", 
"Russian Federation" ,
"Sao Tome and Principe",  
"Sint Maarten (Dutch part)", 
"Slovak Republic" ,
"Solomon Islands" ,
"South Sudan",
"St. Lucia" ,
"St. Vincent and the Grenadines", 
"Syrian Arab Republic",
"Venezuela, RB" ,
"Virgin Islands (U.S.)",
"Yemen, Rep." 
))] <- c( "Antigua and Barb.",
 "Bahamas",
 "Bosnia and Herz." ,
 "Brunei" ,
 "Cape Verde", 
 "Cayman Is." ,
 "Central African Rep.", 
 "Dem. Rep. Congo",
 "Congo",
 "Côte d'Ivoire",
 "Curaçao",
 "Czech Rep.",
 "Dominican Rep.",
"Egypt",
 "Eq. Guinea",
"Swaziland",
"Faeroe Is.",
 "Fr. Polynesia",
"Gambia",
 "Hong Kong",
 "Iran",
 "Korea",
 "Kyrgyzstan",
 "Macao",
 "Marshall Is.",
"Micronesia",
 "Russia",
 "São Tomé and Principe" ,
 "Sint Maarten",
 "Slovakia",
 "Solomon Is.",
 "S. Sudan",
 "Saint Lucia",
 "St. Vin. and Gren.",
 "Syria",
"Venezuela",
 "U.S. Virgin Is.",
 "Yemen"
 )
gdp2$Country.Name <- as.factor(gdp2$Country.Name)
@
% data cleaning
<<clean, echo=FALSE, results='hide', message=FALSE, warning=FALSE>>=

# clean lifex csv
lifex <- lifex[,c(-3, -4, -64, -65, -66)]
names(lifex)[3:61] <- substring(names(lifex)[3:61],2,5)
lifex <- melt(setDT(lifex), id.vars = 1:2, variable.name = "Year")
lifex <- lifex[complete.cases(lifex),]
lifex$Year <- as.integer(as.character(lifex$Year))
names(lifex)[4] <- "LifeExpectancy"
# lifex <- lifex[,-2]

# clean gdp2 csv
gdp2 <- gdp2[,c(-2,-3, -4, -64, -65, -66)]
names(gdp2)[2:60] <- substring(names(gdp2)[2:60],2,5)
gdp2 <- melt(setDT(gdp2), id.vars = 1, variable.name = "Year")
gdp2 <- gdp2[complete.cases(gdp2),]
gdp2$Year <- as.integer(as.character(gdp2$Year))
names(gdp2)[3] <- "GDP"


# merge
le <- merge(lifex, gdp2, by.x = c("Country.Name", "Year"), by.y = c("Country.Name", "Year"))

continent <- merge(le, continent, by.x = c("Country.Code"), by.y = c("Country.Code"))

le <- le[,-3] # remove country code
@

% set argmin and argmax as math operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\title{Mixtures of Partially Linear Models with Monotone Shape Constraints}

\author[1]{Daniel Leibovitz}
\author[2]{Matthias Loffler}
\affil[1]{daniel.leibovitz@uzh.ch}
\affil[2]{matthias.loeffler@stat.math.ethz.ch}

\keywords{Mixture Models, Shape Constraints, Isotonic Regression}

\begin{abstract}
Mixtures of non-parametric monotone regressions are readily applicable to clustering problems where there is prior knowledge about appropriate shape constraints within the resulting model. For example, option pricing functions in finance and risk-exposure relationships in epidemiology are both \emph{a priori} monotonic. The current standard for estimating such models involves fitting a series of non-parametric regression functions without shape constraints using an EM algorithm, as described by \cite{zhangetal}, followed by a monotonic estimate given the resulting latent variable classifications. In this paper, we propose to remove redundancy by incorporating the non-parametric monotone regression function estimate into the M-step of the EM algorithm. We demonstrate the effectiveness of the algorithm when applied to both simulated data and real-world data on global life expectancy and GDP from the World Bank.
\end{abstract}

\begin{document}


\flushbottom
\maketitle
\thispagestyle{empty}

\section{Introduction}

The mixture of partially linear regressions with monotone shape constraints takes the following form:

\begin{equation} \label{modstrucbrief}
  Y = 
  \begin{dcases}
    \sum_{h=1}^{p} g_{h1} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{j1} X_{j} \ +\ \epsilon_1, \text{  with probability $\pi_1$; }\\
    \vdots \\
    \sum_{h=1}^{p} g_{hk} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{jk} X_{j} \ +\ \epsilon_k, \text{  with probability $\pi_k$; } \\
  \end{dcases}
\end{equation}

where the model has $K$ components, $\boldsymbol{X} \in \mathbb{R}^p$, $\boldsymbol{Z} \in \mathbb{R}^p$, $\beta \in \mathbb{R}^p$, and each function $g_{hk}()$ is assumed monotone. The error $\epsilon$ is assumed to be normally distributed with mean $0$ and to be independent of the covariates $(\boldsymbol{X}, \boldsymbol{Z})$. The prior probabilities $\pi_k$ satisfy the conditions $\pi_k \in (0,1)$ and $\sum_{1}^{k} \pi_k = 1$.

Such a model has broad applications for clustering of data in domains where monotone relationships are known \emph{a priori}. These domains include, for example, epidemiology, where risk-exposure relationships may be modeled monotonically (\cite{morton}, \cite{carcinogen}); finance, where option pricing functions may be restricted to both monotonicity and ANDOR or convexity (\cite{optionpricing}); biomedical research, where biochemical kinetics may be monotone over time (\cite{kinetics}). 


A similar type of model has previously been described by \cite{zhangetal}, and takes the following form for a $k$-component mixture:

\begin{equation} \label{zhangstruc}
  Y = 
  \begin{dcases}
    g_{1} (Z) \ +\  \epsilon_1 , \text{  with probability $\pi_1(Z)$; }\\
    \vdots \\
    g_{k} (Z) \ +\  \epsilon_k , \text{  with probability $\pi_k(Z)$; }\\
  \end{dcases}
\end{equation}

where $\boldsymbol{Z} \in \mathbb{R}^1$, and each function $g_{k}()$ is assumed monotone. The variables $\epsilon_k$ and $\pi_k$ are not constant, as in model \ref{modstrucbrief}, but rather nonparametric functions $\epsilon_k(Z)$ and $\pi_k(Z)$. They nonetheless satisfy the same conditions as in model \ref{modstrucbrief}, namely, $\pi_k(Z) \in (0,1)$ and $\sum_{1}^{k} \pi_k(Z) = 1$ for any $Z$, and $\epsilon_k(Z)$ is normally distributed with $E(\epsilon_k | Z) = 0$, $Var(\epsilon_k | Z) = \sigma_k(Z)$. 

We have identified four drawbacks of in the model and estimator of Zhang et al.:

\begin{enumerate}[noitemsep] 
  \item The model proposed by Zhang et al. cannot be generalized to a semiparametric approach, i.e., one cannot include linear effects in the mixture components. Being able to include linear effects in the mixture components is conducive to two distinct purposes: 
    \begin{enumerate}[noitemsep]
      \item First, it can be used when the data to be clustered has multiple independent variables that are not of primary interest, but which the user would still like to include in the model. As Zhang et al. point out, including such varibles with nonparametric effects can explode the complexity of the algorithm beyond usability (see Section \ref{seccomplex}). Including such variables as linear effects is a safe alternative that keeps the complexity of the algorithm tractable.
      \item Second, users who are mainly concerned with linear effects of components within a mixture model can flexibly control for one or more nuisance variables that are suspected of being monotone in effect.
    \end{enumerate}
  \item The Zhang et al. estimator fits mixture components in two, sequential steps, first fitting an unconstrained function and then applying monotone constraints once the components membership has been calculated. This approach introduces a potential bias when components are not clearly identifiable.
  \item The Zhang et al. estimator requires a tuning parameter for the fitting of each unconstrained mixture component function, which adds computational complexity.
  \item Zhang et al. state that their model can be extended to the multivariate case, i.e., where $Z$ in model \ref{zhangstruc} is multivariate, but decline to explicitly define this model or its estimation.
\end{enumerate}

We propose model \ref{modstrucbrief} as an alternative, more generalized form of the approach suggested by Zhang et al. that avoids the drawbacks mentioned above. Specifically, our model accepts any number of non-parametric monotone or linear effects within each mixture component; it fits the monotone functions within each component in a single step; and it does not require the calibration of any tuning parameters.

The main weakness of our generalized model is that the monotone component of each regression is not a smooth function, as is the case in the approach by Zhang et al., but rather a step-wise function. However, if the user does not require smooth monotone functions, the advantages of the algorithm we propose may outweigh this cost.

The remainder of this paper is organized as follows. In Section 2, we discuss previous research in the domains of mixture models, partial linear models, and isotonic regression. In Section 3, we discuss the components of the proposed model. In Section 4, we describe the theoretical structure of the proposed model (\ref{modstrucbrief}) as well as the estimation algorithm, followed by the model's asymptotic properties and empirical complexity. In Section 5, we apply the proposed model to simulated data as well as World Bank data on global life expectancy through the end of the 20\textsuperscript{th} century. In the final section, we discuss implications and future work.


\section{Previous Work}

The model proposed in this article draws from several branches of statistical research. In this section, we briefly discuss the history and current state of said research.

\subsection{Regression with shape constraints}

Parametric regression models are constrained in their shape by construction. They can be further constrained, often trivially, by estimating their shape within a limited parameter space. A univariate linear model $E(Y) = X\beta$ in $\mathbb{R}^1$, for example, can be constrained to be non-decreasing by estimating $\hat{\beta}$ to be non-negative, i.e., $\hat{\beta} \in [0,+\infty)$. When estimating nonparametric regressions, this triviality is lost and one must reconsider how to estimate constrained functions. The resulting estimators are often applications of classical techniques such as maximum likelihood, and are often free of tuning parameters, making them attractive alternatives to typical, unconstrained nonparametric estimators (\cite{guntu}).

Several types of constraint have been considered over the years. Frisen described unimodal regression, i.e., the case where for $E(Y) = f(X)$, $f()$ has a unique local maximum or local minimum (\cite{unimodal}). Convex/concave regression, where intuitively, in the unidimensional case, the first derivatives of the estimated functions are non-decreasing/non-increasing respectively, was first given a least-squares point estimate by \cite{hildreth}. The Hildreth estimate was later proved to be consistent by \cite{hanson}, while its rate of convergence was established by \cite{mammenconvex}. More recently, convex estimation has been considered by \cite{seijo}, \cite{mazumder}, \cite{kuosmanen}, and \cite{groeneboom}.

Isotonic, or monotone, regression and its variants have received the most attention in the statistical literature, perhaps due to their continuing relevance. Monotone regressions have seen diverse applications across research domains; They have been used by \cite{jianhua} to analyze dose-response in bioinformatics, by \cite{luss} to estimate gene-gene interactions, and by \cite{diggle} to estimate disease risk as functions of spatial exposure, to name just a few examples. 

Isotonic regression maximum likelihood estimators with no smoothness requirements were first explored by \cite{brunk} and \cite{grenander}. The asymptotic distribution of these estimators was later established by \cite{wright}. The original Pool Adjacent Violators algorithm for the estimation of the isotonic regression MLE with normally distributed errors was first suggested by \cite{ayer}.

The literature regarding isotonic regression diverges at the point of choosing an estimator based on the crucial assumption of whether the resulting function is smooth or step-wise. For applications in which a step-wise function is acceptable, the estimators of Brunk, Grenander, Ayer, etc., have an exact solution and no tuning parameter. However, for applications in which a smooth function is required, alernate approaches have had to be developed, and a two-step procedure has become common. These two-step procedures either estimate a smooth function and apply a monotonic constraint on the resulting function (e.g., Friedman \& Tibshirani (\cite{friedman})), or estimate a monotonic function and then smooth the resulting estimate (e.g., Cheng \& Lin (\cite{cheng})). Mammen compares the asymptotic behaviours of these approaches in a comprehensive treatment of smooth, monotonic nonparametric regression. Although these procedures differ in their asymptotic behaviours, all of them require the selection of a tuning parameter (\cite{mammen}).

% discuss history, estimation methods (splines, inversion,), variance bounds.

% discuss monotone regressions in particular. different approaches -- pava, active set, etc.




\subsection{Partial Linear Models}

The Generalized Additive Model, or GAM, was first suggested by \cite{hastiegam}, and can be seen both as a generalization of the GLM to include nonparametric terms, and as a generalization of additive models to models with error terms from the exponential family. GAMs provide a large amount of flexibility to the nonparametric modeling of multivariate data, but avoid the slow convergence -- the curse of dimensionality -- associated with multivariate non-parametrics by imposing an additive structure between terms. 

Partial Linear Models, or PLMs, predate GAMs by several years, having been introduced in 1986 by \cite{engle} for the modelling of weather and energy-use. PLMs can nonetheless be considered more productively as a slightly more restrictive subset of GAMs, wherein some proportion of the model terms are required to be linear.

Both GAMs and PLMs have often been fit using the ``backfitting'' algorithm, first introduced by \cite{backfit} and used in the first descriptions of GAMs by Tibshirani and Hastie.

% numerical comparisons by \cite{hua}

\subsection{Mixture Models}

Model-based clustering, or mixture models, are a common method for producing models of latent clusters with probabalistic or ``soft'' cluster assignment. The history of mixture models is particularly long, with the first such models having been implemented more than a century ago by \cite{newcomb} and \cite{pearson}. In much of the following century, progress in the theory of mixture models and their estimation stalled due to a lack of computational power and for lack of an efficient estimating strategy.

This dry spell was called to a close by the introduction by \cite{dempster} of the EM algorithm for estimation of models with supposed latent variables, which vastly simplified the estimation of mixture models. Since then, development and research in mixture modelling has proceeded rapidly, with implementations of bayesian mixture models (\cite{marin}), infinite mixture models (\cite{infinite}), \emph{must-link} and \emph{cannot-link} constraints (\cite{wagstaff}), and many variations of normal mixture models (\cite{mclachlannormal}, \cite{fraley}) all having been published within the last two decades.

Yet more recently, mixtures of regressions have received increased attention (\cite{mixglm}, \cite{viele}, \cite{hurn}) as easily interpretable clustering methods that do specify a dependence structure amongst observed covariates without modelling the distributions of the covariates themselves. 


% discuss asymptotic MLE.

% Finite mixture models date from the 19\textsuperscript{th} century (CITE), while the more commonly known implementation via the EM algorithm was first introduced in 1977 (Dempster, Laird, Rubin).
% Ifinite mixture models: \cite{infinte}

% \subsection{Mixtures of Regressions}

% From the larger set of mixture models, mixtures of regressions play a useful role in cases where one wishes to model a dependence structure amongst observed covariates without modelling the distributions of the covariates themselves. 
% allows the analysis of such data where the researcher suspects latent categories among the observations. 


\subsection{Mixtures of Nonparametric Regressions}

Both \cite{xiang}, and \cite{huang} have discussed mixtures of nonparametric regressions. The model of Xiang \& Yao estimates mixing proportions and the variance of each component as constants, while allowing the mean of each component to be a nonparametric function of the data. Huang et al., by contrast, propose a model where mixing proportions, mean and variance within each mixture component are all estimated nonparametrically.

Various attempts have been made at generalizing the above approaches to include linear effects, i.e., to model mixtures of partially linear models (PLMs) or generalized additive models (GAMs). \cite{wu} propose a structure for estimating mixtures of PLMs with a univariate nonparametric effect and arbitrary linear effects per component. Most recently, \cite{zhangpan} extended this model to accept arbitrary nonparametric effects.



% \subsection{Mixture Models with Monotone Shape Constraints}

Within the smaller subset of mixtures of non-parametric regressions, one may frequently encounter situations in which one would like to place shape constraints on some, or each, of the components in our mixture model. There has been relatively little previous work in the modelling of mixtures of specifically monotone nonparametric regressions, with the publication by Zhang et al. standing out as the only treatment of this particular issue. 

% A common such shape constraint is the monotonicity constraint, which ensures that in the regression model \( E(Y|X) = f(X) \), the function $f(X)$ is either non-increasing or non-decreasing over the range of $X$. 

\section{An Overview of Contributing Models and Estimators}

\subsection{Mixture Models and the EM Algorithm}

\subsubsection{General Finite Mixture Models}

At its most basic, a finite mixture of $K$ distributions for some positive integer $K$ can be represented by its additive distribution (\ref{genmix}):

\begin{equation} \label{genmix}
  p(X) \ =\ \sum_{k=1}^{K} \pi_k \cdot p(X | \theta_k)
\end{equation}

where the distribution of each component $k$ is parametrized by some set of parameters $\theta_k$, and the number of observations generated by component $k$ is proportionate to $\pi_k$. All $\pi_k \in (0,1)$, and $\sum_{k=1}^{K}\pi_k = 1$. We assume that each observation generated by the mixture is generated uniquely by one component, such that if we are given some number $n$ of observed values $X_1,...,X_n$, we can denote their categorization within the components of a mixture by a latent variable $\mathcal{L}$ such that $\mathcal{L}_i \in \{1,..,K\}$ for all $i \in \{1,...,n\}$. Then, the distribution of $\mathcal{L}$ can be denoted by equation \ref{latentdist}, and the additive distribution in \ref{genmix} can be deconstructed to the conditional distribution in equation \ref{genmixcond}.

\begin{equation} \label{latentdist}
  p(\mathcal{L} = k) \ =\ \pi_k
\end{equation}

\begin{equation} \label{genmixcond}
  p(X | \mathcal{L} = k) \ =\ p_k(X) \ =\ p(X | \theta_k)
\end{equation}


Typically, however, one is simultaneously estimating the number of components $K$, the mixing proportions $\pi_k$, the parameters $\theta_k$, and the latent vector $\mathcal{L}$, at which point it becomes more useful to consider $\boldsymbol{\mathcal{L}}$ an $n \times k$ matrix of sequentially updated probabilities respresenting the probability of observation $n$ having been generated by component $k$. $\boldsymbol{\mathcal{L}}$ must then meet the condition that $\sum_{k=1}^{K}\boldsymbol{\mathcal{L}}_{ik} = 1$ for all $i \in \{1,...,n\}$.

% Given some number $n$ of observed values $X_1,...,X_n$ and a latent, categorical variable $\mathcal{L}$ such that $\mathcal{L}_i \in \{1,..,K\}$ for all $i \in {1,...,n}$ and some integer $K \in \mathbb{R}^1$, a general mixture model has the following structure:
% 
% \begin{equation} \label{genmixlatent}
%   p(x) \ =\ \sum_{k=1}^{K} \pi_k \cdot p(x | \mathcal{L}_k)
% \end{equation}

\subsubsection{Finite Mixtures of Regressions}

If one further specifies the distributions $p_k()$ in equation \ref{genmix} as being regression functions, one is left with a finite mixture of regressions. The structure of such a mixture can be described without specifying the exact form of either the regression model $Y = f(\cdot|\vec{X}) + \epsilon$ or the distribution of $E(Y|\vec{X})$. Suppose we observe, instead of univariate $X$, $Y_i,...,Y_n$ and associated $\vec{X}_i,...,\vec{X}_n$. As before, we assume that each observed set $(Y_i, \vec{X_i})$ belongs to one of $\{1,...,k\}$ unobserved components, for some positive integer $k$, and we denote this by a matrix of probabilities $\boldsymbol{\mathcal{L}}$.

% We can assume some vector of regression model parameters $\theta$ and write the likelihood of the mixture of regressions as such:
% 
% \begin{equation} \label{mixlik}
%   L(\pi, \Theta) \ =\ \prod_{i=1}^n \sum_{j=1}^k \pi_j p_j(y_i\ |\ \vec{x_i},\ \Theta_j)
% \end{equation}
% 
% 
% where $\pi_j$ is the prior probability of component $j$, and $p_j$ is the density of $f(Y)$ at $y_i$ given observed $x_i$ and $\Theta_j$ for component $j$. When the likelihood is maximized and parameters are estimated, the model provides the following:
% 
% \begin{enumerate}[noitemsep] 
%   \item An $n \ \times\ k$ matrix $\mathcal{L}$ representing the posterior probability of each $(Y_i, \vec{X_i})$ belonging to each of $K$ components. 
%   \item A vector $\pi_1,...,\pi_k$ of prior probabilities representing the mixing proportions of each component in the larger mixture model
%   \item A set of parameters $\Theta_k$ for each regression component $k$ 
% \end{enumerate}

Thus equation \ref{genmix} becomes equation \ref{regmix}, in which the distribution of $Y$ is conditioned on the associated covariates $\vec{X}$. The likelihood of this model is written out in equation \ref{mixlike}.

\begin{equation} \label{regmix}
  p(Y) \ =\ \sum_{k=1}^{K}\pi_k p(Y = y\ |\ X = x, \theta_k)
\end{equation}

\begin{equation} \label{mixlike}
   L(\vec{\pi}, \vec{\theta}) \ =\ \prod_{i=1}^n \sum_{k=1}^{K} \pi_k p_k(y_i\ |\ \vec{x_i},\ \theta_k)
\end{equation}


\subsubsection{The EM Algorithm}

The EM algorithm is a general method for discovering the parameter estimates that maximize the likelihood of a model which incorporates unobserved variables representing latent clusters. If we denote such a problem as incorporating observations $x_1, ..., x_n$ and associated latent variable $z_1, ..., z_n$ where $z_i \in \{1,...,k\}$, then the EM algorithm allows us to maximize equation \ref{emlike}, which is the general likelihood of a single observation $x$. Note that this equation already appears quite similar to the likelihood of a mixture model. A set of prior probabilities $\pi_k$ is not required, as here we have assumed that each $x_i$ has a \emph{true} cluster assignment, also known as ``hard'' assignment.

\begin{equation} \label{emlike}
   \ell(\theta) \ =\ log \sum_{k=1}^{K} p_k(X=x, Z=z | \theta)
\end{equation}

The EM algorithm allows us to avoid determining the maximum of equation \ref{emlike} directly, and instead allows us to iteratively maximize a lower bound of the log-likelihood. To demonstrate this, we introduce an arbitrary distribution over $Z$ called $q(Z)$. This allows us to reformulate the likelihood equation as below:

\begin{equation} \label{emlikeqz}
   \ell(\theta) \ =\ log \sum_{k=1}^{K}q(Z) \frac{p_k(X=x, Z=z | \theta)}{q(Z)}
\end{equation}

Since the log-likelihood function is concave, Jensen's inequality (equation \ref{jenin}, \cite{jensen}) applies, which specifies that the expectation over a convex function of $X$ is greater or equal to the convex function of the expectation. This gives us the inequality in equation \ref{emlikejensen}.

\begin{equation} \label{jenin}
   E(f(X)) \geq f(E(X)) \text{, for convex function $f()$}
\end{equation}

\begin{equation} \label{emlikejensen}
   log \sum_{k=1}^{K}q(Z) \frac{p_k(X=x, Z=z | \theta)}{q(Z)} \ \geq\ \sum_{k=1}^{K}q(Z) log \frac{p_k(X=x, Z=z | \theta)}{q(Z)}
\end{equation}

The function $J() = \sum_{k=1}^{K}q(Z) log \frac{p_k(X=x, Z=z | \theta)}{q(Z)}$ therefore serves as a lower bound to the likelihood.

[THIS NEEDS TO BE DEVELOPED]

% the EM algo reaches a local maximum. that's why we run several times.

% The EM algorithm for a mixture model


\subsubsection{Model-Based Generation and Prediction in Mixtures of Regressions} \label{prediction}

In typical mixture model frameworks, the model estimate permits the calculation of unconditioned joint distributions of the data and marginal distributions of any given variable. By extension, such typical models permit the generation of new pseudo-observations that conform with the model structure. The mixture of regressions model does not permit this type of generativity unless the distribution of the covariates is known \emph{a priori}, since the distribution of the covariates is not estimated as a part of the model. The mixture of regressions model thus requires a covariate vector $\vec{x}$ in order to produce the marginal distribution of $Y$.

Similarly, whereas typical mixture models can classify new observations by summing over the product of the mixture prior and the density at the new observed data (\ref{typ_mix}), a mixture of regressions model must classify with a bayesian approach of summing over the likelihood given the observed data (\ref{reg_mix}).

\begin{equation} \label{typ_mix}
  p(Y = y) \ =\ \sum_{k=1}^{K}\pi_k p_k(\vec{Y} = \vec{y}\ | \theta_k)
\end{equation}

\begin{equation} \label{reg_mix}
  p(Z = j | X = x, \theta) \ =\ \frac{\pi_j p_j(y \ | \ X = x, \theta_j)}{\sum_{k=1}^{K}\pi_k p_k(y \ | \ X = x, \theta_k)}
\end{equation}

This is simply the application of Bayes' theorem (\ref{bayes}), where the densities of $y | X = x$ for each component $k$ are multiplied by the uninformed priors $\pi_k$ and normalized over the marginal distribution of $y | X = x$.

\begin{equation} \label{bayes}
  p(\theta | X) \ =\ \frac{p(X|\theta)p(\theta)}{\int_{\theta}p(X|\theta)p(\theta)}
\end{equation}


% It should be noted that the model is NOT generative, that is, there is no distribution assumed for $X$ and therefore no reasonably probabalistic way to determine a marginal distribution of $Y$ without an observed $X$.


\subsection{Partially Linear Models}

\subsubsection{Additive Linear Models} \label{gams}

The general partial linear model is an additive regression model with some finite combination of linear and non-linear components, which can be denoted thus:

\begin{equation} \label{partlin}
  Y = \sum_{h=1}^{p} g_{h} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{j} X_{j} \ +\ \epsilon
\end{equation}

where the model has $h$ non-linear covariates and $j$ linear covariates, where each $g_{h}()$ is some nonparametric function of $Z_{h}$, and where $\epsilon$ is a random variable with mean $0$.

This model overlaps broadly with Generalized Additive Models (GAMs), but differs critically in that we place no restriction on the smoothness of the non-linear components. GAMs are, however, instructive in that they are partly motivated by the difficulty in estimating non-additive, non-parametric models, and the additive structure of our partially linear model is similarly motivated.

% More specifically, as pointed out by \cite{hastiegam}, 

[THIS NEEDS TO BE DEVELOPED]

% For example, \cite{guangcheng} show that, for the model estimated in section \ref{plmmc}, the difference between the multivariate and additive estimate: 
% where it is shown that estimate \ref{partlinloss} is a consistent estimator of $\{\vec{\beta}, g_1,...,g_p\}$ 

% \subsubsection{The Backfitting Algorithm}
% 
% GAMs are additionally instructive in that they can be estimated by the backfitting algorithm (\cite{hastiegam}). Indeed, the backfitting algorithm -- DESCRIBE -- and has the following properties: DESCRIBE
% 
% and involves sequentially updating the linear and non-linear components of the partial linear model in 


\subsubsection{Partially Linear Models with Monotone Constraints} \label{plmmc}

The partial linear model that we apply in the proposed model of this paper can be denoted thus:


\begin{equation} \label{partlinmono}
  Y = \sum_{h=1}^{p} g_{h} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{j} X_{j} \ +\ \epsilon
\end{equation}


where the model has $h$ non-linear covariates with monotone shape constraints and $j$ linear covariates, and where $\epsilon \sim Normal(0, \sigma^2)$. The parameters $\vec{\beta}$ and the functions $g_1(\cdot),...g_p(\cdot)$ are determined as the minimizers of the quadratic loss function, shown in equation \ref{partlinloss}. The estimation of the functions $g_1(\cdot),...g_p(\cdot)$ is a problem of additive isotonic regression, discussed in the next subsection.

\begin{equation} \label{partlinloss}
  \{\hat{\vec\beta}, \hat{g}_1,...,\hat{g}_p\} = \underset{\vec{\beta}, g_1:g_p}{\operatorname{argmin}} \sum_{i=1}^{n} (y_i - \sum_{h=1}^{p} g_{h} (z_{ih}) \ -\  \sum_{j=1}^{q} \beta_{j} x_{ij})^2
\end{equation}


The MLE of the entire partial linear model is obtained via the backfitting algorithm, iterating through the two-step process (\ref{backfit1}, \ref{backfit2}) until convergence.

\begin{equation} \label{backfit1}
  (I) \ \ \ \ \ \ \ \ \ \{\hat{g}_1,...,\hat{g}_p\} = \underset{g_1:g_p}{\operatorname{argmin}} \sum_{i=1}^n\left(y_i-\sum_{j=1}^{q} \beta_{j} x_{ij}-\sum_{h=1}^{p} g_{h} (z_{ih})\right) \\
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  holding \ \ \ \vec\beta \ \ \ fixed
\end{equation}


\begin{equation} \label{backfit2}
  (II) \ \ \ \ \ \ \ \ \ \hat{\vec{\beta}} = \underset{\vec{\beta}}{\operatorname{argmin}} \sum_{i=1}^n\left(y_i-\sum_{h=1}^{p} g_{h} (z_{ih}) - \sum_{j=1}^{q} \beta_{j} x_{ij}\right) \\
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  holding \ \ \ \{g_1,...,g_p\} \ \ \ fixed
\end{equation}

This model and estimator is thoroughly explored by \cite{guangcheng}, where it is shown that, under certain conditions (see section \ref{abplmmc}), $\hat{\beta}_n$ is $\sqrt{n}$-consistent (\ref{asymbeta}), while the estimates $\{\hat{g}_1,...,\hat{g}_p\}$ converge in distribution as to a two-sided brownian motion plus a parabola (\ref{asymg}). 

\begin{equation} \label{asymg}
\begin{aligned}
  n^{1/3}\frac{(2p_{Z_h}(z_h))^{1/3}}{\sigma^{2/3}g_h(z_h)^{1/3}}[\hat{g}_h(z_h) - g_h(z_h)] &\xrightarrow{d} GCM(Z(t) + t^2) \\ 
  \text{where } p_{Z_h}(z_h) &\text{ is the density of $Z_h$ evaluated at $z_h$,} \\
  GCM(Z(t)) &\text{ is the greatest convex minorant of } Z(t) \\
  \text{and } Z(t) &\text{ is a two-sided Brownian motion} 
\end{aligned}
\end{equation}

\begin{equation} \label{asymbeta}
\begin{aligned}
  \sqrt{n}(\hat{\beta}_n - \beta_0) &\xrightarrow{d} N(0, \Sigma) \\
  \text{where } \Sigma &= \sigma^2[E(X - \sum_{h=1}^{p}E(X|Z_h))^{\otimes 2}]^{-1} \\
  \text{and } \sigma^2 &= Var(\epsilon)
\end{aligned}
\end{equation}

The asymptotic distribution of $\hat{\beta}$ evidently has a larger variance than in the case of the ordinary least squares estimate. As Cheng points out, this can be considered the cost of including non-parametric terms in the regression model. The rate of convergence for the non-parametric terms is slower than that of the linear terms, as expected, but significantly faster than would be the case if the nonparametric terms were multivariate monotone rather than univariate monotone and additive.


\subsection{Isotonic Regression}

\subsubsection{Univariate Isotonic Regression}
At the most basic level, with univariate $x$ and $y$ and a simple ordering amongst $x$ such that \( x_{1} \leq x_{2} \leq ... \leq x_{n} \) for all \( x_{i} \in X \), isotonic regression determines a non-decreasing function $g(\cdot)$ such that \( g(x_{1}) \leq g(x_{2}) \leq ... \leq g(x_{n}) \) and for which \( \hat{g}(\cdot) = \argmin_{g} \sum_{i=1}^{n}||g(x_{i}) - x_{i}||_{L} \) for some loss function $||\cdot||_{L}$. If observations are weighted, the objective function becomes \( \hat{g}(\cdot) = \argmin_{g} \sum_{i=1}^{n}w_{i}||g(x_{i}) - x_{i}||_{L} \) for weights $w$. In the so-called antitonic case, the function $g(\cdot)$ is non-increasing such that \( g(x_{1}) \geq g(x_{2}) \geq ... \geq g(x_{n}) \). Without loss of generality, from this point on we are only concerned with the non-decreasing case.

If we consider specifically the squared error loss, our risk function and weighted risk function become equations \ref{lseisotonic} and \ref{lseweightedisotonic} respectively. Equation \ref{lseweightedisotonic} can alternately be written explicitly in the form of a min-max formula, as in equation \ref{minmax} (\cite{jordan}), giving it a characterization which facilitates the study of the properties of the estimator $\hat{g}(\cdot)$. Equation \ref{minmax} breaks the function $g(\cdot)$ into non-decreasing ``blocks'', and assigns to each block the weighted mean of the values $x$ contained in that block.


\begin{equation} \label{lseisotonic}
  \hat{g}(\cdot) = \argmin_{g} \sum_{i=1}^{n}(g(x_{i}) - x_{i})^2
\end{equation}

\begin{equation} \label{lseweightedisotonic}
  \hat{g}(\cdot) = \argmin_{g} \sum_{i=1}^{n}w_{i}(g(x_{i}) - x_{i})^2
\end{equation}


\begin{equation} \label{minmax}
  \hat{g}(x_{i}) = \min_{j \geq i} \max_{k \leq j} \frac{\sum_{k=k}^{j}w_{k}x_{k}}{\sum_{k=k}^{j}w_{k}} \text{,   } i = 1,...,n
\end{equation}

One can see from equation \ref{minmax} that determining the estimate $\hat{g}(\cdot)$ for the least square isotonic regression returns a step function, and requires no tuning parameter.

% \subsubsection{Multivariate Isotonic Regression}
% 
% For the sake of completeness, we briefly describe multivariate isotonic regression here. However, we shall see that, as noted in section \ref{gams}, the rate of convergence in multivariate isotonic functions suffers the well-known ``curse of dimensionality'', rendering it of low practical use in instances where the dimensionality of the data is high and the number of observations is not extremely large.




\subsubsection{The Pool Adjacent Violators Algorithm}

A well-known and efficient way to obtain the least square estimate of $g()$ is through the Pool Adjacent Violators Algorithm (PAVA). The PAVA -- for univariate monotone regression (\ref{pava}) -- returns a step-function fit without either having to select a bandwidth or having to set a congergence tolerance parameter. For multivariable monotone regression (\ref{cpav}), one must take a different approach, suggested by Bacchetti, called the Cyclic Pool Adjacent Violators Algorithm (CPAV). Within the CPAV, one iterates through each univariate function sequentially and update univariate monotone functions until convergence, returning the additive model of equation \ref{cpav}.



\begin{equation} \label{pava}
  Y = g(X) \ +\ \epsilon
\end{equation}


\begin{equation} \label{cpav}
  Y = \sum_{h=1}^{p} g_{h} (X_{h}) \ +\ \epsilon
\end{equation}


% Compare our algo (Algo II) to previous algo (Algo I) here.
% 
% Main difference between algorithms: 
% Algo I has non-parametric priors and non-parametric normal variances which are both functions of X (pi_c(x) and sig_c(x) for component c).
% Algo I provides smooth monotonic estimates.
% Algo I is not readily generalizable to partially linear models
% Algo I uses CV to estimate tuning parameters for kernel density estimation in both mixture on non parametrics AND monotone estimate.
% Steps of Algo I: 
% 1. mixture of nonparametric regressions (nonparametric estimate? bandwidth/lambda?)
% 2. monotone constraint 

\section{Proposed Model}


\subsection{Model Definition}

The model proposed in this article has the following structure:

\begin{equation} \label{modstruc}
  Y = 
  \begin{dcases}
    \sum_{h=1}^{p} g_{h1} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{j1} X_{j} \ +\ \epsilon_1 \text{  with probability $\pi_1$; } \\
    \vdots \\
    \sum_{h=1}^{p} g_{hk} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{jk} X_{j} \ +\ \epsilon_k \text{  with probability $\pi_k$; }\\
  \end{dcases}
\end{equation}

where $\pi_k$ represents the prior probability of mixture component $k$; $g_{hk}(\cdot)$ represents the monotone function of variable $h$ within component $k$; $\beta_{jk}$ represents the linear effect of variable $j$ within mixture component $k$; and $\epsilon_k$ represents the error associated with component $k$. All $\epsilon_k$ are assumed to be normally distributed with mean $0$, and are assumed to be independent of the covariates $(\boldsymbol{X}, \boldsymbol{Z})$ . All $\pi_k$ are assumed to be unknown constants, and all $\pi_k \in (0,1)$ such that $\sum^{K} \pi_k = 1$.

There is no requirement that the $K$ regression functions in model \ref{modstruc} be identical. Specifically, $g_{hk}()$ for any $h \in p$ and any $k \in K$ can be set as monotone non-increasing, monotone non-decreasing, or absent, regardless of the other $g_{hk}$. Likewise, the number of linear effects $\beta_{j}$, including the intercept $\beta_0$, need not be same across different components $k$.


\subsection{Model Estimation}

The proposed model is obtained from a series of nested, iterative algorithms, described below. Algorithm 1 describes the EM algorithm for fitting mixture priors and observation posteriors. Algorithm 2 describes the weighted partial linear regression for the fitting of each component within each M-step of the EM algorithm. Algorithm 3 describes the weighted, cyclic pool adjacent violators algorithm for cases where there is more than one monotone function fit within a single partial linear regression. Algorithm 4 describes the weighted pool adjacent violators algorithm for fitting a single monotone regression. In all cases, convergence thresholds are set by the user.

\begin{algorithm}[H]
\SetAlgoLined
\KwData{\;
  $x$ — an \(n \times p\) matrix (independent variables with no shape constraint)\;
  $z$ — an \(n \times q\) matrix (independent variables with monotone shape constraint)\;
  $y$ — an \(n \times 1\) matrix (dependent variable)\;
  $k$ — a positive integer representing the number of categories of latent variable L\;
}
\KwResult{\;
  \( \mathcal{L} \) — an \(n \times k\) matrix representing the posterior probability of observation \(i = 1,...,n\) belonging to latent category \(j = 1,...,k\). Additionally, for all \(i = 1,...,n\) and \(j = 1,...,k\), \( \mathcal{L}_{ij} \) is a real number in the range \([0,1]\), and \( \sum_{j=1}^{k} \mathcal{L}_{ij} = 1 \) \;
  $\vec{\pi}$ — a vector $\pi_1,...,\pi_k$ of prior probabilities representing the mixing proportions of each component in the larger mixture model \;
  $\vec{\Theta}$ — a set of parameters $\Theta_k$ for each regression component $k$ \;
}
Set iteration index $d \leftarrow 1$ \;
 \For{\( i \in 1,...n \)}{
 With uniform probability across $k$, assign one of the elements of \( [\mathcal{L}_{i1}, ..., \mathcal{L}_{ik}] \) to 1 and all other to 0, such that \( \mathcal{L}_{i} = [0,..., 1, ...,0] \) \;}
 \While{algorithm is not converged}{
  % M-step:\;
  \For{\(j \in 1,...,k\)}
  {Set prior mixture proportion \( \displaystyle{\pi_{j}^{(d)} \leftarrow \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}_{ij}^{(d-1)}} \) \;
Set weighted partial linear model regression parameters such that \( \displaystyle{[\hat{\beta}_{j}, \hat{g}_{j}]^{(d)} \leftarrow \argmin_{\beta, g} \sum_{i=1}^{n}\mathcal{L}_{ij}^{(d-1)}(y - x\beta_{j} - g_{j}(z))^{2} } \) (See Algorithm 2, WPLR)\;}  

  % E-step:\;
  \For{\(i \in 1,...,n\)}{
    \For{\(j \in 1,...,k\)}
    {Set \( \mathcal{L}_{ij}^{(d)} \leftarrow \pi_{j}^{(d)}p(y_{i} | x_{i}, \beta_{j}^{(d)}, g_{j}^{(d)}(z_{i})) \), where \(p(y_{i} | x_{i}, \beta_{j}^{(d)}, g_{j}^{(d)}(z_{i}))\) is the density of a $Normal$ distribution with \(\mu = x_{i}\beta_{j}^{(d)} + g_{j}^{(d)}(z_{i})\) and \(\sigma = \sqrt{\frac{\sum w_{i}r^{2}/\bar{w}}{n-rk(X)}}\) }
    % sigma = sqrt(sum(wates * (resids)^2 / mean(wates))/ (nrow(x)-qr(x)$rank))
  Normalize the posterior probabilities \( [\mathcal{L}_{i1},...,\mathcal{L}_{ik}]^{(d)} \) such that \( \displaystyle{\sum_{j=1}^{k}\mathcal{L}_{ij}^{(d)} = 1} \)\;
  }
  
  $d = d + 1$\;
  
 }
 \caption{EM algorithm for Finite Mixtures of Regressions}
\end{algorithm}


\begin{algorithm}[H]
\SetAlgoLined
\KwData{\;
  $x$ — an \(n \times p\) matrix (independent variables with no shape constraint)\;
  $z$ — an \(n \times q\) matrix (independent variables with monotone shape constraint)\;
  $y$ — an \(n \times 1\) matrix (dependent variable)\;
  $w$ — an \(n \times 1\) matrix (observation weights)\;
}
\KwResult{\( \hat{\beta},\hat{g_{1}},...,\hat{g_{q}} \) such that \( \displaystyle{[\hat{\beta},\hat{g_{1}},...,\hat{g_{q}}] = \argmin_{\beta, g_{1},...,g_{q}} \sum_{i=1}^{n} w_{i}(y_{i} - \sum_{h=1}^{q}g_{h}(z_{ih}) - x_{i}\beta)^{2}} \)}

 Set iteration index $b \leftarrow 1$\;
 Set \( \displaystyle{\hat{\beta}^{(0)} \leftarrow \beta_{x}} \), where \( \displaystyle{[\beta_{x}, \beta_{z}] = \argmin_{\beta_{x}, \beta_{z}} \sum_{i=1}^{n} w_{i}(y_{i} - z_{i}\beta_{z} - x_{i}\beta_{x})^{2}}\)\;
 \While{algorithm is not converged}{
  \eIf{$z$ is univariate}
  {Set \( \displaystyle{\hat{g}^{(b)} \leftarrow \argmin_{g} \sum_{i=1}^{n} w_{i}([y_{i} - x_{i}\beta^{(b-1)}] - g(z_{i}))^{2}} \) holding \( \beta^{(b-1)} \) fixed. (See Algorithm 4, PAVA)}
  {Set \( \displaystyle{\sum_{h=1}^{q}\hat{g}_{h}^{(b)} \leftarrow \argmin_{g_{1},...,g_{q}} \sum_{i=1}^{n} w_{i}([y_{i} - x_{i}\beta^{(b-1)}] - \sum_{h=1}^{q}g_{h}(z_{ih}))^{2}} \) holding \( \beta^{(b-1)} \) fixed. (See Algorithm 3, CPAV)}
  Set \( \displaystyle{\hat{\beta}^{(b)} = \argmin_{\beta} \sum_{i=1}^{n} w_{i}([y_{i} - g^{(b)}(z_{i})] - x_{i}\beta)^{2}} \) holding \( g^{(b)} \) fixed.\;
  $b = b + 1$\;
  
 }
 \caption{Weighted Partial Linear Regression}
\end{algorithm}



\begin{algorithm}[H]
\SetAlgoLined
\KwData{\;
  $x$ — an \(n \times q\) matrix (independent variables)\;
  $z$ — an \(n \times 1\) vector (observation weights)\;
  $y$ — an \(n \times 1\) vector (dependent variable)\;
}
\KwResult{A set of non-decreasing functions \(\hat{f_1},...\hat{f_q}\) such that \( \displaystyle{[\hat{f_1},...\hat{f_q}] = \argmin_{f_1,...f_q]} \sum_{i=1}^{n} w_{i}(y_{i} - \sum_{h=1}^{q}f_h(x_{ih}))^2}  \)}

 
 Set iteration index $m \leftarrow 1$ \;
 \While{algorithm is not converged}{
  \For{\(h \in 1,...,q\)}{
  Set \( \displaystyle{\hat{f_h} \leftarrow \argmin_{f_h} \sum_{i=1}^{n} w_{i}([y_{i} - \sum_{\substack{j=1 \\ j\neq h}}^{q}f_j(x_{ih})] - f_h(x_{ih}))^2}  \) holding all $f_j(), j \neq h$ fixed (See Algorithm 4, PAVA) \;
  }
  
  $m = m + 1$\;
  
 }
 \caption{Weighted Cyclic Pool Adjacent Violators Algorithm}
\end{algorithm}




\begin{algorithm}[H]
\SetAlgoLined
\KwData{\;
  $x$ — an \(n \times 1\) vector (independent variable)\;
  $w$ — an \(n \times 1\) vector (observation weights)\;
  $y$ — an \(n \times 1\) vector (dependent variable)\;
}
\KwResult{A non-decreasing function \( \displaystyle{\hat{f}(\cdot) = \argmin_{f} \sum_{i=1}^{n} w_{i}(y_{i} - f(x_{i}))^2}  \)}
 
 Set iteration index $l \leftarrow 0$ \;
 Set blocks $r \leftarrow 1, ..., B$ where at $ l = 0$, $B = n$ \;
 Set $f^{(l=0)}(x_i) \leftarrow y_i$ \;
 Set initial block membership $f^{(l=0)}(x_i) \in r_i$ \;
 \While{any $f_{r}^{l}(x) \geq f_{r+1}^{l}(x)$}{
  \If{$f_{r}^{l}(x) > f_{r+1}^{l}(x)$}
  {Merge blocks $r$ and $r + 1$}
  Solve $f_{r}^{(l)}()$ for block $r$ as the weighted mean, i.e, \(\displaystyle{f_{r}() = \frac{1}{\sum_{i=1}^{n}w_i}\sum_{i=1}^{n} w_i(y_i)} \) for all $x \in r$\;
  
  $l = l + 1$\;
  
 }
 \caption{Weighted Pool Adjacent Violators Algorithm}
\end{algorithm}

%% Original algorithm without separation of component algorithms.
% We are given
% \begin{itemize}
%   \item[]	$x$ — an \(n \times p\) matrix (independent variables with no shape constraint)
%   \item[]	$z$ — an \(n \times q\) matrix (independent variables with monotone shape constraint)
%   \item[]	$y$ — an \(n \times 1\) matrix (dependent variable)
%   \item[]	$k$ — a positive integer representing the number of categories of latent variable L
%   \item[] \( \mathcal{L} \) — an \(n \times k\) matrix representing the posterior probability of observation \(i = 1,...,n\) belonging to latent category \(j = 1,...,k\). Additionally, for all \(i = 1,...,n\) and \(j = 1,...,k\), \( \mathcal{L}_{ij} \) is a real number in the range \([0,1]\), and \( \sum_{j=1}^{k} \mathcal{L}_{ij} = 1 \)
% \end{itemize}
% 
% \begin{enumerate}
%   \item For each \( i \) = 1,...n, set the vector \( \mathcal{L}_{i} \) = \( [\mathcal{L}_{i1},...,\mathcal{L}_{ik}] \) as an instance of a multinomial distribution with k=k and n=1. I.e., randomly assign one of the vector elements of \( [\mathcal{L}_{i1}, ..., \mathcal{L}_{ik}] \) to 1 and all other lik to 0, such that each \( \mathcal{L}_{i} = [0,..., 1, ...,0] \) where 1 is at a random index.
%   \item In each iteration $(d)$ until convergence:
%     \begin{enumerate}
%       \item M-step
%         \begin{enumerate}
%           \item Calculate prior mixture proportions \([p_{1},...,p_{k}]^{(d)}\) such that \( \displaystyle{p_{k}^{(d)} = \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}_{ik}^{(d-1)}} \)
%           \item Calculate weighted partial linear model regression parameters; For each \( j = 1,..., k \), estimate \( \displaystyle{[\hat{\beta}_{j}, \hat{g}_{j}]^{(d)} = \argmin_{\beta, g} \sum_{i=1}^{n}\mathcal{L}_{ij}^{(d-1)}(y - x\beta_{j} - g_{j}(z))^{2} } \) by iterating the following steps until convergence. For each iteration $b$:
%           \begin{enumerate}
%             \item Weighted PAVA/CPAV: If $z$ is univariate, i.e., an $n \times 1$ matrix, use PAVA to estimate \( \displaystyle{\hat{g}_{j}^{(b)} = \argmin_{g} \sum_{i=1}^{n}\mathcal{L}_{ij}^{(d-1)}([y - x\beta_{j}^{(b-1)}] - g_{j}(z))^{2}} \) holding \( \beta_{j}^{(b-1)} \) fixed. If $z$ is multivariate, use CPAV algorithm*.
%             \item Weighted Linear Model: Estimate \( \displaystyle{\hat{\beta}_{j}^{(b)} = \argmin_{\beta} \sum_{i=1}^{n}\mathcal{L}_{ij}^{(d-1)}([y - g_{j}^{(b)}(z)] - x\beta_{j})^{2}} \) holding \( g_{j}^{(b)} \) fixed.
%           \end{enumerate}
%         \end{enumerate}
%       \item E-step
%         \begin{enumerate}
%           \item Given $k$ regression functions \( [E(Y | X; Z) = X\beta_{k} + g_{k}(Z)]^{(d)} \), for \( i = 1,...,n \), calculate \( [\mathcal{L}_{i1},...,\mathcal{L}_{ik}]^{(d)} \) as \( [p_{1}^{(d)}P(y_{i} | x_{i}, \beta_{1}^{(d)}, g_{1}^{(d)}(z_{i})),...,p_{k}^{(d)}P(y_{i} | x_{i}, \beta_{k}^{(d)}, g_{k}^{(d)}(z_{i}))] \)
%           \item For each \( i = 1,...,n \), normalize the posterior probabilities \( [\mathcal{L}_{i1},...,\mathcal{L}_{ik}]^{(d)} \) such that \( \displaystyle{\sum_{j=1}^{k}\mathcal{L}_{ij}^{(d)} = 1} \)
%         \end{enumerate}
%     \end{enumerate}
% \end{enumerate}

\subsection{Asymptotic Properties of Model and Estimator}

[Discuss the asymptotic standard error estimates of parameters and functions.]


\subsection{Confidence Intervals via Bootstrapping}

We opt to implement and demonstrate the proposed model with confidence intervals calculated by bootstrapping. In this regard, we use and compare two approaches in the remainder of this paper -- the ordinary, or traditional bootstrap, and a component-wise bootstrap conditioned on the posterior matrix of the first model. Each approach has advantages and disadvantages, which we discuss in the following sections.

\subsubsection{Ordinary Bootstrap}

Although the ordinary bootstrap applied to mixture models delivers all the typical advantages of bootstrapping for determining parameter confidence intervals, it runs into the well-known label-switching problem inherent in clustering algorithms. Specifically, when the mixture model is run multiple times, as with the bootstrap, the labeling of the resulting clusters is random and there is no guarantee, nor even a higher probability, that clusters with the same labels in two different models will represent the same underlying mixture component.

One partial solution to this problem, which we apply in the current paper, is to select the estimated parameters and non-parametric functions of the complete model as the starting values when estimating each of the bootstrapped models. This results in both a decreased computational burden in calculating the bootstrapped estimates, and the bypassing of the label-switching problem, since bootstrap-estimated components will likely share the same label as the component from which their starting values were drawn.

The disadvantage of this approach is that it ignores the possibility of bootstrap-estimated parameters reaching different likelihood maxima as a result of having had random as opposed to fixed starting values. For this reason, this approach underestimates the parameter variance. Moreover, if the first model which one fit returned estimates from a local likelihood maximum, subsequent bootstrap estimates may likewise be caught in the same local likelihood maximum. Parameter distributions from such a bootstrap might then be heavily biased.

One obvious approach to the problem of label-switching would be to run the ordinary bootstrap with random starting values and to treat the bootstrap-estimated parameters and non-parametric functions as ``observations''. We would then have a set of observations with latent class membership, and moreover, a known number of classes $k$, which is in fact precisely the problem structure which typically motivates a mixture model. Thus, we would run yet another mixture model on these parameters to determine a probabalistic interpretation of the distributions of parameters per component, as well as an estimate of the posterior probability of each parameter set belonging to each component. The implementation of such a hypothetical model may be difficult given that each observed parameter set includes both point-estimates and step functions, and we therefore leave this task for future research.

\subsubsection{Conditional Bootstrap}

An alternative to the ordinary bootstrap is what we shall call in this section the \emph{conditional bootstrap}. Instead of resampling with replacement from the data and rebuilding the entire mixture model from the beginning for each iteration of the bootstrap, the conditional bootstrap takes the posterior estimates for each observation from the initial model as fixed weights in the calculation of the mixture subcomponents. The data for the conditional bootstrap are resampled from the complete dataset as usual.

The conditional bootstrap can thus be thought of as the ordinary bootstrap for the mixture components, conditioned on the weights being equal to the point-estimate posteriors from the initial model. There are two clear advantages to this approach: First, the label-switching problem is entirely avoided, as the component labels are \emph{a priori} known. Second, the bootstrap model refitting is extremely computationally efficient, as the original mixture model fitting process is run only once.

The disadvantage to this approach is, of course, that the uncertainty in the posterior estimates is entirely ignored in the construction of the final parameter distributions. Moreover, depending on the data, the posterior estimates may themselves be highly unstable. Thus, this approach underestimates the variance of the parameter estimates, and the degree to which it underestimates the variance is unknown. Moreover, if the initial model from which the posterior point-estimates are drawn terminated in a local maximum, the results of the conditional bootstrap may be highly biased and misleading.

% \subsection{Model Selection} TODO include a model selection section?



\subsection{Computational Complexity of Estimator} \label{seccomplex}

A common concern amongst users of this algorithm will be the speed of the estimator, and by extension, the computational complexity of the estimator. It is not possible to specify exact $O(\cdot)$ notation given that the number of iterations within each optimization step of the algorithm is problem-dependent. However, given the generalized nature of the algorithm, we can compare its complexity empirically for different types and numbers of features within the sub-component regression models by running timed applications on pseudo-data. In the benchmarking table below, we compare the run-time of the estimation of 4 models -- with one monotone covariate and no linear effects; with two monotone covariates and no linear effects; with one monotone covariate and four linear effects; with two monotone covariates and three linear effects -- and all with the number of components, 4, known \emph{a priori}.

% \begin{equation} \label{modstruc}
%   Y = 
%   \begin{dcases}
%     3 + X_1 + N(0,3) \text{  with probability $0.25$; } \\
%     X_1^{3} + N(0,4) \text{  with probability $0.25$; }\\
%     \sum_{h=1}^{p} g_{hk} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{jk} X_{j} \ +\ \epsilon_k \text{  with probability $\pi_k$; }\\
%     \sum_{h=1}^{p} g_{hk} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{jk} X_{j} \ +\ \epsilon_k \text{  with probability $\pi_k$; }\\
%   \end{dcases}
% \end{equation}

% TIMED COMPLEXITY: here.
\begin{minipage}{0.8\textwidth}
<<complexity_benchmarking, eval=TRUE, echo=FALSE>>=

# data with 4 latent categories

################
X <- cbind(
  runif(1000, -5, 5),
  runif(1000, -10, 10),
  runif(1000, -100, 100),
  runif(1000, -100, 100),
  runif(1000, -100, 100)
)
################

# print benchmarks (all excluding intercept)
benchmark(
"Univariate monotone without linear effects" = {
  Y1 <- (X[1:250,1])+3 + rnorm(250, 0, 3) # component 1
  Y2 <- (X[251:500,1])^3 + rnorm(250, 0, 4) # component 2
  Y3 <- 2*((X[501:750,1])+5) + rnorm(250, 0, 3) # component 3
  Y4 <- 2*((X[751:1000,1])-5) + rnorm(250, 0, 4) # component 4
  df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
  names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
  ###
  m1 <- flexmix(Y ~ X1 -1, data = df_3, k = 4, 
                model = mono_reg(mon_inc_names = "X1"))
},
"Bivariate monotone without linear effects" = {
  Y1 <- (X[1:250,1])+3 + 1.5*X[1:250,2] + 
    rnorm(250, 0, 3) # component 1
  Y2 <- (X[251:500,1])^3 + 3*X[251:500,2] + 
    rnorm(250, 0, 4) # component 2
  Y3 <- 2*((X[501:750,1])+5) + 5*X[501:750,2] + 
    rnorm(250, 0, 3) # component 3
  Y4 <- 2*((X[751:1000,1])-5) + 10*X[751:1000,2] + 
    rnorm(250, 0, 4) # component 4
  df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
  names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
  ###
  m2 <- flexmix(Y ~ X1 + X2 -1, data = df_3, k = 4, 
                model = mono_reg(mon_inc_names = c("X1", "X2"))) # c("X1", "X2")
},
"Univariate monotone with linear effects" = {
  Y1 <- (X[1:250,1])+3 + 1.5*X[1:250,2] - 1.5*X[1:250,3] -
    1*X[1:250,4] + X[1:250,5] + rnorm(250, 0, 3) # component 1
  Y2 <- (X[251:500,1])^3 + 3*X[251:500,2] + 2*X[251:500,3] -
    2*X[251:500,4] + 2*X[251:500,5] + 
    rnorm(250, 0, 4) # component 2
  Y3 <- 2*((X[501:750,1])+5) + 5*X[501:750,2] - 1*X[501:750,3] +
    2*X[501:750,4] + 4*X[501:750,5] + 
    rnorm(250, 0, 3) # component 3
  Y4 <- 2*((X[751:1000,1])-5) + 10*X[751:1000,2] -
    3*X[751:1000,3] - 3*X[751:1000,4] + 3*X[751:1000,5] + 
    rnorm(250, 0, 4) # component 4
  df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
  names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
  ###
  m3 <- flexmix(Y ~ . -1, data = df_3, k = 4, 
                model = mono_reg(mon_inc_names = "X1"))
},
"Bivariate monotone with linear effects" = {
  Y1 <- (X[1:250,1])+3 + 1.5*X[1:250,2] - 1.5*X[1:250,3] -
    1*X[1:250,4] + X[1:250,5] + rnorm(250, 0, 3) # component 1
  Y2 <- (X[251:500,1])^3 + 3*X[251:500,2] + 2*X[251:500,3] -
    2*X[251:500,4] + 2*X[251:500,5] + 
    rnorm(250, 0, 4) # component 2
  Y3 <- 2*((X[501:750,1])+5) + 5*X[501:750,2] - 1*X[501:750,3] +
    2*X[501:750,4] + 4*X[501:750,5] + 
    rnorm(250, 0, 3) # component 3
  Y4 <- 2*((X[751:1000,1])-5) + 10*X[751:1000,2] - 
    3*X[751:1000,3] - 3*X[751:1000,4] + 3*X[751:1000,5] + 
    rnorm(250, 0, 4) # component 4
  df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
  names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
  ###
  m4 <- flexmix(Y ~ . -1, data = df_3, k = 4, 
                model = mono_reg(mon_inc_names = c("X1", "X2"))) # c("X1", "X2")
},
replications = 1,
columns = c("test", "replications", "elapsed", "relative")
)

@
\captionof{figure}[Abbreviated Caption]{In the table above, we see the average elapsed time for four different types of model constructions -- with and without linear effects, and with and without multiple monotone terms.}
\end{minipage}

One can see that -- for a model with 4 latent components -- adding a second monotone nonparametric effect within each component multiplies the computation time approximately 50. This is an indication of the heavy cost of increasing even slightly the dimensionality of the non-parametric estimation within each component model.

By comparison, adding linear effects within the component models comes essentially for free. In fact, the estimation of models with univariate monotone effects and 4 linear effects is \emph{faster} than the complementary model without linear effects. 

\section{Model Applications}
\subsection{Simulated Data}

In this section, we demonstrate the application of the proposed model by fitting it to randomly generated pseudo-data. We begin by modeling 1000 observations generated from 2 latent categories with the following underlying structure:

\begin{align*}
  Y_{1} &= 50 + X^3 + \epsilon_1 \\
  Y_{2} &= -50 + 0.04 \cdot X^5 + 30 \cdot X + \epsilon_2 \\
\end{align*}

where 

\begin{align*}
  \epsilon_1 &\sim N(0,200) \\
  \epsilon_2 &\sim N(0,300) \\
\end{align*}

and

\begin{align*}
  \pi_1 &= 0.65 \\
  \pi_2 &= 0.35 \\
\end{align*}

and

\begin{align*}
  X &\sim Uniform(-10,10) \\
\end{align*}


We proceed to estimate a mixture of univariate regressions (\ref{m2}), with the number of components known \emph{a priori} as 2. The fitted model includes only monotone non-decreasing function of covariate $X$, and no intercept. The regression models are identical for each of the 2 components.

\begin{equation} \label{m2}
  Y = \sum_{k=1}^{2}\pi_k (g_{k} (X) \ +\  \epsilon_k)
\end{equation}

% TODO complete boostrap implementation. Have flexmix bootstrap return model with monotone fit(s) as well as posteriors for a bootstrapped rootogram
\begin{figure}
<<pseudo_mixture_monovar, fig.keep='high', warning=F, echo=FALSE, fig.align='center', eval=TRUE, fig.height=3>>=

# data with 2 latent categories
################
pi1 <- 0.65
pi2 <- 0.35
n <- 1000

Xa <- runif(n*pi1,-10,10) #seq(-10,10, length.out=1001)
Xb <- runif(n*pi2,-10,10) #seq(-10,10, length.out=1001)


Y1t <- 50 + Xa^3 
Y1 <- Y1t + rnorm(n*pi1, 0, 200) # component 1
Y2t <- -50 + 0.04*(Xb)^5 + 30*Xb
Y2 <- Y2t + rnorm(n*pi2, 0, 300) # component 2
cat <- c(rep.int(1, n*pi1), rep.int(2,n*pi2))
# plot(X,Y1, type="p", cex=0.05)
# lines(X,Y2,type="p", cex=0.05)

df_2 <- data.frame(c(Y1, Y2), c(Xa,Xb), c(Y1t, Y2t), cat)
names(df_2) <- c("Y", "X", "Yt", "cat")
################


# build model
m2 <- flexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X"))

# m2step <- stepFlexmix(Y ~ X-1, data = df_2, k = 1:5, model = mono_reg(mon_inc_names = "X"))

# build bootstrapped model
m2boot <- boot(initFlexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X")), R=200, verbose=0, model=TRUE, initialize_solution=TRUE)


# plot fitted model
plot(m2, ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=1)
@
% TODO describe the confidence intervals of the bootstrapped rootogram
\captionof{figure}[Abbrviated Caption]{The rootogram of the two-component mixture model shows the distribution of posterior probabilities with reference to the binary latent variable, for all observations used to fit the model. The model indicates higher confidence in the identification of clusters and the classification of individual observations when the observations accumulate near the limits of the rootogram, at 0 and 1. Conversely, greater mass at the center of the rootogram represents observations that are less confidently classified.}
\end{figure}

\begin{figure}
<<pseudo_mixture_monovar_b, fig.keep='high', warning=F, echo=FALSE, fig.align='center', eval=TRUE, fig.height=3>>=
plot(m2boot@object, boot_CI=build_CI_trad(m2boot), ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=2) + 
  geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
  geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")

# plot(m2boot@object, boot_CI=build_CI_trad(m2boot),  palet="Dark2", root_scale="sqrt", subplot=2) + 
#   geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
#   geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")

@
% TODO add description/explanation of how the model has difficulty estimating the functions at/after the points where the functions cross one another.
\captionof{figure}[Abbrviated Caption]{The estimated monotone functions of the two-component mixture model, with overlaid, dotted black lines representing the true functions. The confidence intervals are generated by 1000 iterations of an ordinary (non-parametric) bootstrap.} \label{fig:pseudo_mixture_monovar_b}
\end{figure}

\begin{figure} 
<<pseudo_mixture_monovar_c, fig.keep='high', warning=F, echo=FALSE, fig.align='center', eval=TRUE, fig.height=3>>=
plot(m2, boot_CI=build_CI_cond(m2, R = 500), ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=2) + 
  geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
  geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")

# plot(m2boot@object, boot_CI=build_CI_trad(m2boot),  palet="Dark2", root_scale="sqrt", subplot=2) + 
#   geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
#   geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")

@
% TODO add description/explanation of how the model has difficulty estimating the functions at/after the points where the functions cross one another.
\captionof{figure}[Abbrviated Caption]{The estimated monotone functions of the two-component mixture model, now with confidence intervals generated via 1000 iterations of the conditional bootstrap.}
\end{figure}

As can be seen from figure (\ref{fig:pseudo_mixture_monovar_b}), the algorithm is more uncertain of the monotone regression shapes where the true data generating functions overlap. 

% Alternately, we can fit mixtures with alternate specifications for each component. In the next demonstration, we model data generated from 2 latent categories with contrary monotonic effects, i.e., one with a monotone non-increasing true function and one with a monotone non-decreasing true function. The data has the following underlying structure:
% 
% \begin{align*}
%   Y_{1} &= X^3 + \epsilon_1 \\
%   Y_{2} &= 100 + 0.02 \cdot X^5 + \epsilon_2 \\
% \end{align*}
% 
% where 
% 
% \begin{align*}
%   \epsilon_1 &\sim N(0,30) \\
%   \epsilon_2 &\sim N(0,20) \\
% \end{align*}
% 
% and
% 
% \begin{align*}
%   \pi_1 &= 0.7 \\
%   \pi_2 &= 0.3 \\
% \end{align*}
% 
% and
% 
% \begin{align*}
%   X &\sim Uniform(-10,10) \\
% \end{align*}
% 
% 
% We proceed to estimate a mixture of univariate regressions (\ref{m2a}), with the number of components known \emph{a priori} as 2. The fitted model includes only monotone non-decreasing function of covariate $X$, and no intercept. The regression models are as follows for each of the 2 components:
% 
% \begin{equation} \label{m2a}
%   Y = 
%   \begin{dcases}
%     \pi_1 (g_{1} (X) \ +\  \epsilon_1) \\
%     \pi_2 (g_{2} (X) \ +\  \epsilon_2)
%   \end{dcases}
% \end{equation}
% 
% where $g_{1}$ is non-decreasing, and $g_{2}$ is non-increasing.
% 
% \begin{minipage}{0.8\textwidth}
% pseudo_mixture_monovar_contrary, fig.keep='high', warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=
% 
% # TODO currently, you cannot specify different models for different components. the larger monoreg mixture object has inc/dec attributes, which it shouldn't -- these should belong to the components.
% 
% 
% # data with 2 latent categories
% ################
% pi1 <- 0.7
% pi2 <- 0.3
% n <- 1000
% 
% Xa <- runif(n*pi1,-10,10) #seq(-10,10, length.out=1001)
% Xb <- runif(n*pi2,-10,10) #seq(-10,10, length.out=1001)
% 
% Y1t <- 50 + -Xa^3 
% Y1 <- Y1t + rnorm(n*pi1, 0, 30) # component 1
% Y2t <- -50 + 0.04*(Xb)^5 + 30*Xb
% Y2 <- Y2t + rnorm(n*pi2, 0, 20) # component 2
% cat <- c(rep.int(1, n*pi1), rep.int(2,n*pi2))
% # plot(X,Y1, type="p", cex=0.05)
% # lines(X,Y2,type="p", cex=0.05)
% 
% df_2 <- data.frame(c(Y1, Y2), c(X,X), c(Y1t, Y2t), cat)
% names(df_2) <- c("Y", "X", "Yt", "cat")
% ################
% 
% 
% # build model
% # TODO mono_reg model is FORCING a linear component here. It should fit a purely nonparametric model when it sees that the monotone indices are all variables minus the intercept.
% m2a <- flexmix( ~ X-1, data = df_2, k = 2, model = list(mono_reg(Y~., mon_inc_names = "X"), mono_reg(Y~., mon_dec_names = "X")) )
% 
% # m2boot <- boot(initFlexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X")), R=10, verbose=1, model=TRUE, initialize_solution=TRUE)
% 
% # plot fitted model
% plot(m2a, ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=1)
% @
% % TODO describe the confidence intervals of the bootstrapped rootogram
% \captionof{figure}[Abbrviated Caption]{The rootogram of the two-component mixture model shows the distribution of posterior probabilities with reference to the binary latent variable, for all observations used to fit the model. The model indicates higher confidence in the identification of clusters and the classification of individual observations when the observations accumulate near the limits of the rootogram, at 0 and 1. Conversely, greater mass at the center of the rootogram represents observations that are less confidently classified.}
% \end{minipage}
% 
% \begin{minipage}{0.8\textwidth}
% pseudo_mixture_monovar_contraryb, fig.keep='high', warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=
% plot(m2a, ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=2) + 
%   geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
%   geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")
% 
% @
% 
% \captionof{figure}[Abbrviated Caption]{The estimated monotone functions of the two-component mixture model, with overlaid, dotted black lines representing the true functions. The confidence intervals are generated by an ordinary (non-parametric) bootstrap.}
% \end{minipage}





We continue the demonstration with the inclusion of linear effects. For the next model, we generate pseudo-data from 4 latent categories with the following underlying structure:


\begin{align*}
  Y_{1} &= 10 + X_1^3 + 1.5\cdot X_2 - 1.5\cdot X_3 - X_4 + X_5 + \epsilon_1 \\
  Y_{2} &= -10 + 25 \cdot X_1 + 3\cdot X_2 + 2\cdot X_3 - 2\cdot X_4 + 2\cdot X_5 + \epsilon_2 \\
  Y_{3} &= -4 + 1.5 \cdot (X_1^3) - 2\cdot X_2 - X_3 + 2\cdot X_4 + 4\cdot X_5 + \epsilon_3 \\
  Y_{4} &= 4 + 0.1 \cdot (X_1^5) - 3\cdot X_2 - 3\cdot X_3 - 3\cdot X_4 + 3\cdot X_5 + \epsilon_4 \\
\end{align*}

where 

\begin{equation*}
\begin{aligned}[c]
  \epsilon_1 &\sim N(0,3) \\
  \epsilon_2 &\sim N(0,10) \\
  \epsilon_3 &\sim N(0,3) \\
  \epsilon_4 &\sim N(0,7) \\
\end{aligned}
\begin{aligned}[c]
  X_1 &\sim Uniform(-5,5) \\
  X_2 &\sim Uniform(-10,10) \\
  X_3 &\sim Uniform(-100,100) \\
  X_4 &\sim Uniform(-100,100) \\
  X_5 &\sim Uniform(-100,100) \\
\end{aligned}
\end{equation*}


We proceed to estimate a mixture of partial linear regressions (\ref{m3}), with the number of components known \emph{a priori} as 4. The fitted model includes one monotone non-decreasing function of covariate $X_1$, an intercept, and a linear effect for each of $X_2,...,X_5$. The regression models are identical for each of the 4 components.

\begin{equation} \label{m3}
  Y = \sum_{k=1}^{4}\pi_k (g_{k} (X_1) \ +\  \beta_{0,k} \ +\ \beta_{1,k}\cdot X_2 \ +\ \beta_{2,k}\cdot X_3 \ +\ \beta_{3,k}\cdot X_4 \ +\ \beta_{4,k}\cdot X_5 \ +\ \epsilon_k)
\end{equation}

% TODO include chart comparing distributions of parameters and g() for increasing n, i.e., n = 400, 800, 2000,

\begin{figure}
<<pseudo_mixture, warning=F, echo=FALSE, fig.align='center', eval=TRUE, fig.height=3>>=

# data with 4 latent categories
################
X <- cbind(
  runif(4000, -5, 5),
  runif(4000, -10, 10),
  runif(4000, -100, 100),
  runif(4000, -100, 100),
  runif(4000, -100, 100)
)


Y1 <- 10 + (X[1:1000,1])^3 + 1.5*X[1:1000,2] - 1.5*X[1:1000,3] - 1*X[1:1000,4] + X[1:1000,5] + rnorm(1000, 0, 150) # component 1
Y2 <- -10 + 40*(X[1001:2000,1]) + 3*X[1001:2000,2] + 2*X[1001:2000,3] - 2*X[1001:2000,4] + 2*X[1001:2000,5] + rnorm(1000, 0, 100) # component 2
Y3 <- -4 + 2*((X[2001:3000,1])^3) - 2*X[2001:3000,2] - 1*X[2001:3000,3] + 2*X[2001:3000,4] + 4*X[2001:3000,5] + rnorm(1000, 0, 70) # component 3
Y4 <- 4 + 0.1*((X[3001:4000,1])^5) - 3*X[3001:4000,2] - 3*X[3001:4000,3] - 3*X[3001:4000,4] + 3*X[3001:4000,5] + rnorm(1000, 0, 80) # component 4

df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
################

X <- seq(-5, 5, length.out=4000)



Y1 <- (X)^3  # component 1
Y2 <- 40*(X)  # component 2
Y3 <- 2*((X)^3)  # component 3
Y4 <- 0.1*((X)^5)  # component 4

df_t <- data.frame(Y1, Y2, Y3, Y4, X)
names(df_t) <- c("Y1","Y2","Y3","Y4", "X1")

###############


# build model
m3 <- flexmix(Y ~ ., data = df_3, k = 4, model = mono_reg(mon_inc_names = "X1"), control = list(minprior = 0.1))

# m3step <- stepFlexmix(Y ~ ., data = df_3, k = 4:6, model = mono_reg(mon_inc_names = "X1"), nrep = 5)

# bootstrap model
m3boot <- boot(initFlexmix(Y ~ ., data = df_3, k = 4, model = mono_reg(mon_inc_names = "X1")), R=100, initialize_solution=TRUE, verbose=0, model=TRUE)

# plot fitted model
plot(m3, ylim=c(-100,100), palet="Dark2", root_scale="sqrt", subplot=1) 

@
\captionof{figure}[Abbrviated Caption]{The rootogram of the four-component mixture model represented by equation \ref{m3}.}
\end{figure}

\begin{figure}
<<pseudo_mixture_b, warning=F, echo=FALSE, fig.align='center', eval=TRUE, fig.height=3>>=

plot(m3boot@object, boot_CI=build_CI_trad(m3boot), ylim=c(-250,250), palet="Dark2", root_scale="sqrt", subplot=2) + 
  geom_line(data = df_t, aes(X1, Y1), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y2), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y3), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y4), linetype="dotted") 

# plot(m2, boot_CI=build_CI_cond(m2, R = 1000), ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=2) + 

@
\captionof{figure}[Abbreviated Caption]{The estimated monotone functions of the four-component mixture model, with overlaid, dotted black lines representing the true functions. The confidence intervals are generated by 1000 iterations of an ordinary (non-parametric) bootstrap.}
\end{figure}


\begin{figure}
<<pseudo_mixture_c, warning=F, echo=FALSE, fig.align='center', eval=TRUE, fig.height=3>>=

plot(m3, boot_CI=build_CI_cond(m3, R=100, CI = 0.95), ylim=c(-250,250), palet="Dark2", root_scale="sqrt", subplot=2) + 
  geom_line(data = df_t, aes(X1, Y1), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y2), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y3), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y4), linetype="dotted") 

@
\captionof{figure}[Abbrviated Caption]{The estimated monotone functions of the two-component mixture model, now with confidence intervals generated via 1000 iterations of the conditional bootstrap.}
\end{figure}

<<pseudo_mixture_d, warning=F, echo=FALSE, fig.align='center', eval=TRUE, fig.height=3, results='asis'>>=

# TODO this is terrible and hacky layout. Figure out a better layout of coefficient confidence intervals

holder <- build_CI_trad(m3boot)


t1 <- kable(CI_table(holder)[[1]], format = "latex", booktabs = TRUE)
t2 <- kable(CI_table(holder)[[2]], format = "latex", booktabs = TRUE)

t3 <- kable(CI_table(holder)[[3]], format = "latex", booktabs = TRUE)
t4 <- kable(CI_table(holder)[[4]], format = "latex", booktabs = TRUE)

t5 <- kable(CI_table(holder)[[5]], format = "latex", booktabs = TRUE)

cat(c("\\begin{table}[!htb]
    \\begin{minipage}{.5\\linewidth}
      \\caption{Component 1}
      \\centering",
        t1,
    "\\end{minipage}%
    \\begin{minipage}{0.7\\linewidth}
      \\centering
        \\caption{Component 2}",
        t2,
    "\\end{minipage} 
\\end{table}"
))  

cat(c("\\begin{table}[!htb]
    \\begin{minipage}{.5\\linewidth}
      \\caption{Component 3}
      \\centering",
        t3,
    "\\end{minipage}%
    \\begin{minipage}{0.7\\linewidth}
      \\centering
        \\caption{Component 4}",
        t4,
    "\\end{minipage} 
\\end{table}"
))  

cat(c("\\begin{table}[!htb]
    \\begin{minipage}{.5\\linewidth}
      \\caption{Priors}
      \\centering",
        t5,
    "\\end{minipage}%
\\end{table}"
))  

@



% TODO plot the forest-plot of linear-effect estimates and confidence intervals. How do we plot them while accounting for label switching?

\subsection{Algorithm Comparisons with Simulated Data}

In such instances, the proposed algorithm is demonstrably tighter around the true functions than the algorithm of Zhang et al. 

(INCLUDE COMPARISON PLOTS HERE).



\subsection{Real Data: Global Life Expectancy }

In this section, we apply the mixture of monotone regressions to global life expectancy data. Consider data on 'GDP per person' and 'Life expectancy' for all countries between the years 1960 and 2018, drawn from the free online resources of the World Bank [@worldbank]. Specifically, the data consists of $n$ observations $(y_1, \vec{x_1}),...,(y_n,\vec{x_n})$, where $Y$ represents Life Expectancy and the vector $\vec{X}$ represents both GDP and Year. 

Moreover, this data has two properties that are very common in real world data:
\begin{enumerate}[noitemsep] 
  \item Missing Data: Not all countries have data for all years, and several have gaps due to years of conflict in which data was not collected.
  \item \emph{A priori} Groupings: This data contains multiple observations per country, but we expect that our model will constrain countries to be clustered together. 
\end{enumerate}

On a first pass visualization of this data, we find a mostly linear relationship between Life Expectancy \& Year (\ref{lexy}), and a mostly logarithmic relationship between Life Expectancy \& GDP (\ref{lexgdp}). 

\begin{figure}
<<motivation, echo=F, fig.align='center', fig.height=3, message=FALSE>>=


# exploratory vizualization

continent %>%
  filter(Region != "") %>%
  group_by(Region, Year) %>%
  summarise(mean_LE = mean(LifeExpectancy)) %>%
ggplot(mapping = aes(x = Year, y = mean_LE, color = as.factor(Region))) + 
  geom_line() +
  theme_minimal() +
  labs(title = "Life Expectancy by Global Region",
       color="Regions") +
  ylab("Life Expectancy (years)") +
  xlab("Year")
@

\captionof{figure}[Abbrviated Caption]{The relationships between Life Expectancy and time are largely linear across all global regions. Sub-saharan Africa uniquely demonstrates what appears to be a cubic relationship over time.} \label{lexy}
\end{figure}

\begin{figure}
<<motivation_b, echo=F, fig.align='center', fig.height=3, message=FALSE>>=
continent %>%
  filter(Region != "") %>%
  group_by(Region, Year) %>%
  summarise(mean_GDP = log(mean(GDP)), mean_LE = mean(LifeExpectancy)) %>%
ggplot(mapping = aes(x = mean_GDP, y = mean_LE, color = as.factor(Region))) + 
  geom_line() +
  theme_minimal() +
  labs(title = "GDP per Capita by Global Region (log scale)",
       color="Regions") +
  ylab("Life Expectancy (years)") +
  xlab("GDP per Capita (current USD, log scale)")
  
  
@
\captionof{figure}[Abbrviated Caption]{The relationships between Life Expectancy and GDP per capita are largely log-linear across global regions. Nearly all regions feature brief noisy sections surrounded by notably larger smooth sections.}  \label{lexgdp}
\end{figure}

For the purposes of demonstration, we choose to model this data without log-transforming the GDP data in order to preserve its highly non-linear relationship with Life Expectancy. We proceed by building two step models: with and without an intercept, and each with `GDP` as the monotone covariate. Each model is in fact a series of 21 mixture models, 3 for each of $k = 1,...,7$. These models have the form of \ref{m6} and \ref{m7} respectively.

\begin{equation} \label{m6}
  Y = \sum_{k=1}^{K}\pi_k (g_{k} (GDP) \ +\  \beta_{k}\cdot Year \ +\ \epsilon_k)
\end{equation}

\begin{equation} \label{m7}
  Y = \sum_{k=1}^{K}\pi_k (g_{k} (GDP) \ +\  \beta_{0,k} \ +\ \beta_{1,k}\cdot Year \ +\ \epsilon_k)
\end{equation}

For each series, we plot the AIC and BIC per $k$, the rootogram of the model with the lowest AIC, and the fitted monotone functions for the model with the lowest AIC.


% [PLOT MODELS AIC/BIC AND MONOTONE COMPS HERE]

<<le_mod6, eval=TRUE, echo=FALSE, results=F>>=

m6 <- stepFlexmix(LifeExpectancy ~ .-1-Country.Name|Country.Name, data = le, k = 1:4, model = mono_reg(mon_inc_names = "GDP")) # step model | no intercept | grouped | monotone GDP

@

<<le_mod7, eval=TRUE, echo=FALSE, results=F>>=

m7 <- stepFlexmix(LifeExpectancy ~ .-Country.Name|Country.Name, data = le, k = 1:4, model = mono_reg(mon_inc_names = "GDP")) # step model | with intercept | grouped | monotone GDP

@





\begin{figure}
<<le_mod_plot1, eval=TRUE, echo=F, message=F, fig.align='center', fig.height=4>>=

# m6
plot(m6, main="AIC / BIC / ICL in Model 6 by Number of Components")

# TODO select ACTUAL lowest AIC
num <- which.max(apply(m6@logLiks, 1, function(x) mean(x)))

plot(m6@models[[num]], palet="Dark2", root_scale="sqrt", subplot=1) # rootogram



 # format data for adding fitted Life Expectancy curves by country
plot_data <- le
plot_data$Cluster <- factor(m6@models[[num]]@cluster)
plot_data$fitle <- plot_data$LifeExpectancy - (sapply(plot_data$Cluster, function(x) m6@models[[num]]@components[[x]][[1]]@parameters$coef)*plot_data$Year)
plot(m6@models[[num]], log=T, subplot=2) + # overlay monotone plots with plots of individual countries
  geom_line(data = plot_data, mapping = aes(GDP, fitle, group=Country.Name, color=Cluster), alpha=0.1) +
  scale_color_brewer(palette="Dark2") +
  theme_bw() + 
  scale_x_log10() # set to log scale
# TODO 3-D plot?

@
\captionof{figure}[Abbreviated Caption]{Here we observe the various model selection metrics -- AIC, BIC and ICL -- for models represented by equation \ref{m6}, constructed with each of $k$ components. Below, we have the estimated monotone components of the model with lowest AIC.}  
\end{figure}

\begin{figure}
<<le_mod_plot2, eval=TRUE, echo=F, message=F, fig.align='center', fig.height=4>>=

############
# m7
plot(m7, main="AIC / BIC / ICL in Model 7 by Number of Components")

# TODO select ACTUAL lowest AIC
num <- which.max(apply(m7@logLiks, 1, function(x) mean(x)))

plot(m7@models[[num]], palet="Dark2", root_scale="sqrt", subplot=1) # rootogram

 # format data for adding fitted Life Expectancy curves by country
plot_data <- le
plot_data$Cluster <- factor(m7@models[[num]]@cluster)
plot_data$fitle <- plot_data$LifeExpectancy - apply(t(sapply(plot_data$Cluster, function(x) m7@models[[num]]@components[[x]][[1]]@parameters$coef))*cbind(rep.int(1, dim(plot_data)[2]), plot_data$Year), 1, sum) # jesus... subtract from life expectancy the SUM of the intercept and the GDP coeffecient (of the correct cluster) x the correct country-year GDP
plot(m7@models[[num]], log=T, subplot=2) + # overlay monotone plots with plots of individual countries
  geom_line(data = plot_data, mapping = aes(GDP, fitle, group=Country.Name, color=Cluster), alpha=0.1) +
  scale_color_brewer(palette="Dark2") +
  theme_bw() +
  scale_x_log10() # set to log scale
@
\captionof{figure}[Abbreviated Caption]{Here we observe the various model selection metrics -- AIC, BIC and ICL -- for models represented by equation \ref{m7}, constructed with each of $k$ components. Below, we have the estimated monotone components of the model with lowest AIC.}
\end{figure}
 

Next, we plot the distribution of clusters within the lowest-AIC model of each step-model series, projected onto a world map. In these world map plots, the colors of each cluster span a spectrum from white to full-color, representing the strength of the posterior and the confidence of the model in placing a given country within a given cluster. The world-maps indicate what the rootograms had previously indicated, namely that the resulting models are extremely confident about the clustering of nearly all countries.

\begin{figure}
<<worldmaps1, eval=TRUE, echo=F, message=F, fig.align='center', fig.height=4>>=

world <- ne_countries(scale = "medium", returnclass = "sf")

mod6 <- m6@models[[which.min(AIC(m6))]]


maxpost <- apply(mod6@posterior$scaled, 1, function(x) max(x))
clustdat <- tibble(le$Country.Name, mod6@cluster, maxpost)
names(clustdat) <- c("name", "cluster", "posterior")
plot_data <- merge(world, clustdat, by = "name", all.x = TRUE)

gg <- ggplot()
pal <- brewer.pal(length(mod6@components), "Dark2")

for(j in 1:length(mod6@components)){
  gg <- gg + geom_sf(data = plot_data[plot_data$cluster==j,], aes(fill = posterior)) + 
  scale_fill_gradient2(paste("Cluster", j), limits=c(0,1), low = "white", high = pal[j]) +
  theme(legend.direction = "horizontal") +
 
  new_scale("fill")    
}

gg <- gg + theme_minimal() + 
  theme(legend.direction = "horizontal") + 
  ggtitle("Map of Country Clusters in m6")


# TODO extract legend grob, edit, and replace with title. If this doesnt work, we resort to adding a text annotation
g <- ggplotGrob(gg) # Get the ggplot grob
leg <- g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] # Get the legend
title_grob <- textGrob("Posterior", gp = gpar(fontsize = 12))
table_grob <- gtable_add_rows(leg, heights = grobHeight(title_grob) + unit(5,'mm'), pos = 0)
table_grob <- gtable_add_grob(table_grob, title_grob, 1, 1, 1, ncol(table_grob), clip = "off")
g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] <- table_grob # replace original grob with edited grob

ggpubr::as_ggplot(g) + theme_minimal() # cast back to ggplot, and print


@
\captionof{figure}[Abbreviated Caption]{Here we observe the world map with colour code corresponding to the results of model \ref{m6}.}
\end{figure}

\begin{figure}
<<worldmaps2, eval=TRUE, echo=F, message=F, fig.align='center', fig.height=4>>=

##################
world <- ne_countries(scale = "medium", returnclass = "sf")

mod7 <- m7@models[[which.min(AIC(m7))]]


maxpost <- apply(mod7@posterior$scaled, 1, function(x) max(x))
clustdat <- tibble(le$Country.Name, mod7@cluster, maxpost)
names(clustdat) <- c("name", "cluster", "posterior")
plot_data <- merge(world, clustdat, by = "name", all.x = TRUE)

gg <- ggplot()
pal <- brewer.pal(length(mod7@components), "Dark2")

for(j in 1:length(mod7@components)){
  gg <- gg + geom_sf(data = plot_data[plot_data$cluster==j,], aes(fill = posterior)) +
  scale_fill_gradient2(paste("Cluster", j), limits=c(0,1), low = "white", high = pal[j]) +
  theme(legend.direction = "horizontal") +

  new_scale("fill")
}

gg <- gg + theme_minimal() +
  theme(legend.direction = "horizontal") +
  ggtitle("Map of Country Clusters in m7")

# TODO extract legend grob, edit, and replace with title. If this doesnt work, we resort to adding a text annotation
g <- ggplotGrob(gg) # Get the ggplot grob
leg <- g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] # Get the legend
title_grob <- textGrob("Posterior", gp = gpar(fontsize = 12))
table_grob <- gtable_add_rows(leg, heights = grobHeight(title_grob) + unit(5,'mm'), pos = 0)
table_grob <- gtable_add_grob(table_grob, title_grob, 1, 1, 1, ncol(table_grob), clip = "off")
g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] <- table_grob # replace original grob with edited grob

ggpubr::as_ggplot(g) + theme_minimal() # cast back to ggplot, and print


@
\captionof{figure}[Abbreviated Caption]{Here we observe the world map with colour code corresponding to the results of model \ref{m7}.}
\end{figure}


Finally, we refit model \ref{m6} while excluding all observations from Australia in order to demonstrate the predictive capacities of the mixture model. As stated previously, in section \ref{prediction}, there are two possible contexts in which one might use the mixture model predictively. One may have a complete observation or set of observations, e.g., the \texttt{Year}, \texttt{GDP} and \texttt{Life Expectancy} data for Australia over several years, in which case one could use the mixture model to generate a set of posterior probabilities representing the probability of Australia belonging to each of the model clusters. Alternately, one may have an incomplete observation or set of observations, e.g., the \texttt{Year} and \texttt{GDP} data (but not \texttt{Life Expectancy}) for Australia over several years, in which case one could use the mixture model to generate a conditional marginal distribution of \texttt{Life Expectancy} for Australia.

<<predictions, eval=FALSE, echo=F, message=F, fig.align='center'>>=
# TODO demonstrate predictions of new observed X
# TODO predictions should give: a posterior distribution of cluster belonging, and
# TODO a marginal distribution of Y given X

exclude <- "Australia"
exdat <- le[le$Country.Name==exclude,] # get observations
# refit model without 'exclude' observations
m7refit <- flexmix(LifeExpectancy ~ .-Country.Name|Country.Name, data = le[le$Country.Name!=exclude,], k = num, model = mono_reg(mon_inc_names = "GDP")) # num-component model | with intercept | grouped | monotone GDP

# TODO predict cluster membership for complete covariate set (y, x)
predict_cluster(m7refit, exdat)

# TODO predict marginal distribution of life expectancy for covariate set, minus y (x) 
predict_marginal(m7refit, exdat[, !names(exdat) %in% "LifeExpectancy"])


@


\section{Discussion}

\subsection{Applications}

\subsection{Weaknesses}

\subsection{Future Developments}


\section*{Acknowledgments}

Here I acknowledge all my homies.

\bibliography{mono}



\begin{appendices}

\chapter{Asymptotic Behaviour of Partially Linear Models with Monotone Constraints} \label{abplmmc}

As discussed in (REF SECTION), the asymptotic behaviour of partially linear models with monotone constraints described by \cite{guangcheng} requires that the following assumptions hold:

page 1983 guang cheng

\chapter{R Code}


All results in this paper were produced by an extension of Flexmix (CITE), a flexible implementation of generalized mixture models written by FLEISCH and GRUN. The package provides a framework for implementing specific types of mixture models based on a central, universal framework. It is thus intended to allow users to elaborate a specific estimator or "driver" for the modeling of mixture components, while the more abstract behaviours are managed by the Flexmix code. More specifically, Flexmix implements:

\begin{enumerate}
  \item The universal functions of the EM algorithm for assigning prior values $\pi_k$ to each of the mixture components and posterior values $\mathcal{L}_i$ to each of the observations upon each iteration of the EM algorithm;
  \item Threshold constants for the convergence of the EM algorithm;
  \item Restrictions on the model estimate, e.g., restricting components to have an estimated prior above a certain threshhold;
  \item Ordinary- and parametric-bootstrap methods for the estimation of confidence intervals with respect to each component.
\end{enumerate}

Conversely, the user must provide a model estimator and a model object with a log-likelihood method, logLik() (FONT), which returns the density of an observation given a component's parameters. In the case of mixtures of regressions, this additionally implies a prediction function, predict() (FONT), which gives an expected value of an observation's dependent variable given the observed independent variables.

WHAT ELSE DOES FLEXMIX DO FOR US?

The code for the extension of Flexmix for modelling mixtures of partially-linear monotone regressions is included below. The first code block implements the partial linear model with monotone shape constraints; the second code block integrates the partial linear model with the Flexmix framework; the third code block provides additional functions for the visualization of results; the fourth code block implements the "conditional bootstrap" described in section (LINK).

initialize solution (CITE Finite Mixture Model Diagnostics Using Resampling Methods)


<<part_fit, eval=FALSE>>=
# Define function for fitting a partial linear model with arbitrary monotone-constrained component

# import libraries
library(gridExtra)
library(dplyr)


cpav <- function(x_mat, y, weights, inc_index=NULL, dec_index=NULL, max_iters_cpav=NULL, max_delta_cpav=NULL){
  
  joint_ind <- c(inc_index, dec_index)
  
  if(!is.matrix(x_mat)) stop("x_mat is not of class matrix, and will be rejected by lm.wfit")
  if(any(weights == 0)) stop("monoreg(), and therefore cpav(), cannot take weights of 0!")
  if(length(y) != length(weights) | length(y) != dim(x_mat)[1]) stop("The dimension of the inputs is not 
                                        equal to the dimension of the weights")
  
  # if there is only 1 monotone component, apply ordinary monotone regression
  if(length(joint_ind) == 1){
    if(length(inc_index) == 1){ # the component is monotone increasing
      return( # cast the monoreg object as a matrix, with all attributes as rows in the first column
        matrix(suppressWarnings(monoreg(x = x_mat[,inc_index], y = y, w = weights)), 
               dimnames = list(c("x", "y", "w", "yf", "type", "call")))
      )
    }
    else{ # the component is monotone decreasing
      return( # cast the monoreg object as a matrix, with all attributes as rows in the first column
        
        matrix(suppressWarnings(monoreg(x = x_mat[,dec_index], y = y, w = weights, type = "antitonic")), 
               dimnames = list(c("x", "y", "w", "yf", "type", "call")))
      )
    }
  }
  
  else{ # the monotone components are multiple, so continue with cyclic algorithm
    
    # fit ordinary lm on x_mat and y
    start_betas <- coef(lm.wfit(x=x_mat[,joint_ind], y=y, w=weights))
    
    # set initial monotone reg estimates by calling each monoreg() against y - lm.predict(all other vars)
    
    mr_fits <- sapply(1:length(joint_ind), function(i) 
      if(joint_ind[i] %in% inc_index){
        # I apologize to anyone trying to read this line, but think: the columns of the x_matrix
        # indicated by join_ind, except the value of joint_ind at the ith place in joint_ind
        suppressWarnings(monoreg(x = x_mat[,joint_ind[i]], 
                y = (y - (as.matrix(x_mat[,joint_ind[-i]]) %*% start_betas[-i]) ), w = weights, type = "isotonic"))
        }
      else if(joint_ind[i] %in% dec_index){
        suppressWarnings(monoreg(x = x_mat[,joint_ind[i]], 
                y = (y - (as.matrix(x_mat[,joint_ind[-i]]) %*% start_betas[-i]) ), w = weights, type = "antitonic"))
      })
    
    # iterate through mr_fits. each column of mr_fits (e.g., mr_fits[,1]) is a monoreg fitted object,
    # and its attributes can be called (e.g., mr_fits[,1]$yf)
    iters <- 0
    delta <- 0.5
    if(is.null(max_iters_cpav)){
      max_iters_cpav <- 100
    }
    if(is.null(max_delta_cpav)){
      max_delta_cpav <- 0.0000001
    }
    
    while(abs(delta) > max_delta_cpav & iters < max_iters_cpav){
      old_SS <- mean( (y - get_pred(mr_fits, x_mat[,joint_ind]))^2 )
      
      for(i in 1:length(joint_ind)){
        if(joint_ind[i] %in% inc_index){
          # I apologize to anyone trying to read this line, but think: the columns of the x_matrix
          # indicated by join_ind, except the value of joint_ind at the ith place in joint_ind
          mr_fits[,i] <- suppressWarnings(monoreg(x = x_mat[,joint_ind[i]],
                                 y = (y - get_pred(mr_fits[,-i], x_mat[,joint_ind[-i]]) ), w = weights, type = "isotonic"))
        }
        else if(joint_ind[i] %in% dec_index){
          mr_fits[,i] <- suppressWarnings(monoreg(x = x_mat[,joint_ind[i]],
                                 y = (y - get_pred(mr_fits[,-i], x_mat[,joint_ind[-i]]) ), w = weights, type = "antitonic"))
        }
        
      }

      new_SS <- mean( (y - get_pred(mr_fits, x_mat[,joint_ind]))^2 )
      delta <- (old_SS - new_SS)/old_SS
      iters <- iters + 1
    }
    
    return(mr_fits)
  }
}


# first, define function for obtaining f(x_new) for monotone regression f()
# get_pred returns a vector of length = nrows(xvals), ie, a value for each observation of xvals
get_pred <- function(mr_obj, xvals){
  xvals <- as.matrix(xvals)
  mr_obj <- as.matrix(mr_obj)
  
  if(dim(mr_obj)[2] != dim(xvals)[2]) stop("get_pred() must take an X-matrix with as many columns
                                            as monoreg() objects")
  
  apply(sapply(1:ncol(xvals), function(j)
         mr_obj[,j]$yf[sapply(xvals[,j], function(z)
           ifelse( z < mr_obj[,j]$x[1], 1,
            ifelse(z >= tail(mr_obj[,j]$x, n=1), length(mr_obj[,j]$x), 
                   which.min(mr_obj[,j]$x <= z)-1 )))]
         ), 1, function(h) sum(h))

}


# define partial linear regression of y on x with weights w
# inputs are: x, y, wates, mon_inc_index, mon_dec_index, max_iter
part_fit <- function(x, y, wates = NULL, mon_inc_index=NULL, mon_dec_index=NULL, max_iter=NULL, 
                     component = NULL, na.rm=T, mon_inc_names = NULL, mon_dec_names = NULL, ...){

  # cast x to matrix
  x <- as.matrix(x)
  
  # set default weights
  if(is.null(wates)) wates <- rep(1, length(y))
  
  # remove incomplete cases
  if(T){ # for now, there is no alternative to na.rm=T.  All incomplete cases are removed.
    cc <- complete.cases(y) & complete.cases(x) & complete.cases(wates)
    y <- y[cc]
    x <- x[cc,]
    wates <- wates[cc]
    cc <- NULL
  }
  
  x <- as.matrix(x) # cast again. hacky but necessary?
  
  
  # make sure y and wates is not multivariate
  if(length(y) != dim(x)[1] | length(y) != length(wates)) stop("Inputs are not of the same dimension!")
  
  # take monotone indices of previous component
  if(!is.null(component)){
    inc_ind <- component$mon_inc_index
    dec_ind <- component$mon_dec_index
  }
  else{
    # assume that monotone variable is first column in x and increasing, unless specified otherwise
    if(!is.null(mon_inc_index)){
      inc_ind <- mon_inc_index
    } 
    else{
      inc_ind <- 1
    }
    if(!is.null(mon_dec_index)){
      dec_ind <- mon_dec_index
    } 
    else{
      dec_ind <- NULL
    }
  }
 
  # throw warning if there are duplicates in inc_ind or dec_ind, and then remove
  if(anyDuplicated(inc_ind) | anyDuplicated(dec_ind)){
    warning("There are duplicate index instructions; Duplicates are being removed.")
    inc_ind <- unique(inc_ind)
    dec_ind <- unique(dec_ind)
  }
  
  # throw error if indices overlap
  if(length(intersect(inc_ind, dec_ind)) > 0) stop("At least one variable was marked as BOTH
                                               monotone increasing and monotone decreasing.")
  
  # throw error if indices are not integers
  if(!is.null(inc_ind)){
    if(any(inc_ind != as.integer(inc_ind))) stop("Monotone increasing indices are not integers.")
  }
  if(!is.null(dec_ind)){
    if(any(dec_ind != as.integer(dec_ind))) stop("Monotone decreasing indices are not integers.")
  }
  
  # throw error if indices are not positive 
  if(any(c(inc_ind, dec_ind) < 1)) stop("all monotone component indices must be positive")
  
  # throw error if the number of indices exceeds columns of x
  if(length(c(inc_ind, dec_ind)) > ncol(x)) stop("Number of proposed monotonic relationships exceeds columns of x.")
  
  # If there is an intercept but no other linear effects, stop
  if((length(c(inc_ind, dec_ind))+1) == ncol(x) & "(Intercept)" %in% colnames(x)){
    stop("For identifiability purposes, you cannot build a part_fit with only an intercept as a linear component.")
  }
  
  # option for fit with no linear independent components and one or multiple monotone components:
  if(length(c(inc_ind, dec_ind)) == ncol(x)){
    
    yhat <- cpav(x_mat = as.matrix(x[wates != 0,]), y = y[wates != 0], weights = wates[wates != 0], 
                 inc_index = inc_ind, dec_index = dec_ind)

    # get residuals of model
    resids <- y - get_pred(yhat, x[,c(inc_ind, dec_ind)])
    
    # mod must have: coef attribute, sigma attribute, cov attribute, df attribute, ..., and 
    # may have mon_inc_index and mon_dec_index attributes
    mod <- list(coef = NULL, fitted_pava = NULL, sigma = NULL, df = NULL,
                mon_inc_index = NULL, mon_dec_index = NULL, iterations = NULL, 
                mon_inc_names = NULL, mon_dec_names = NULL)
  
    mod$coef <- NULL
    mod$fitted_pava <- yhat
    mod$mon_inc_index <- inc_ind
    mod$mon_dec_index <- dec_ind
    mod$sigma <- sqrt(sum(wates * (resids)^2 /
                                     mean(wates))/ (nrow(x)-qr(x)$rank))
    mod$df <- ncol(x)+1
    
    class(mod) <- "part_fit"
    
    return(mod)
  }
  else{
    # for starting values, fit a regular lm
    fit <- lm.wfit(x=x, y=y, w=wates)
    betas <- coef(fit)[-c(inc_ind, dec_ind)]
  
    # set maximum iterations for convergence
    if(!is.null(max_iter) & !is.list(max_iter)){
      if(max_iter < 1) stop("max_iter must be positive")
      maxiter <- max_iter
    }
    else{ 
      maxiter <- 10000
    }
      
    # set while loop initial values
    iter <- 0
    delta <- 10
    # iterate between pava and linear model
    while(delta > 1e-6 & iter < maxiter){ # works well enough with delta > 1e-12. Trying 1e-6
  
      yhat <- cpav(x_mat = as.matrix(x[wates != 0,]), y = (y[wates != 0] - (as.matrix(x[wates != 0,-c(inc_ind, dec_ind)]) %*% betas)), 
                   weights = wates[wates != 0], inc_index = inc_ind, dec_index = dec_ind)

      old_betas <- betas    # save old betas for distance calculation
      # to retrieve old ordering of y for fitted values, we use y[match(x, sorted_x)]
      betas <- coef(lm.wfit(x=as.matrix(x[,-c(inc_ind, dec_ind)]), y= (y - get_pred(yhat, x[,c(inc_ind, dec_ind)]) ), w=wates))

      # get euclidian distance between betas transformed into unit vectors
      delta <- dist(rbind( as.vector(betas)/norm(as.vector(betas), type ="2"), 
                           as.vector(old_betas)/norm(as.vector(old_betas), type="2")
                           ))
  
      iter <- iter + 1    # iterate maxiter
    }
  }
  
  # get residuals of model
  resids <- y - (get_pred(yhat, x[,c(inc_ind, dec_ind)]) + (as.matrix(x[,-c(inc_ind, dec_ind)]) %*% betas))

  # mod must have: coef attribute, sigma attribute, cov attribute, df attribute, ..., and 
  # may have mon_inc_index and mon_dec_index attributes
  mod <- list(coef = NULL, fitted_pava = NULL, sigma = NULL, df = NULL,
              mon_inc_index = NULL, mon_dec_index = NULL, iterations = NULL, 
              mon_inc_names = NULL, mon_dec_names = NULL)
  
  mod$coef <- betas
  mod$fitted_pava <- yhat
  mod$iterations <- iter
  mod$mon_inc_index <- inc_ind
  mod$mon_dec_index <- dec_ind
  mod$sigma <- sqrt(sum(wates * (resids)^2 / mean(wates))/ (nrow(x)-qr(x)$rank))
  mod$df <- ncol(x)+1

  class(mod) <- "part_fit"
  
  return(mod)
}


# write plot method for objects returned from part_fit()

append_suffix <- function(num){
  suff <- case_when(num %in% c(11,12,13) ~ "th",
                    num %% 10 == 1 ~ 'st',
                    num %% 10 == 2 ~ 'nd',
                    num %% 10 == 3 ~'rd',
                    TRUE ~ "th")
  paste0(num, suff)
}

plot.part_fit <- function(z){
  if(dim(as.matrix(z$fitted_pava))[2] > 1){
    temp <- list()
    for(i in 1:dim(as.matrix(z$fitted_pava))[2]){
      temp[[i]] <- ggplotGrob(ggplot() +
        geom_line(aes(x = z$fitted_pava[,i]$x, y = z$fitted_pava[,i]$yf)) + 
        theme_bw() +
        labs(title = paste(append_suffix(i), " Monotone Regression"),
             x = "X",
             y = "Y"))
    }
    return(grid.arrange(grobs=temp, ncol=1))
  }
  else{
    temp <- ggplot() +
      geom_line(aes(x = z$fitted_pava[,1]$x, y = z$fitted_pava[,1]$yf)) + 
      theme_bw() +
      labs(title = "Monotone Regression",
           x = "X",
           y = "Y")
    return(temp)
  }
}

@


<<M_driver, eval=FALSE>>=

# The M-step of the EM Algorithm. Meshes with Flexmix Package.

# allow slots defined for numeric to accept NULL
setClassUnion("numericOrNULL",members=c("numeric", "NULL"))
setClassUnion("characterOrNULL", members = c("character", "NULL"))
setOldClass("monoreg")
setClassUnion("matrixOrMonoreg", members = c("matrix", "monoreg"))

# Define new classes
setClass(
  "FLX_monoreg_component",
  contains="FLXcomponent",
  # allow mon_index to take either numeric or NULL
  slots=c(mon_inc_index="numericOrNULL", 
          mon_dec_index="numericOrNULL",
          mon_obj="matrix",
          mon_inc_names="characterOrNULL",
          mon_dec_names="characterOrNULL"
          )
) 

# Define FLXM_monoreg 
setClass("FLXM_monoreg",
         # TODO what does FLXM_monoreg need to inherit?
         contains = "FLXM",
         slots = c(mon_inc_index="numericOrNULL", 
                   mon_dec_index="numericOrNULL",
                   mon_inc_names="characterOrNULL",
                   mon_dec_names="characterOrNULL"))






# definition of monotone regression model.
mono_reg <- function (formula = .~., mon_inc_names = NULL, 
                      mon_dec_names = NULL, mon_inc_index=NULL, mon_dec_index=NULL, ...) {

  # only names or indices can be indicated, not both
  if((!is.null(mon_inc_names)|!is.null(mon_dec_names)) &
     (!is.null(mon_inc_index)|!is.null(mon_dec_index))) stop("mono_reg() can accept either monotone
                                                             names or indices can be chosen, but not both.")
  
  retval <- new("FLXM_monoreg", weighted = TRUE,
                formula = formula,
                name = "partially linear monotonic regression",
                mon_inc_index= sort(mon_inc_index),
                mon_dec_index= sort(mon_dec_index), 
                mon_inc_names= mon_inc_names,
                mon_dec_names= mon_dec_names) 
  
  # @defineComponent: Expression or function constructing the object of class FLXcomponent
  # fit must have attributes: coef, sigma, cov, df, ..., and 
  # may have mon_inc_index and mon_dec_index attributes
  # ... all must be defined by fit() function
  retval@defineComponent <- function(fit, ...) {
                  # @logLik: A function(x,y) returning the log-likelihood for observations in matrices x and y
                  logLik <- function(x, y) { 
                    dnorm(y, mean=predict(x, ...), sd=fit$sigma, log=TRUE)
                  }
                  # @predict: A function(x) predicting y given x. 
                  # TODO x must be partitioned into linear and monotone covars
                  predict <- function(x) {
                    inc_ind <- fit$mon_inc_index
                    dec_ind <- fit$mon_dec_index
                    
                    p <-  get_pred(fit$fitted_pava, x[,c(inc_ind, dec_ind)])
                    if(!is.null(fit$coef)){
                      p <- p + (as.matrix(x[,-c(inc_ind, dec_ind)]) %*% fit$coef)
                    }
                    p
                  }
                  # return new FLX_monoreg_component object
                  new("FLX_monoreg_component", parameters =
                        list(coef = fit$coef, sigma = fit$sigma),
                      df = fit$df, logLik = logLik, predict = predict,
                      mon_inc_index = fit$mon_inc_index,
                      mon_dec_index = fit$mon_dec_index,
                      mon_obj = fit$fitted_pava,
                      mon_inc_names = fit$mon_inc_names,
                      mon_dec_names = fit$mon_dec_names)
  }
  
  # @fit: A function(x,y,w) returning an object of class "FLXcomponent"
  retval@fit <- function(x, y, w, component, mon_inc_index = retval@mon_inc_index, 
                         mon_dec_index = retval@mon_dec_index, 
                         mon_inc_names = retval@mon_inc_names,
                         mon_dec_names = retval@mon_dec_names, ...) {
    
                  
                  if(is.null(mon_inc_index) & is.null(mon_dec_index)){
                    
                    # if not all monotone names are in the design matrix, stop & print the name that is missing
                    if(!all(c(mon_inc_names, mon_dec_names) %in% colnames(x))){
                      stop(paste(setdiff(c(mon_inc_names, mon_dec_names), colnames(x)),
                                 "could not be found in the model matrix. Check your spelling."))
                    } 
                    # Discover correct monotone indices
                    if(any(colnames(x) %in% mon_inc_names)){
                      mon_inc_index <- which(colnames(x) %in% mon_inc_names)
                    }
                    if(any(colnames(x) %in% mon_dec_names)){
                      mon_dec_index <- which(colnames(x) %in% mon_dec_names)
                    }
                  }
                  if(is.null(mon_inc_names) & is.null(mon_dec_names)){
                    # Discover correct monotone names
                    mon_inc_names <- colnames(x)[sort(mon_inc_index)]
                    mon_dec_names <- colnames(x)[sort(mon_dec_index)]
                  }
    
                  # if(any(apply(x, 2, function(x) is.factor(x)))) stop("x cannot have factor columns as monotone components")

                  fit <- part_fit(x, y, w, component, mon_inc_index=mon_inc_index, 
                                  mon_dec_index=mon_dec_index, ...)
                  
                  retval@defineComponent(fit, ...)
                  }
  retval 
  }



@


<<flexmix_wrappers, eval=FALSE>>=

# Wrapper functions for Flexmix objects with monoreg components

# import libraries
library(ggplot2)
library(grid)
library(gridExtra)
library(RColorBrewer)


# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}


####

# overwrite method for plot.flexmix
setMethod('plot',  signature(x="flexmix", y="missing"),
          function(x, mark=NULL, markcol=NULL, col=NULL, 
                   eps=1e-4, root=TRUE, ylim=NULL, xlim=NULL, main=NULL, xlab=NULL, ylab=NULL,
                   as.table = TRUE, endpoints = c(-0.04, 1.04), rootogram=F, palet = NULL, 
                   root_scale = "unscaled", subplot=NULL, ...) {
            
            if(is.null(palet)){
              palet <- "Accent"
            }
            
           
  if(is(x@components[[1]][[1]], "FLX_monoreg_component")){ # check that this is a mixture of part_fits
    # assign appropriate names for graph labelling
    if(is.null( c(x@model[[1]]@mon_inc_names, x@model[[1]]@mon_dec_names) )){
      xnames <- sapply(1:dim(x@components[[1]][[1]]@mon_obj)[2], function(x) paste0("X", x))
      mono_names <- c("Y", xnames)
    }
    else{
      mono_names <- c(x@formula[[2]], c(x@model[[1]]@mon_inc_names, x@model[[1]]@mon_dec_names))
    }
    
            # get dimension of monotone components by reading columns of fitted_pava object
            if(dim( x@components[[1]][[1]]@mon_obj )[2] > 1){ 
              np <- list()
              for(i in 1:dim( x@components[[1]][[1]]@mon_obj )[2]){
                holder <- ggplot()
                
                if(length(x@components) == 1){
                    holder <- holder + 
                      geom_line(aes(x = x@components[[1]][[1]]@mon_obj[,i]$x,
                                y = x@components[[1]][[1]]@mon_obj[,i]$yf)) +
                      theme_bw() +
                      labs(title = paste(append_suffix(i), " Monotone Regression"),
                       x = mono_names[i+1],
                       y = mono_names[1])
                    }
                
                if(length(x@components) > 1){
                  
                  monlist <- list()
                  for(b in 1:length(x@components)){
                    monlist[[b]] <- data.frame(x = x@components[[b]][[1]]@mon_obj[,i]$x, 
                                               yf = x@components[[b]][[1]]@mon_obj[,i]$yf)
                  }
                  
                  mondf   <- cbind(Cluster=rep(1:length(x@components), 
                                               sapply(monlist,nrow)),do.call(rbind,monlist))
                  mondf$Cluster <- as.factor(mondf$Cluster)
                  
                  holder <- holder + geom_line(mondf, mapping = aes(x,yf, color=Cluster)) + 
                    scale_color_brewer(palette=palet) +
                    theme_bw() +
                    labs(title = paste(append_suffix(i), " Monotone Regression"),
                         x = mono_names[i+1],
                         y = mono_names[1])
                }
                
                if(!is.null(ylim)){
                  if(length(ylim) != dim( x@components[[1]][[1]]@mon_obj )[2] |
                     length(ylim[[1]]) != 2 ){
                    stop("If you pass a ylim argument, it must have as many element pairs 
                         as the model has monotone components. Try formulating the argument
                         as: ylim = list(c(i,j), c(i,j), ...)")}
                  holder <- holder + ylim(ylim[[i]])
                }
                if(!is.null(xlim)){
                  if(length(xlim) != dim( x@components[[1]][[1]]@mon_obj )[2] |
                     length(xlim[[1]]) != 2 ){
                    stop("If you pass a xlim argument, it must have as many element pairs 
                         as the model has monotone components. Try formulating the argument
                         as: xlim = list(c(i,j), c(i,j), ...)")}
                  holder <- holder + xlim(xlim[[i]])
                }
                if(!is.null(ylab)){
                  if(length(ylab) != dim( x@components[[1]][[1]]@mon_obj )[2]){
                    stop("If you pass a ylab argument, it must have as many elements 
                         as the model has monotone components. Try formulating the argument
                         as: ylab = c(\"first\",\"second\",...)")}
                  holder <- holder + ylab(ylab[[i]])
                }
                if(!is.null(xlab)){
                  if(length(xlab) != dim( x@components[[1]][[1]]@mon_obj )[2]){
                    stop("If you pass a xlab argument, it must have as many elements 
                         as the model has monotone components. Try formulating the argument
                         as: xlab = c(\"first\",\"second\",...)")}
                  holder <- holder + xlab(xlab[[i]])
                }
                if(!is.null(main)){
                  if(length(main) != dim( x@components[[1]][[1]]@mon_obj )[2]){
                    stop("If you pass a main argument, it must have as many elements 
                         as the model has monotone components. Try formulating the argument
                         as: main = c(\"first\",\"second\",...)")}
                  holder <- holder + ggtitle(main[[i]])
                }
                
                
                
                np[[i]] <- holder
              }
              # return(grid.arrange(grobs=np, ncol=1))
              # return(grid.arrange(grobs=np, ncol=1))
            }
            else{
              np <- ggplot()
              
              if(length(x@components) == 1){
                np <- np + geom_line(aes(x = x@components[[1]][[1]]@mon_obj[,1]$x, y = 
                                           x@components[[1]][[1]]@mon_obj[,1]$yf)) + 
                  theme_bw() +
                  labs(title = "Monotone Component",
                     x = mono_names[2],
                     y = mono_names[1]) 
                }
              
              if(length(x@components) > 1){
                
                monlist <- list()
                for(b in 1:length(x@components)){
                  monlist[[b]] <- data.frame(x = x@components[[b]][[1]]@mon_obj[,1]$x, 
                                                   yf = x@components[[b]][[1]]@mon_obj[,1]$yf)
                }
                
                mondf   <- cbind(Cluster=rep(1:length(x@components), 
                                             sapply(monlist,nrow)), do.call(rbind, monlist))
                mondf$Cluster <- as.factor(mondf$Cluster)
                
                np <- np + geom_line(mondf, mapping = aes(x,yf, color=Cluster)) + 
                  scale_color_brewer(palette=palet) +
                  theme_bw() +
                  labs(title = "Monotone Component",
                       x = mono_names[2],
                       y = mono_names[1])
              }
              
              
              if(!is.null(ylim)){
                np <- np + ylim(ylim)
              }
              if(!is.null(xlim)){
                np <- np + xlim(xlim)
              }
              if(!is.null(ylab)){
                np <- np + ylab(ylab)
              }
              if(!is.null(xlab)){
                np <- np + xlab(xlab)
              }
              if(!is.null(main)){
                np <- np + ggtitle(main)
              }
              
              
              
              
              # return(np)
            }
  }
            # plot and append rootogram
            post <- data.frame(x@posterior$scaled) # collect posteriors
            names(post) <- 1:dim(post)[2] # change columns of posteriors to cluster numbers
            post <- melt(setDT(post), measure.vars = c(1:dim(post)[2]), variable.name = "Cluster")
            rg <- ggplot(post, aes(x=value, fill=Cluster)) + # plot rootogram, with color indicating cluster
              geom_histogram(binwidth = 0.05) + 
              scale_fill_brewer(palette=palet) +
              theme_bw() +
              labs(title = "Rootogram",
                   x = "Posteriors",
                   y = "Count")
            
            if(root_scale == "sqrt"){rg <- rg + 
              scale_y_sqrt() +
              labs(title = "Rootogram (square root scale)",
                   x = "Posteriors",
                   y = "Count (square root)")}
            if(root_scale == "log"){rg <- rg + 
              scale_y_log10() + 
              labs(title = "Rootogram (log scale)",
                   x = "Posteriors",
                   y = "Count (log)")}
            
            if(!is.null(subplot)){
              return(list(rg, np)[[subplot[1]]])
            }
            else{
              multiplot(rg, np)
            }
          }          
)

            
            


          

@

\end{appendices}

\end{document}