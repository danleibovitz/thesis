\documentclass[12pt,twoside]{article}
\renewcommand{\abstractname}{\vspace{-\baselineskip}}
\usepackage{hyperref}
\usepackage{setspace}
\urlstyle{same}
\usepackage{geometry}
 \geometry{left=1.25in, right= 1.25in ,top=1.7in}
\setlength{\parindent}{.5in}
\renewcommand{\baselinestretch}{1.3}
\setlength{\parskip}{\baselineskip}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage[font=normal]{caption}
\renewcommand{\headrulewidth}{0pt}
\usepackage[labelfont=bf]{caption}
\captionsetup[figure]{labelfont={bf},name={Figure},labelsep=period}
\usepackage{natbib}
\setlength{\bibhang}{0.5in}
\bibliographystyle{apalike}
\usepackage{graphicx}
%\usepackage{float}
% \usepackage{floatrow}
% \floatsetup[table]{capposition=top}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{titling}
\settowidth{\thanksmarkwidth}{*}
\setlength{\thanksmargin}{-\thanksmarkwidth}
\usepackage[hang,flushmargin]{footmisc}
\setlength{\skip\footins}{1.2pc plus 5pt minus 2pt}
\usepackage{titlesec}
\setlength{\droptitle}{-5em}
\setlength\thanksmarkwidth{.5em}
\setlength\thanksmargin{-\thanksmarkwidth}
\titleformat{\section}{\normalfont\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\bfseries}{\thesubsection}{1em}{\itshape}
\titleformat{\subsubsection}{\normalfont\bfseries}{\thesubsubsection}{1em}{\itshape}
\titlespacing*{\section}
  {0pt}{-.1\baselineskip}{-.1\baselineskip}  
\titlespacing*{\subsection}
   {0pt}{-.1\baselineskip}{-.1\baselineskip}  
\usepackage[T1]{fontenc}

%% my packages:
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{subfig}
\usepackage{bm}
\usepackage{courier}
\usepackage{float}
\usepackage[toc,page]{appendix}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue
}
\usepackage{algorithm}
\usepackage{algpseudocode}
\algrenewcommand\algorithmicensure{\textbf{Result:}}
%%


\date{}
\providecommand{\keywords}[1]
{
   \small	
  \textit{\hspace{-1em} Keywords: } #1
}
% \newfontfamily\headingfont[]{Arial}
%Add Author names to headers and footers here:
\renewcommand{\footrulewidth}{0.4pt}% default is 0pt
\fancypagestyle{firstpage}{
  \fancyhf{}% clear all fields
  \fancyfoot[L]{ \footnotesize } % first page footnote
  \lhead{ \footnotesize UZH Master Program in Biostatistics} % header
\rhead{ \footnotesize Master Thesis (STA495)}
}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{ \footnotesize } % journal name, if there is one?
\fancyhead[RO]{ \footnotesize  \thepage} % top right
\fancyfoot[L]{ \footnotesize Daniel Leibovitz, daniel.leibovitz@uzh.ch}
\fancyhead[RE]{ \footnotesize  \thepage} % top right
%% end latex setup


% set argmin and argmax as math operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

% import libraries, code, and data
<<import, echo=FALSE, results='hide', message=FALSE, warning=FALSE>>=
library(lattice)
library(flexmix)
library(fdrtool)
library(dplyr)
library(data.table)
library(readr)
library(rnaturalearth)
library(ggnewscale)
library(gtable)
library(ggplot2)
library(RColorBrewer)
library(grid)
library(ggpubr)
library(gganimate)
library(gifski)
library(av)
library(rbenchmark)
library(rgeos)
library(utile.visuals)
# library(MASS)
library(graphics)
library(LICORS)
library(LaplacesDemon)
library(R.matlab)
library(matlabr)
library(monreg)
library(formatR)
library(scales)


# import monotone_mixture code
source("../monotone_mixture/monotone_driver/part_fit.R")
source("../monotone_mixture/monotone_driver/M_driver.R")
source("../monotone_mixture/pseudo_data/data_generator.R")
source("../monotone_mixture/monotone_driver/flex_wrapper.R")

# import GDP/Life expectancy data

lifex <- read.csv("../monotone_mixture/monotone_driver/API_SP.DYN.LE00.IN_DS2_en_csv_v2_1926713.csv", skip = 3)
continent <- read.csv("../monotone_mixture/monotone_driver/Metadata_Country_API_NY.GDP.MKTP.CD_DS2_en_csv_v2_1994746.csv")
gdp2 <- read.csv("../monotone_mixture/monotone_driver/API_NY.GDP.PCAP.CD_DS2_en_csv_v2_1926744.csv", skip=3)
  


lifex$Country.Name <- as.character(lifex$Country.Name)
lifex$Country.Name[which(lifex$Country.Name %in% c(
"Antigua and Barbuda",
"Bahamas, The" ,
"Bosnia and Herzegovina", 
"Brunei Darussalam" ,
"Cabo Verde"  ,
"Cayman Islands", 
"Central African Republic" ,
"Congo, Dem. Rep.", 
"Congo, Rep." ,
"Cote d'Ivoire", 
"Curacao" ,
"Czech Republic", 
"Dominican Republic", 
"Egypt, Arab Rep." ,
"Equatorial Guinea" ,
"Eswatini" ,
"Faroe Islands", 
"French Polynesia", 
"Gambia, The" ,
"Hong Kong SAR, China",
"Iran, Islamic Rep.",
"Korea, Rep." ,
"Kyrgyz Republic", 
"Macao SAR, China", 
"Marshall Islands" ,
"Micronesia, Fed. Sts.", 
"Russian Federation" ,
"Sao Tome and Principe",  
"Sint Maarten (Dutch part)", 
"Slovak Republic" ,
"Solomon Islands" ,
"South Sudan",
"St. Lucia" ,
"St. Vincent and the Grenadines", 
"Syrian Arab Republic",
"Venezuela, RB" ,
"Virgin Islands (U.S.)",
"Yemen, Rep." 
))] <- c( "Antigua and Barb.",
 "Bahamas",
 "Bosnia and Herz." ,
 "Brunei" ,
 "Cape Verde", 
 "Cayman Is." ,
 "Central African Rep.", 
 "Dem. Rep. Congo",
 "Congo",
 "Côte d'Ivoire",
 "Curaçao",
 "Czech Rep.",
 "Dominican Rep.",
"Egypt",
 "Eq. Guinea",
"Swaziland",
"Faeroe Is.",
 "Fr. Polynesia",
"Gambia",
 "Hong Kong",
 "Iran",
 "Korea",
 "Kyrgyzstan",
 "Macao",
 "Marshall Is.",
"Micronesia",
 "Russia",
 "São Tomé and Principe" ,
 "Sint Maarten",
 "Slovakia",
 "Solomon Is.",
 "S. Sudan",
 "Saint Lucia",
 "St. Vin. and Gren.",
 "Syria",
"Venezuela",
 "U.S. Virgin Is.",
 "Yemen"
 )
lifex$Country.Name <- as.factor(lifex$Country.Name)




gdp2$Country.Name <- as.character(gdp2$Country.Name)
gdp2$Country.Name[which(gdp2$Country.Name %in% c(
"Antigua and Barbuda",
"Bahamas, The" ,
"Bosnia and Herzegovina", 
"Brunei Darussalam" ,
"Cabo Verde"  ,
"Cayman Islands", 
"Central African Republic" ,
"Congo, Dem. Rep.", 
"Congo, Rep." ,
"Cote d'Ivoire", 
"Curacao" ,
"Czech Republic", 
"Dominican Republic", 
"Egypt, Arab Rep." ,
"Equatorial Guinea" ,
"Eswatini" ,
"Faroe Islands", 
"French Polynesia", 
"Gambia, The" ,
"Hong Kong SAR, China",
"Iran, Islamic Rep.",
"Korea, Rep." ,
"Kyrgyz Republic", 
"Macao SAR, China", 
"Marshall Islands" ,
"Micronesia, Fed. Sts.", 
"Russian Federation" ,
"Sao Tome and Principe",  
"Sint Maarten (Dutch part)", 
"Slovak Republic" ,
"Solomon Islands" ,
"South Sudan",
"St. Lucia" ,
"St. Vincent and the Grenadines", 
"Syrian Arab Republic",
"Venezuela, RB" ,
"Virgin Islands (U.S.)",
"Yemen, Rep." 
))] <- c( "Antigua and Barb.",
 "Bahamas",
 "Bosnia and Herz." ,
 "Brunei" ,
 "Cape Verde", 
 "Cayman Is." ,
 "Central African Rep.", 
 "Dem. Rep. Congo",
 "Congo",
 "Côte d'Ivoire",
 "Curaçao",
 "Czech Rep.",
 "Dominican Rep.",
"Egypt",
 "Eq. Guinea",
"Swaziland",
"Faeroe Is.",
 "Fr. Polynesia",
"Gambia",
 "Hong Kong",
 "Iran",
 "Korea",
 "Kyrgyzstan",
 "Macao",
 "Marshall Is.",
"Micronesia",
 "Russia",
 "São Tomé and Principe" ,
 "Sint Maarten",
 "Slovakia",
 "Solomon Is.",
 "S. Sudan",
 "Saint Lucia",
 "St. Vin. and Gren.",
 "Syria",
"Venezuela",
 "U.S. Virgin Is.",
 "Yemen"
 )
gdp2$Country.Name <- as.factor(gdp2$Country.Name)
@
% data cleaning
<<clean, echo=FALSE, results='hide', message=FALSE, warning=FALSE>>=

# clean lifex csv
lifex <- lifex[,c(-3, -4, -64, -65, -66)]
names(lifex)[3:61] <- substring(names(lifex)[3:61],2,5)
lifex <- melt(setDT(lifex), id.vars = 1:2, variable.name = "Year")
lifex <- lifex[complete.cases(lifex),]
lifex$Year <- as.integer(as.character(lifex$Year))
names(lifex)[4] <- "LifeExpectancy"
# lifex <- lifex[,-2]

# clean gdp2 csv
gdp2 <- gdp2[,c(-2,-3, -4, -64, -65, -66)]
names(gdp2)[2:60] <- substring(names(gdp2)[2:60],2,5)
gdp2 <- melt(setDT(gdp2), id.vars = 1, variable.name = "Year")
gdp2 <- gdp2[complete.cases(gdp2),]
gdp2$Year <- as.integer(as.character(gdp2$Year))
names(gdp2)[3] <- "GDP"


# merge
le <- merge(lifex, gdp2, by.x = c("Country.Name", "Year"), by.y = c("Country.Name", "Year"))

continent <- merge(le, continent, by.x = c("Country.Code"), by.y = c("Country.Code"))

le <- le[,-3] # remove country code

@


%Article Title goes here
\title{\large  \textbf{Mixtures of Partially Linear Models with Monotone Shape Constraints} \vspace{-1.5em}} 
%Author names and affiliations go here, any Acknowledgements / authors' note / disclosure statement goes into \thanks{}
\author{\normalsize Daniel Leibovitz \\ \normalsize UZH, Z{\"u}rich 
\vspace{1em} \\ 
  \small supervised by \\
  \normalsize Matthias L{\"o}ffler \\ \normalsize ETH, Z{\"u}rich 
  \thanks{Author: daniel.leibovitz@uzh.ch} 
  \thanks{Supervisor: matthias.loeffler@stat.math.ethz.ch} 
  \thanks{Date submitted: 2021-04-30}} 
\renewcommand\footnotemark{}
\begin{document}
\raggedbottom
\maketitle
\thispagestyle{firstpage}
\vspace{-8em}
\begin{abstract}
  \noindent  
  Mixtures of non-parametric monotone regressions are applicable to clustering problems where there is prior knowledge about appropriate shape constraints within the resulting model. The current standard for estimating such models involves fitting a series of non-parametric regression functions without shape constraints using an EM algorithm, as described by \cite{zhangetal}, followed by a monotonic estimate given the resulting latent variable classifications. In this paper, we propose to remove redundancy by incorporating the non-parametric monotone regression function estimate into the M-step of the EM algorithm. We demonstrate the effectiveness of the algorithm when applied to both simulated data and real-world data on global life expectancy and GDP from the World Bank.
\end{abstract}

\keywords{\textit{Mixture Models, Shape Constraints, Isotonic Regression} \vspace{8ex}}

\pagebreak
\section*{Acknowledgments}

I would like to thank: Drs. Holger Dette, Runze Li, and Mian Huang for providing the necessary code to run the Zhang et al. algorithm. Dr. Matthias L{\"o}ffler for initially suggesting the topic of this manuscript, and for pushing me in the right direction on all the occasions when I seemed to have reached a dead end. The faculty of the Master's in Biostatistics at the UZH for two intense, creative and academically stimulating years, and for shepherding me on an academic journey from a state of statistical ignorance to a state of somewhat less statistical ignorance. Dr. Torsten Hothorn in particular for agreeing to sponsor this master thesis and thereby demonstrating significant trust that I would return with something useful. My family for supporting me through an unexpected number of video calls and a global pandemic. My friends in Z{\"u}rich for making 6 months writing a thesis in home-office not only something I don't regret, but something I would do again.


\pagebreak
\renewcommand*\contentsname{\large Table of Contents}
\tableofcontents

\pagebreak
\section{Introduction}

A mixture of partially linear regressions with monotone shape constraints takes the following form:

\begin{equation} \label{modstrucbrief}
  Y = 
  \begin{dcases}
    \sum_{h=1}^{p} g_{h1} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{j1} X_{j} \ +\ \epsilon_1, \text{  with probability $\pi_1$; }\\
    \vdots \\
    \sum_{h=1}^{p} g_{hk} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{jk} X_{j} \ +\ \epsilon_k, \text{  with probability $\pi_k$; } \\
  \end{dcases}
\end{equation}

where the model has $K$ components, $\boldsymbol{X} \in \mathbb{R}^q$, $\boldsymbol{Z} \in \mathbb{R}^p$, $\beta \in \mathbb{R}^q$, and each function $g_{hk}()$ is assumed to be monotone. The error $\epsilon$ is assumed to be Gaussian distributed with mean zero and to be independent of the covariates $(\boldsymbol{X}, \boldsymbol{Z})$. The prior probabilities $\pi_k$ satisfy the conditions $\pi_k \in (0,1)$ and $\sum_{1}^{k} \pi_k = 1$.

Such a model has broad applications for clustering of data in domains where monotone relationships are known \emph{a priori}. These domains include, for example, epidemiology, where risk-exposure relationships may be modeled monotonically (\cite{morton}, \cite{carcinogen}); finance, where option pricing functions may be restricted to both monotonicity and and/or or convexity (\cite{optionpricing}); and biomedical research, where biochemical kinetics may be monotone over time (\cite{kinetics}). 


A similar type of model has previously been described by \cite{zhangetal}, and takes the following form for a $k$-component mixture:

\begin{equation} \label{zhangstruc}
  Y = 
  \begin{dcases}
    g_{1} (Z) \ +\  \epsilon_1 , \text{  with probability $\pi_1(Z)$; }\\
    \vdots \\
    g_{k} (Z) \ +\  \epsilon_k , \text{  with probability $\pi_k(Z)$; }\\
  \end{dcases}
\end{equation}

where $\boldsymbol{Z} \in \mathbb{R}^1$, and each function $g_{k}()$ is assumed monotone. The variables $\epsilon_k$ and $\pi_k$ are not constant, as in model [\ref{modstrucbrief}], but are modelled as nonparametric functions $\epsilon_k(Z)$ and $\pi_k(Z)$. They nonetheless satisfy the same conditions as in model [\ref{modstrucbrief}], namely, $\pi_k(Z) \in (0,1)$ and $\sum_{1}^{k} \pi_k(Z) = 1$ for any $Z$, and $\epsilon_k(Z)$ is Gaussian distributed with $E(\epsilon_k | Z) = 0$ and $Var(\epsilon_k | Z) = \sigma_k(Z)$. 

The model and estimator of Zhang et al. has four drawbacks:

\begin{enumerate} %[noitemsep] 
  \item The model proposed by Zhang et al. cannot be generalized to a semiparametric approach, i.e., one cannot include linear effects in the mixture components. Being able to include linear effects in the mixture components is conducive to two distinct purposes: 
    \begin{enumerate} %[noitemsep]
      \item First, it can be used when the data to be clustered has multiple independent variables that are not of primary interest, but which the user would still like to include in the model. As Zhang et al. point out, including such varibles with nonparametric effects can explode the complexity of the algorithm beyond usability (see Section [\ref{seccomplex}]). Including such variables as linear effects is a alternative that keeps the complexity of the algorithm tractable.
      \item Second, users who are mainly concerned with linear effects of components within a mixture model can flexibly control for one or more nuisance variables that are suspected of being monotone in effect.
    \end{enumerate}
  \item The Zhang et al. estimator fits mixture components in two, sequential steps, first fitting an unconstrained function and then applying monotone constraints once the components membership has been calculated. This approach introduces a potential bias when components are not clearly identifiable.
  \item The Zhang et al. estimator requires a tuning parameter for the fitting of each unconstrained mixture component function, which is difficult in practice and adds computational complexity.
  % \item Zhang et al. state that their model can be extended to the multivariate case, i.e., where $Z$ in model \ref{zhangstruc} is multivariate, but decline to explicitly define this model or its estimation.
\end{enumerate}

We propose model [\ref{modstrucbrief}] as an alternative, more generalized form of the approach suggested by Zhang et al. that avoids the drawbacks mentioned above. Specifically, our model accepts any number of non-parametric monotone or linear effects within each mixture component; it fits the monotone functions within each component in a single step; and it does not require the calibration of any tuning parameters.

The remainder of this paper is organized as follows. In Section 2, we discuss previous research in the domains of mixture models, partial linear models, and isotonic regression. In Section 3, we discuss the components of the proposed model. In Section 4, we describe the theoretical structure of the proposed model (model [\ref{modstrucbrief}]) as well as the estimation algorithm, followed by the model's asymptotic properties and empirical complexity. In Section 5, we apply the proposed model to simulated data as well as World Bank data on global life expectancy through the end of the 20\textsuperscript{th} century. In the final section, we discuss implications and future work.


\section{Previous Work}

The model proposed in this article draws from several branches of statistical research. In this section, we briefly discuss the history and current state of said research.

\subsection{Regression with shape constraints}

Parametric regression models are constrained in their shape by construction. They can be further constrained, often trivially, by estimating their shape within a limited parameter space. A univariate linear model $E(Y) = X\beta$, for example, can be constrained to be non-decreasing by estimating $\hat{\beta}$ to be non-negative, i.e., $\hat{\beta} \in [0,+\infty)$. When estimating nonparametric regressions, this triviality is lost and one must reconsider how to estimate constrained functions. The resulting estimators are often applications of classical techniques such as maximum likelihood, and are often free of tuning parameters, making them attractive alternatives to typical, unconstrained nonparametric estimators (\cite{guntu}).

Several types of constraint have been considered over the years. Frisen described unimodal regression, i.e., the case where for $E(Y) = f(X)$, $f()$ has a unique local maximum or local minimum (\cite{unimodal}). Convex/concave regression, where the first derivatives of the estimated functions are non-decreasing/non-increasing respectively, was first given a least-squares point estimate by \cite{hildreth}. The Hildreth estimate was later proved to be consistent by \cite{hanson}, while its rate of convergence was established by \cite{mammenconvex}. More recently, convex estimation has been considered by \cite{seijo}, \cite{mazumder}, \cite{kuosmanen}, and \cite{groeneboom}.

Isotonic, or monotone, regression and its variants have received the most attention in the statistical literature, perhaps due to their continuing relevance. Monotone regression can be defined most simply as a mapping $f: x \rightarrow y$ where $f$ is non-decreasing or non-increasing over the range of $x$. Monotone regressions have seen diverse applications across research domains; They have been used by \cite{jianhua} to analyze dose-response in bioinformatics, by \cite{luss} to estimate gene-gene interactions, and by \cite{diggle} to estimate disease risk as functions of spatial exposure, to name just a few examples. 

Isotonic regression maximum likelihood estimators with no smoothness requirements were first explored by \cite{brunk} and \cite{grenander}. The asymptotic distribution of these estimators was later established by \cite{wright}. The Pool Adjacent Violators algorithm for the estimation of the isotonic regression MLE with Gaussian distributed errors was first suggested by \cite{ayer}.

The literature regarding isotonic regression diverges at the point of choosing an estimator based on the crucial assumption of whether the resulting function is smooth. For applications in which a step-wise function is acceptable, the estimators of Brunk, Grenander, Ayer, etc., have an exact solution and no tuning parameter. However, for applications in which a smooth function is required, alernate approaches have had to be developed, and a two-step procedure has become common. These two-step procedures either estimate a smooth function and apply a monotonic constraint on the resulting function (\cite{friedman}), or estimate a monotonic function and then smooth the resulting estimate (\cite{cheng}). Mammen compares the asymptotic behaviours of these approaches in a comprehensive treatment of smooth, monotonic nonparametric regression. Although these procedures differ in their asymptotic behaviours, all of them require the selection of a tuning parameter (\cite{mammen}).

% discuss history, estimation methods (splines, inversion,), variance bounds.

% discuss monotone regressions in particular. different approaches -- pava, active set, etc.




\subsection{Partial Linear Models}

The Generalized Additive Model, or GAM (see equation [\ref{gameq}]), was first suggested by \cite{hastiegam}, and can be seen both as a generalization of the GLM to include nonparametric terms, and as a generalization of additive models to models with error terms from the exponential family. GAMs provide a large amount of flexibility to the nonparametric modeling of multivariate data, but avoid the slow convergence -- the curse of dimensionality -- associated with multivariate non-parametrics by imposing an additive structure between terms. 
\begin{equation} \label{gameq}
  g(E(X)) \ =\ \beta_0 + \sum_{i=1} f_i(x_i)
\end{equation}

Partial Linear Models, or PLMs, predate GAMs by several years, having been introduced in 1986 by \cite{engle} for the modelling of weather and energy-use. PLMs can nonetheless be considered more productively as a slightly more restrictive subset of GAMs, wherein some proportion of the model terms are required to be linear.

Both GAMs and PLMs have often been fit using the ``backfitting'' algorithm, first introduced by \cite{backfit} and used in the first descriptions of GAMs by Tibshirani and Hastie, though several alternatives have been developed over the years. Notably, Speckman introduced the profile least-squares estimator in 1988 \cite{speckman}; Hua proposed the penalized spline estimator \cite{hua}; and Hamilton and Truong proposed the local linear estimator \cite{truong}. A thorough review of these various estimators, including numerical comparisons, was performed by \cite{hua}.

\subsection{Mixture Models} \label{mmhistory}

Model-based clustering, or mixture models, are a common method for producing models of latent clusters with probabalistic or ``soft'' cluster assignment. The history of mixture models is particularly long, with the first such models having been implemented more than a century ago by \cite{newcomb} and \cite{pearson}. In much of the following century, progress in the theory of mixture models and their estimation stalled due to a lack of computational power and for lack of an efficient estimating strategy.

This dry spell was called to a close by the introduction by \cite{dempster} of the EM algorithm for estimation of models with supposed latent variables, which vastly simplified the estimation of mixture models. Since then, development and research in mixture modelling has proceeded rapidly, with implementations of Bayesian mixture models (\cite{marin}), infinite mixture models (\cite{infinite}), \emph{must-link} and \emph{cannot-link} constraints (\cite{wagstaff}), and many variations of normal mixture models (\cite{mclachlannormal}, \cite{fraley}) all having been published within the last two decades.

Yet more recently, mixtures of regressions have received increased attention (\cite{mixglm}, \cite{viele}, \cite{hurn}) as easily interpretable clustering methods that specify a dependence structure amongst observed covariates without modelling the distributions of the covariates themselves. 


% discuss asymptotic MLE.

% Finite mixture models date from the 19\textsuperscript{th} century (CITE), while the more commonly known implementation via the EM algorithm was first introduced in 1977 (Dempster, Laird, Rubin).
% Ifinite mixture models: \cite{infinte}

% \subsection{Mixtures of Regressions}

% From the larger set of mixture models, mixtures of regressions play a useful role in cases where one wishes to model a dependence structure amongst observed covariates without modelling the distributions of the covariates themselves. 
% allows the analysis of such data where the researcher suspects latent categories among the observations. 


\subsection{Mixtures of Nonparametric Regressions}

Both \cite{xiang}, and \cite{huang} have considered mixtures of nonparametric regressions. The model of Xiang \& Yao estimates mixing proportions and the variance of each component as constants, while allowing the mean of each component to be a nonparametric function of the data. Huang et al., by contrast, propose a model where mixing proportions, mean and variance within each mixture component are all estimated nonparametrically.

Various attempts have been made at generalizing the above approaches to include linear effects, i.e., to model mixtures of partially linear models (PLMs) or generalized additive models (GAMs). \cite{wu} propose a structure for estimating mixtures of PLMs with a univariate nonparametric effect and arbitrary linear effects per component. Most recently, \cite{zhangpan} extended this model to accept arbitrary nonparametric effects.



% \subsection{Mixture Models with Monotone Shape Constraints}

Within the smaller subset of mixtures of non-parametric regressions, one may frequently encounter situations in which one would like to place shape constraints on some, or each, of the components in our mixture model. There has been relatively little previous work in the modelling of mixtures of specifically monotone nonparametric regressions, with the publication by Zhang et al. standing out as the only treatment of this particular issue. 

% A common such shape constraint is the monotonicity constraint, which ensures that in the regression model \( E(Y|X) = f(X) \), the function $f(X)$ is either non-increasing or non-decreasing over the range of $X$. 

\section{An Overview of Contributing Models and Estimators}

\subsection{Mixture Models and the EM Algorithm}

\subsubsection{General Finite Mixture Models}

At its most basic, a finite mixture of $K$ distributions for some positive integer $K$ can be represented by its additive distribution:

\begin{equation} \label{genmix}
  p(X) \ =\ \sum_{k=1}^{K} \pi_k \cdot p(X | \boldsymbol{\theta}_k)
\end{equation}

where the distribution of each component $k$ is parametrized by some set of parameters $\boldsymbol{\theta}_k$, and the number of observations generated by component $k$ is proportionate to $\pi_k$. Necessarily, all $\pi_k \in (0,1)$, and $\sum_{k=1}^{K}\pi_k = 1$. We assume that each observation generated by the mixture is generated uniquely by one component, such that if we are given some number $n$ of observed values $X_1,...,X_n$, we can denote their categorization within the components of a mixture by a latent (i.e., unobserved) variable $\Lambda$ such that $\Lambda_i \in \{1,..,K\}$ for all $i \in \{1,...,n\}$. Then, the distribution of $\Lambda$ can be denoted by equation [\ref{latentdist}], and the additive distribution in equation [\ref{genmix}] can be deconstructed to the conditional distribution in equation [\ref{genmixcond}].

\begin{equation} \label{latentdist}
  p(\Lambda = k) \ =\ \pi_k
\end{equation}

\begin{equation} \label{genmixcond}
  p(X_i | \Lambda_i = k) \ =\ p_k(X) \ =\ p(X | \theta_k)
\end{equation}


Typically, however, one is simultaneously estimating the number of components $K$, the mixing proportions $\pi_k$, the parameters $\boldsymbol{\theta}_k$, and the latent variable $\Lambda$, at which point it becomes more useful to consider $\boldsymbol{\Lambda}$ an $n \times k$ matrix of sequentially updated probabilities respresenting the probability of observation $n$ having been generated by component $k$. $\boldsymbol{\Lambda}$ must then meet the condition that $\sum_{k=1}^{K}\boldsymbol{\Lambda}_{ik} = 1$ for all $i \in \{1,...,n\}$.

This is an extremely flexible basic model structure, with $p(\cdot)$ only needing to be a valid probability density function. Thus, one may have a mixture of gaussians (normals), of binomials, of poissons, etc. These distributions may also be multivariate, and again, there is no restriction on the presumed distribution belonging to each dimension within each cluster $k$. Some specific such models are known by alternate names. For example, with some minimal restrictions added to the mixture of multivariate gaussians, namely, when all component covariances are diagonal, equal, and the variances are infinitesimal or the observations are given ``hard assignments'' (i.e., $\boldsymbol{\Lambda}_{ik} \in \{0,1\}$), one has the well-known $k$-means model.

% Cite? https://en.wikipedia.org/wiki/K-means_clustering#Relation_to_other_algorithms

% Given some number $n$ of observed values $X_1,...,X_n$ and a latent, categorical variable $\Lambda$ such that $\Lambda_i \in \{1,..,K\}$ for all $i \in {1,...,n}$ and some integer $K \in \mathbb{R}^1$, a general mixture model has the following structure:
% 
% \begin{equation} \label{genmixlatent}
%   p(x) \ =\ \sum_{k=1}^{K} \pi_k \cdot p(x | \Lambda_k)
% \end{equation}

\subsubsection{Finite Mixtures of Regressions} \label{fmrs}

If one further specifies the distributions $p_k()$ in equation [\ref{genmix}] as regression functions, one is left with a finite mixture of regressions. The structure of such a mixture can be described without specifying the exact form of either the regression model $Y = f_k(\cdot|\vec{X}) + \epsilon_k$ or the distribution of the random component, $\epsilon_k$, as long as there is a weighted procedure for estimating $\hat{f}(\cdot)$. 

Suppose then that, instead of univariate $X$, we observe $Y_i,...,Y_n$ and associated $\vec{X}_i,...,\vec{X}_n$. As before, we assume that each observed set $(Y_i, \vec{X_i})$ belongs to one of $\{1,...,k\}$ unobserved components for some positive integer $k$, and we denote this by a matrix of probabilities $\boldsymbol{\Lambda}$. We further assume some vector of regression model parameters $\boldsymbol{\theta}_k$. 

% \begin{equation} \label{mixlik}
%   L(\pi, \Theta) \ =\ \prod_{i=1}^n \sum_{j=1}^k \pi_j p_j(y_i\ |\ \vec{x_i},\ \Theta_j)
% \end{equation}


% where $\pi_j$ is the prior probability of component $j$, and $p_j$ is the density of $f(Y)$ at $y_i$ given observed $x_i$ and $\Theta_j$ for component $j$. 


Thus equation [\ref{genmix}] becomes equation [\ref{regmix}], in which the distribution of $Y$ is conditioned on the associated covariates $\vec{X}$. The likelihood of this model is written out in equation [\ref{mixlike}]. When the likelihood is maximized and parameters are estimated, the model provides the following:

\begin{enumerate} %[noitemsep]
  \item The previously mentioned $n \ \times\ k$ matrix $\boldsymbol{\Lambda}$ representing the posterior probability of each $(Y_i, \vec{X_i})$ belonging to each of $K$ components.
  \item A vector $\pi_1,...,\pi_k$ of prior probabilities representing the mixing proportions of each component in the larger mixture model
  \item A set of parameters $\boldsymbol{\theta}_k$ for each regression component $k$
\end{enumerate}

\begin{equation} \label{regmix}
  p(Y) \ =\ \sum_{k=1}^{K}\pi_k p(Y = y\ |\ X = x, \boldsymbol{\theta}_k)
\end{equation}

\begin{equation} \label{mixlike}
   L(\boldsymbol{\pi}, \boldsymbol{\theta}) \ =\ \prod_{i=1}^n \sum_{k=1}^{K} \pi_k p_k(y_i\ |\ \vec{x_i},\ \boldsymbol{\theta}_k)
\end{equation}


In contrast with other mixture models of multivariate data, this is in fact a 1-deminsonal mixture of conditional normals. The convenience of this structuring is that no assumptions or restrictions are placed on covariates $\boldsymbol{X}$. As a result, none of the parameters of the mixture model external to the model components, i.e., neither the priors $\boldsymbol{\pi}$ nor the estimated posterior matrix $\boldsymbol{\Lambda}$, depend on covariates $\boldsymbol{X}$ except through $Y$. One could choose to include this dependence by modelling the priors $\boldsymbol{\pi}$ as a function of the covariates, i.e., $\boldsymbol{\pi}(\boldsymbol{X})$. The result is what is sometimes called a mixture-of-experts model, with $\boldsymbol{\pi}(\boldsymbol{X})$ being called \emph{gating coefficients} or \emph{gating networks} (\cite{mixexperts}).




\subsubsection{The EM Algorithm}

The EM algorithm is a method for discovering the parameter estimates that maximize the likelihood of a model which incorporates unobserved variables representing latent clusters. If we denote such a problem as incorporating observations $x_1, ..., x_n$ and associated latent variable $z_1, ..., z_n$ where $z_i \in \{1,...,k\}$, then the EM algorithm allows us to maximize equation [\ref{emlike}], which is the general likelihood of a single observation $x$. Note that this equation already appears quite similar to the likelihood of a mixture model. A set of prior probabilities $\pi_k$ is not required, as here we have assumed that each $x_i$ has a \emph{true} cluster assignment, i.e., a ``hard'' assignment.

\begin{equation} \label{emlike}
   \ell(\theta) \ =\ \log \sum_{k=1}^{K} p_k(X=x, Z=z | \theta)
\end{equation}

The EM algorithm allows us to avoid determining the maximum of equation [\ref{emlike}] directly, and instead allows us to iteratively maximize a lower bound of the log-likelihood. To demonstrate this, we introduce an arbitrary distribution over $Z$ called $q(Z)$. This allows us to reformulate the likelihood equation as below:

\begin{equation} \label{emlikeqz}
   \ell(\theta) \ =\ \log \sum_{k=1}^{K}q(Z) \frac{p_k(X=x, Z=z | \theta)}{q(Z)}
\end{equation}

Since the log-likelihood function is concave, Jensen's inequality (equation [\ref{jenin}], \cite{jensen}) applies, which specifies that the expectation over a convex function of $X$ is greater or equal to the convex function of the expectation. This gives us the inequality in equation [\ref{emlikejensen}].

\begin{equation} \label{jenin}
   E(f(X)) \geq f(E(X)) \text{, for convex function $f()$}
\end{equation}

\begin{equation} \label{emlikejensen}
   \log \sum_{k=1}^{K}q(Z) \frac{p_k(X=x, Z=z | \theta)}{q(Z)} \ \geq\ \sum_{k=1}^{K}q(Z) \log \frac{p_k(X=x, Z=z | \theta)}{q(Z)}
\end{equation}

The function $J(q, \theta) = \sum_{k=1}^{K}q(Z) \log \frac{p_k(X=x, Z=z | \theta)}{q(Z)}$ therefore serves as a lower bound to the likelihood. Given this formulation of $J(q, \theta)$, the expectation and maximization steps of the EM algorithm, in equations [\ref{estep}] and [\ref{mstep}] respectively, can be seen as complementary maximizers of $J(q, \theta)$. Since each step only maximizes $J(q, \theta)$ with respect to either $q$ or $\theta$, the steps can only increase $J(q, \theta)$ (or keep it constant), thus giving us the inequalities in equations [\ref{eineq}] and [\ref{mineq}]. Moreover, since the lower bound on the likelihood can only increase, the likelihood can only increase, thus demonstrating that the EM algorithm converges to at least a local maximum.

\begin{equation} \label{estep}
   q^{(t)} \ = \ \argmax_q J(q, \theta^{(t)})
\end{equation}
\begin{equation} \label{mstep}
   \theta^{(t+1)} \ = \ \argmax_{\theta} J(q^{(t)}, \theta)
\end{equation}

\begin{equation} \label{eineq}
   J(q^{(t)}, \theta^{(t)}) \ \geq \ J(q^{(t-1)}, \theta^{(t)})
\end{equation}
\begin{equation} \label{mineq}
   J(q^{(t)}, \theta^{(t+1)}) \ \geq \ J(q^{(t)}, \theta^{(t)})
\end{equation}

% the EM algo reaches a local maximum. that's why we run several times.

% The EM algorithm for a mixture model


\subsubsection{Model-Based Generation and Prediction in Mixtures of Regressions} \label{prediction}

In typical mixture model frameworks, the model estimate permits the calculation of unconditioned joint distributions of the data and marginal distributions of any given variable. By extension, such typical models permit the generation of new pseudo-observations that conform with the model structure. The mixture of regressions model does not permit this type of generativity unless the distribution of the covariates is known \emph{a priori}, since the distribution of the covariates is not estimated as a part of the model. The mixture of regressions model thus requires a covariate vector $\vec{x}$ in order to produce the marginal distribution of $Y$.

Similarly, whereas typical mixture models can classify new observations in a Bayesian manner by summing over the product of the mixture prior and the density at the new observed data (equation [\ref{typ_mix}]), a mixture of regressions model must classify by summing over the density \emph{given} the observed data (equation [\ref{reg_mix}]).

\begin{equation} \label{typ_mix}
  p(Y = y) \ =\ \sum_{k=1}^{K}\pi_k p_k(\vec{Y} = \vec{y}\ | \theta_k)
\end{equation}

\begin{equation} \label{reg_mix}
  p(Z = j | X = x, \theta) \ =\ \frac{\pi_j p_j(y \ | \ X = x, \theta_j)}{\sum_{k=1}^{K}\pi_k p_k(y \ | \ X = x, \theta_k)}
\end{equation}

This is simply the standard application of Bayes' theorem (equation [\ref{bayes}]), where the densities of $y | X = x$ for each component $k$ are multiplied by the uninformed priors $\pi_k$ and normalized over the marginal distribution of $y | X = x$.

\begin{equation} \label{bayes}
  p(\theta | X) \ =\ \frac{p(X|\theta)p(\theta)}{\int_{\theta}p(X|\theta)p(\theta)}
\end{equation}


% It should be noted that the model is NOT generative, that is, there is no distribution assumed for $X$ and therefore no reasonably probabalistic way to determine a marginal distribution of $Y$ without an observed $X$.


\subsection{Partially Linear Models}

\subsubsection{Additive Linear Models} \label{gams}

The general partial linear model is an additive regression model with some finite combination of linear and non-linear components, which can be denoted thus:

\begin{equation} \label{partlin}
  Y = \sum_{h=1}^{p} g_{h} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{j} X_{j} \ +\ \epsilon
\end{equation}

where the model has $h$ non-linear covariates and $j$ linear covariates, where each $g_{h}()$ is some nonparametric function of $Z_{h}$, and where $\epsilon$ is a random variable with mean $0$.

This model overlaps broadly with Generalized Additive Models (GAMs), but differs critically in that we place no restriction on the smoothness of the non-linear components. GAMs are, however, instructive in that they are partly motivated by the difficulty in estimating non-additive, non-parametric models, and the additive structure of our partially linear model is similarly motivated.

More specifically, although nonparametric methods exist for multiple regression, they are faced with the well-known ``curse of dimensionality'' (\cite{curse}). The slow convergence associated with the curse of dimensionality is avoided by fitting the less general, additive model, where each term is fit within one dimension. Both GAMs and partial linear models can be estimated using the backfitting algorithm (\cite{backfit}), which iteratively estimates the individual terms of an additive model and, up to a user-specified threshold, determines an optimal solution.

% [THIS NEEDS TO BE DEVELOPED]

% For example, \cite{guangcheng} show that, for the model estimated in section \ref{plmmc}, the difference between the multivariate and additive estimate: 
% where it is shown that estimate \ref{partlinloss} is a consistent estimator of $\{\vec{\beta}, g_1,...,g_p\}$ 

% \subsubsection{The Backfitting Algorithm}
% 
% GAMs are additionally instructive in that they can be estimated by the backfitting algorithm (\cite{hastiegam}). Indeed, the backfitting algorithm -- DESCRIBE -- and has the following properties: DESCRIBE
% 
% and involves sequentially updating the linear and non-linear components of the partial linear model in 


\subsubsection{Partially Linear Models with Monotone Constraints} \label{plmmc}

The partial linear model that we apply in the proposed model of this paper can be denoted thus:


\begin{equation} \label{partlinmono}
  Y = \sum_{h=1}^{p} g_{h} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{j} X_{j} \ +\ \epsilon
\end{equation}


where the model has $h$ non-linear covariates with monotone shape constraints and $j$ linear covariates, and where $\epsilon \sim Normal(0, \sigma^2)$. The parameters $\vec{\beta}$ and the functions $g_1(\cdot),...g_p(\cdot)$ are determined as the minimizers of the quadratic loss function, shown in equation [\ref{partlinloss}]. The estimation of the functions $g_1(\cdot),...g_p(\cdot)$ is a problem of additive isotonic regression, discussed in the next subsection.

\begin{equation} \label{partlinloss}
  \{\hat{\vec\beta}, \hat{g}_1,...,\hat{g}_p\} = \underset{\vec{\beta}, g_1:g_p}{\operatorname{argmin}} \sum_{i=1}^{n} (y_i - \sum_{h=1}^{p} g_{h} (z_{ih}) \ -\  \sum_{j=1}^{q} \beta_{j} x_{ij})^2
\end{equation}


The MLE of the entire partial linear model is obtained via the backfitting algorithm, iterating through the two-step process (equations [\ref{backfit1}], [\ref{backfit2}]) until convergence.

\begin{equation} \label{backfit1}
  (I) \ \ \ \ \ \ \ \ \ \{\hat{g}_1,...,\hat{g}_p\} = \underset{g_1:g_p}{\operatorname{argmin}} \sum_{i=1}^n\left(y_i-\sum_{j=1}^{q} \beta_{j} x_{ij}-\sum_{h=1}^{p} g_{h} (z_{ih})\right) \\
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  holding \ \ \ \vec\beta \ \ \ fixed
\end{equation}


\begin{equation} \label{backfit2}
  (II) \ \ \ \ \ \ \ \ \ \hat{\vec{\beta}} = \underset{\vec{\beta}}{\operatorname{argmin}} \sum_{i=1}^n\left(y_i-\sum_{h=1}^{p} g_{h} (z_{ih}) - \sum_{j=1}^{q} \beta_{j} x_{ij}\right) \\
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  holding \ \ \ \{g_1,...,g_p\} \ \ \ fixed
\end{equation}

This model and estimator is thoroughly explored by \cite{guangcheng}, where it is shown that, under certain conditions (see section [\ref{abplmmc}] of the appendix), $\hat{\beta}_n$ is $\sqrt{n}$-consistent (equation [\ref{asymbeta}]), while the estimates $\{\hat{g}_1,...,\hat{g}_p\}$ converge in distribution as to a two-sided brownian motion plus a parabola (equation [\ref{asymg}]). 

\begin{equation} \label{asymg}
\begin{aligned}
  n^{1/3}\frac{(2p_{Z_h}(z_h))^{1/3}}{\sigma^{2/3}g_h(z_h)^{1/3}}[\hat{g}_h(z_h) - g_h(z_h)] &\xrightarrow{d} GCM(Z(t) + t^2) \\ 
  \text{where } p_{Z_h}(z_h) &\text{ is the density of $Z_h$ evaluated at $z_h$,} \\
  GCM(Z(t)) &\text{ is the greatest convex minorant of } Z(t) \\
  \text{and } Z(t) &\text{ is a two-sided Brownian motion} 
\end{aligned}
\end{equation}

\begin{equation} \label{asymbeta}
\begin{aligned}
  \sqrt{n}(\hat{\beta}_n - \beta_0) &\xrightarrow{d} N(0, \Sigma) \\
  \text{where } \Sigma &= \sigma^2[E(X - \sum_{h=1}^{p}E(X|Z_h))^{\otimes 2}]^{-1} \\
  \text{and } \sigma^2 &= Var(\epsilon)
\end{aligned}
\end{equation}

The asymptotic distribution of $\hat{\beta}$ evidently has a larger variance than in the case of the ordinary least squares estimate. As Cheng points out, this can be considered the cost of including non-parametric terms in the regression model. 

The rate of convergence for the non-parametric terms is slower than that of the linear terms, as expected, but significantly faster than would be the case if the nonparametric terms were multivariate monotone rather than univariate monotone and additive. Moreover, Cheng demonstrates that in the additive case, the estimators $\{\hat{g}_1,...,\hat{g}_p\}$ possess the oracle property, meaning each $\hat{g}(\cdot)$ can be estimated as well as it could be if all other components, including all other $\hat{g}(\cdot)$, are known. Thus the rate of convergence of any $\hat{g}(\cdot)$ is not diminished by the inclusion of other monotone terms.
% Moreover, 


\subsection{Isotonic Regression}

\subsubsection{Univariate Isotonic Regression}
At the most basic level, with univariate $x$ and $y$ and a simple ordering amongst $x$ such that \( x_{1} \leq x_{2} \leq ... \leq x_{n} \) for all \( x_{i} \in X \), isotonic regression determines a non-decreasing function $g(\cdot)$ such that \( g(x_{1}) \leq g(x_{2}) \leq ... \leq g(x_{n}) \) and for which \( \hat{g}(\cdot) = \argmin_{g} \sum_{i=1}^{n}||g(x_{i}) - x_{i}||_{L} \) for some loss function $||\cdot||_{L}$. If observations are weighted, the objective function becomes \( \hat{g}(\cdot) = \argmin_{g} \sum_{i=1}^{n}w_{i}||g(x_{i}) - x_{i}||_{L} \) for weights $w$. In the so-called antitonic case, the function $g(\cdot)$ is non-increasing such that \( g(x_{1}) \geq g(x_{2}) \geq ... \geq g(x_{n}) \). Without loss of generality, from this point on we are only concerned with the non-decreasing case.

If we consider specifically the squared error loss, our risk function and weighted risk function become equations [\ref{lseisotonic}] and [\ref{lseweightedisotonic}] respectively. Equation [\ref{lseweightedisotonic}] can alternately be written explicitly in the form of a min-max formula, as in equation [\ref{minmax}] (\cite{jordan}), giving it a characterization which facilitates the study of the properties of the estimator $\hat{g}(\cdot)$. Equation [\ref{minmax}] breaks the function $g(\cdot)$ into non-decreasing ``blocks'', and assigns to each block the weighted mean of the values $x$ contained in that block.


\begin{equation} \label{lseisotonic}
  \hat{g}(\cdot) = \argmin_{g} \sum_{i=1}^{n}(g(x_{i}) - x_{i})^2
\end{equation}

\begin{equation} \label{lseweightedisotonic}
  \hat{g}(\cdot) = \argmin_{g} \sum_{i=1}^{n}w_{i}(g(x_{i}) - x_{i})^2
\end{equation}


\begin{equation} \label{minmax}
  \hat{g}(x_{i}) = \min_{j \geq i} \max_{k \leq j} \frac{\sum_{k=k}^{j}w_{k}x_{k}}{\sum_{k=k}^{j}w_{k}} \text{,   } i = 1,...,n
\end{equation}

One can see from equation [\ref{minmax}] that determining the estimate $\hat{g}(\cdot)$ for the least square isotonic regression returns a step function, and requires no tuning parameter.

% \subsubsection{Multivariate Isotonic Regression}
% 
% For the sake of completeness, we briefly describe multivariate isotonic regression. However, we shall see that, as noted in section \ref{gams}, the rate of convergence in multivariate isotonic functions suffers the well-known ``curse of dimensionality'', rendering it of low practical use in instances where the dimensionality of the data is high and the number of observations is not extremely large.






\subsubsection{The Pool Adjacent Violators Algorithm}

A well-known and efficient way to obtain the least square estimate of $g()$ is through the Pool Adjacent Violators Algorithm (PAVA). The PAVA -- for univariate monotone regression (equation [\ref{pava}]) -- returns a step-function fit without either having to select a bandwidth or having to set a congergence tolerance parameter. For multivariable monotone regression (equation [\ref{cpav}]), one must take a different approach, suggested by Bacchetti, called the Cyclic Pool Adjacent Violators Algorithm (CPAV). Within the CPAV, one iterates through each univariate function sequentially and update univariate monotone functions until convergence, returning the additive model of equation [\ref{cpav}].



\begin{equation} \label{pava}
  Y = g(X) \ +\ \epsilon
\end{equation}


\begin{equation} \label{cpav}
  Y = \sum_{h=1}^{p} g_{h} (X_{h}) \ +\ \epsilon
\end{equation}


% Compare our algo (Algo II) to previous algo (Algo I) here.
% 
% Main difference between algorithms: 
% Algo I has non-parametric priors and non-parametric normal variances which are both functions of X (pi_c(x) and sig_c(x) for component c).
% Algo I provides smooth monotonic estimates.
% Algo I is not readily generalizable to partially linear models
% Algo I uses CV to estimate tuning parameters for kernel density estimation in both mixture on non parametrics AND monotone estimate.
% Steps of Algo I: 
% 1. mixture of nonparametric regressions (nonparametric estimate? bandwidth/lambda?)
% 2. monotone constraint 

\section{Proposed Model}


\subsection{Model Definition}

The model proposed in this article has the following structure:

\begin{equation} \label{modstruc}
  Y = 
  \begin{dcases}
    \sum_{h=1}^{p} g_{h1} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{j1} X_{j} \ +\ \epsilon_1 \text{  with probability $\pi_1$; } \\
    \vdots \\
    \sum_{h=1}^{p} g_{hk} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{jk} X_{j} \ +\ \epsilon_k \text{  with probability $\pi_k$; }\\
  \end{dcases}
\end{equation}

where $\pi_k$ represents the prior probability of mixture component $k$; $g_{hk}(\cdot)$ represents the monotone function of variable $h$ within component $k$; $\beta_{jk}$ represents the linear effect of variable $j$ within mixture component $k$; and $\epsilon_k$ represents the error associated with component $k$. All $\epsilon_k$ are assumed to be normally distributed with mean $0$, and are assumed to be independent of the covariates $(\boldsymbol{X}, \boldsymbol{Z})$ . All $\pi_k$ are assumed to be unknown constants, and all $\pi_k \in (0,1)$ such that $\sum^{K} \pi_k = 1$.

There is no requirement that the $K$ regression functions in model [\ref{modstruc}] be identical. Specifically, $g_{hk}()$ for any $h \in p$ and any $k \in K$ can be set as monotone non-increasing, monotone non-decreasing, or absent, regardless of the other $g_{hk}$. Likewise, the number of linear effects $\beta_{j}$, including the intercept $\beta_0$, need not be same across different components $k$.


\subsection{Model Estimation}

The proposed model is obtained from a series of nested, iterative algorithms, described below. Algorithm 1 describes the EM algorithm for fitting mixture priors and observation posteriors. Algorithm 2 describes the weighted partial linear regression for the fitting of each component within each M-step of the EM algorithm. Algorithm 3 describes the weighted, cyclic pool adjacent violators algorithm for cases where there is more than one monotone function fit within a single partial linear regression. Algorithm 4 describes the weighted pool adjacent violators algorithm for fitting a single monotone regression. In all cases, convergence thresholds are set by the user.

\begin{algorithm}[H]
\caption{EM algorithm for Finite Mixtures of Regressions}
\begin{algorithmic}[1]
% % \SetAlgoLined

\Require
  \Statex $x$ — an \(n \times p\) matrix (independent variables with no shape constraint)
  \Statex $z$ — an \(n \times q\) matrix (independent variables with monotone shape constraint)
  \Statex $y$ — an \(n \times 1\) matrix (dependent variable)
  \Statex $k$ — a positive integer representing the number of categories of latent variable L

\Ensure
  \Statex \( \Lambda \) — an \(n \times k\) matrix representing the posterior probability of observation \(i = 1,...,n\) belonging to latent category \(j = 1,...,k\). Additionally, for all \(i = 1,...,n\) and \(j = 1,...,k\), \( \Lambda_{ij} \) is a real number in the range \([0,1]\), and \( \sum_{j=1}^{k} \Lambda_{ij} = 1 \)
  \Statex $\vec{\pi}$ — a vector $\pi_1,...,\pi_k$ of prior probabilities representing the mixing proportions of each component in the larger mixture model
  \Statex $\vec{\Theta}$ — a set of parameters $\Theta_k$ for each regression component $k$
  \Statex % blank line 
\State Set iteration index $d \leftarrow 1$
\For{\( i \in 1,...n \)}
\State With uniform probability across $k$, assign one of the elements of \( [\Lambda_{i1}, ..., \Lambda_{ik}] \) to 1 and all other to 0, such that \( \Lambda_{i} = [0,..., 1, ...,0] \)
\EndFor
\algstore{myalg}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]                     
\begin{algorithmic} [1]                   
\algrestore{myalg} % continue where we left off
 \While{algorithm is not converged}
  % M-step:\;
  \For{\(j \in 1,...,k\)} \Comment{Begin M-step}
  
  \State Set prior mixture proportion \( \displaystyle{\pi_{j}^{(d)} \leftarrow \frac{1}{n}\sum_{i=1}^{n}\Lambda_{ij}^{(d-1)}} \)
\State Set weighted partial linear model regression parameters such that \( \displaystyle{[\hat{\beta}_{j}, \hat{g}_{j}]^{(d)} \leftarrow \argmin_{\beta, g} \sum_{i=1}^{n}\Lambda_{ij}^{(d-1)}(y - x\beta_{j} - g_{j}(z))^{2} } \) (See Algorithm 2, WPLR)
  \EndFor \Comment{End M-step}

  % E-step:\;
  \For{\(i \in 1,...,n\)} \Comment{Begin E-step}
    \For{\(j \in 1,...,k\)}
    \State Set \( \Lambda_{ij}^{(d)} \leftarrow \pi_{j}^{(d)}p(y_{i} | x_{i}, \beta_{j}^{(d)}, g_{j}^{(d)}(z_{i})) \), where \(p(y_{i} | x_{i}, \beta_{j}^{(d)}, g_{j}^{(d)}(z_{i}))\) is the density of a $Normal$ distribution with \(\mu = x_{i}\beta_{j}^{(d)} + g_{j}^{(d)}(z_{i})\) and \(\sigma = \sqrt{\frac{\sum w_{i}r^{2}/\bar{w}}{n-rk(X)}}\) 
    \EndFor
    % sigma = sqrt(sum(wates * (resids)^2 / mean(wates))/ (nrow(x)-qr(x)$rank))
  \State Normalize the posterior probabilities \( [\Lambda_{i1},...,\Lambda_{ik}]^{(d)} \) such that \( \displaystyle{\sum_{j=1}^{k}\Lambda_{ij}^{(d)} = 1} \)
  \EndFor \Comment{End E-step}

  \State $d = d + 1$

 \EndWhile

 \end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{Weighted Partial Linear Regression} \label{wplralgo}
\begin{algorithmic}[1]
\Require
  \Statex $x$ — an \(n \times p\) matrix (independent variables with no shape constraint)
  \Statex $z$ — an \(n \times q\) matrix (independent variables with monotone shape constraint)
  \Statex $y$ — an \(n \times 1\) matrix (dependent variable)
  \Statex $w$ — an \(n \times 1\) matrix (observation weights)

\Ensure
  \Statex \( \hat{\beta},\hat{g_{1}},...,\hat{g_{q}} \) such that \( \displaystyle{[\hat{\beta},\hat{g_{1}},...,\hat{g_{q}}] = \argmin_{\beta, g_{1},...,g_{q}} \sum_{i=1}^{n} w_{i}(y_{i} - \sum_{h=1}^{q}g_{h}(z_{ih}) - x_{i}\beta)^{2}} \)
  \Statex % blank line

  \State Set iteration index $b \leftarrow 1$
  \State Set \( \displaystyle{\hat{\beta}^{(0)} \leftarrow \beta_{x}} \), where \( \displaystyle{[\beta_{x}, \beta_{z}] = \argmin_{\beta_{x}, \beta_{z}} \sum_{i=1}^{n} w_{i}(y_{i} - z_{i}\beta_{z} - x_{i}\beta_{x})^{2}}\)
 \While{algorithm is not converged}
  \If{$z$ is univariate} \Comment{See Algorithm 4, PAVA}
  \State Set \( \displaystyle{\hat{g}^{(b)} \leftarrow \argmin_{g} \sum_{i=1}^{n} w_{i}([y_{i} - x_{i}\beta^{(b-1)}] - g(z_{i}))^{2}} \) holding \( \beta^{(b-1)} \) fixed. 
  \Else \Comment{See Algorithm 3, CPAV}
  \State Set \( \displaystyle{\sum_{h=1}^{q}\hat{g}_{h}^{(b)} \leftarrow \argmin_{g_{1},...,g_{q}} \sum_{i=1}^{n} w_{i}([y_{i} - x_{i}\beta^{(b-1)}] - \sum_{h=1}^{q}g_{h}(z_{ih}))^{2}} \) holding \( \beta^{(b-1)} \) fixed. 
  \EndIf
  \State Set \( \displaystyle{\hat{\beta}^{(b)} \leftarrow \argmin_{\beta} \sum_{i=1}^{n} w_{i}([y_{i} - g^{(b)}(z_{i})] - x_{i}\beta)^{2}} \) holding \( g^{(b)} \) fixed.
  \State $b = b + 1$

 \EndWhile
 \end{algorithmic}
\end{algorithm}



\begin{algorithm}[H]
\caption{Weighted Cyclic Pool Adjacent Violators Algorithm} \label{cpavalgo}
\begin{algorithmic}[1]

\Require
  \Statex $x$ — an \(n \times q\) matrix (independent variables)
  \Statex $z$ — an \(n \times 1\) vector (observation weights)
  \Statex $y$ — an \(n \times 1\) vector (dependent variable)

\Ensure
  \Statex A set of non-decreasing functions \(\hat{f_1},...\hat{f_q}\) such that \( \displaystyle{[\hat{f_1},...\hat{f_q}] = \argmin_{f_1,...f_q]} \sum_{i=1}^{n} w_{i}(y_{i} - \sum_{h=1}^{q}f_h(x_{ih}))^2}  \)
  \Statex % blank line

  \State Set iteration index $m \leftarrow 1$ 
 \While{algorithm is not converged}
  \For{\(h \in 1,...,q\)} \Comment{See Algorithm 4, PAVA}
  \State Set \( \displaystyle{\hat{f_h} \leftarrow \argmin_{f_h} \sum_{i=1}^{n} w_{i}([y_{i} - \sum_{\substack{j=1 \\ j\neq h}}^{q}f_j(x_{ih})] - f_h(x_{ih}))^2}  \) holding all $f_j(), j \neq h$ fixed 
  \EndFor

  \State $m = m + 1$

 \EndWhile
\end{algorithmic}
\end{algorithm}




\begin{algorithm}[H]
\caption{Weighted Pool Adjacent Violators Algorithm}
\begin{algorithmic}[1]
\Require
  \Statex $x$ — an \(n \times 1\) vector (independent variable)
  \Statex $w$ — an \(n \times 1\) vector (observation weights)
  \Statex $y$ — an \(n \times 1\) vector (dependent variable)

\Ensure
  \Statex A non-decreasing function \( \displaystyle{\hat{f}(\cdot) = \argmin_{f} \sum_{i=1}^{n} w_{i}(y_{i} - f(x_{i}))^2}  \)
  \Statex % blank line
  
 \State Set iteration index $l \leftarrow 0$ 
 \State Set blocks $r \leftarrow 1, ..., B$ where at $ l = 0$, $B = n$ 
 \State Set $f^{(l=0)}(x_i) \leftarrow y_i$ 
 \State Set initial block membership $f^{(l=0)}(x_i) \in r_i$ 
 \While{any $f_{r}^{l}(x) \geq f_{r+1}^{l}(x)$}
  \If{$f_{r}^{l}(x) > f_{r+1}^{l}(x)$}
    \State Merge blocks $r$ and $r + 1$
    \EndIf
  \State Solve $f_{r}^{(l)}()$ for block $r$ as the weighted mean, i.e, \(\displaystyle{f_{r}() = \frac{1}{\sum_{i=1}^{n}w_i}\sum_{i=1}^{n} w_i(y_i)} \) for all $x \in r$

  \State $l = l + 1$

 \EndWhile
\end{algorithmic}
\end{algorithm}

%% Original algorithm without separation of component algorithms.
% We are given
% \begin{itemize}
%   \item[]	$x$ — an \(n \times p\) matrix (independent variables with no shape constraint)
%   \item[]	$z$ — an \(n \times q\) matrix (independent variables with monotone shape constraint)
%   \item[]	$y$ — an \(n \times 1\) matrix (dependent variable)
%   \item[]	$k$ — a positive integer representing the number of categories of latent variable L
%   \item[] \( \Lambda \) — an \(n \times k\) matrix representing the posterior probability of observation \(i = 1,...,n\) belonging to latent category \(j = 1,...,k\). Additionally, for all \(i = 1,...,n\) and \(j = 1,...,k\), \( \Lambda_{ij} \) is a real number in the range \([0,1]\), and \( \sum_{j=1}^{k} \Lambda_{ij} = 1 \)
% \end{itemize}
% 
% \begin{enumerate}
%   \item For each \( i \) = 1,...n, set the vector \( \Lambda_{i} \) = \( [\Lambda_{i1},...,\Lambda_{ik}] \) as an instance of a multinomial distribution with k=k and n=1. I.e., randomly assign one of the vector elements of \( [\Lambda_{i1}, ..., \Lambda_{ik}] \) to 1 and all other lik to 0, such that each \( \Lambda_{i} = [0,..., 1, ...,0] \) where 1 is at a random index.
%   \item In each iteration $(d)$ until convergence:
%     \begin{enumerate}
%       \item M-step
%         \begin{enumerate}
%           \item Calculate prior mixture proportions \([p_{1},...,p_{k}]^{(d)}\) such that \( \displaystyle{p_{k}^{(d)} = \frac{1}{n}\sum_{i=1}^{n}\Lambda_{ik}^{(d-1)}} \)
%           \item Calculate weighted partial linear model regression parameters; For each \( j = 1,..., k \), estimate \( \displaystyle{[\hat{\beta}_{j}, \hat{g}_{j}]^{(d)} = \argmin_{\beta, g} \sum_{i=1}^{n}\Lambda_{ij}^{(d-1)}(y - x\beta_{j} - g_{j}(z))^{2} } \) by iterating the following steps until convergence. For each iteration $b$:
%           \begin{enumerate}
%             \item Weighted PAVA/CPAV: If $z$ is univariate, i.e., an $n \times 1$ matrix, use PAVA to estimate \( \displaystyle{\hat{g}_{j}^{(b)} = \argmin_{g} \sum_{i=1}^{n}\Lambda_{ij}^{(d-1)}([y - x\beta_{j}^{(b-1)}] - g_{j}(z))^{2}} \) holding \( \beta_{j}^{(b-1)} \) fixed. If $z$ is multivariate, use CPAV algorithm*.
%             \item Weighted Linear Model: Estimate \( \displaystyle{\hat{\beta}_{j}^{(b)} = \argmin_{\beta} \sum_{i=1}^{n}\Lambda_{ij}^{(d-1)}([y - g_{j}^{(b)}(z)] - x\beta_{j})^{2}} \) holding \( g_{j}^{(b)} \) fixed.
%           \end{enumerate}
%         \end{enumerate}
%       \item E-step
%         \begin{enumerate}
%           \item Given $k$ regression functions \( [E(Y | X; Z) = X\beta_{k} + g_{k}(Z)]^{(d)} \), for \( i = 1,...,n \), calculate \( [\Lambda_{i1},...,\Lambda_{ik}]^{(d)} \) as \( [p_{1}^{(d)}P(y_{i} | x_{i}, \beta_{1}^{(d)}, g_{1}^{(d)}(z_{i})),...,p_{k}^{(d)}P(y_{i} | x_{i}, \beta_{k}^{(d)}, g_{k}^{(d)}(z_{i}))] \)
%           \item For each \( i = 1,...,n \), normalize the posterior probabilities \( [\Lambda_{i1},...,\Lambda_{ik}]^{(d)} \) such that \( \displaystyle{\sum_{j=1}^{k}\Lambda_{ij}^{(d)} = 1} \)
%         \end{enumerate}
%     \end{enumerate}
% \end{enumerate}

\subsection{Asymptotic Properties of Model and Estimator}

Under various sets of regularity assumptions, other authors have been able to derive analytically the asymptotic variance of estimators for mixtures of various types of non-parametric and semi-parametric regressions (see, for example, the publications of \cite{zhangetal}, \cite{huang}, \cite{zhangpan}). This can be helpful if the analytic variance allows for the construction of simple, parametrized confidence intervals for model coefficients, or if the analytic representation gives some insight into the asymptotic shape and spread of the variance. 

We know that the asymptotic variance of the model will be some function of the asymptotic variance of the model components. We therefore know that in this case, given the findings of \cite{guangcheng}, the asymptotic variance of the model will be functions of a tensor power of a sum over the expectations of the linear covariates given the non-linear covariates (see equations [\ref{asymg}] and [\ref{asymbeta}] for the complete variance as proven by Guang). As Guang points out, these expectations (i.e., $\sum_{h=1}^{p} E(X|Z_h$) can be estimated by a kernel method.

This means that calculating the analytic variance of the model would require the selection of a tuning parameter, whereas one of our model's primary strengths is its lack of tuning parameters. Moreover, the variance of both the linear coefficients and the nonparametric functions, as presented by Guang, are sufficiently complex as to offer little in the way of intuitive understanding. For these reasons, we opt to forego the analytic demonstration of the model variance and instead calculate the distributions of model estimates via the ordinary bootstrap.

% We presume that 
% [Discuss the asymptotic standard error estimates of parameters and functions.]
% [In prinicple, we expect the same behaviour but we cant figure it out]

\subsection{Confidence Intervals via Bootstrapping}

We opt to implement and demonstrate the proposed model with confidence intervals calculated via bootstrapping. Although the ordinary bootstrap applied to mixture models delivers all the typical advantages of bootstrapping for determining parameter confidence intervals (\cite{efron}), it runs into the well-known label-switching problem inherent in clustering algorithms. Specifically, when the mixture model is run multiple times, as with the bootstrap, the labeling of the resulting clusters is random and there is no guarantee, nor even a higher probability, that clusters with the same labels in two different models will represent the same underlying mixture component.

One partial solution to this problem, which we apply in the current paper, is to select the estimated parameters and non-parametric functions of the complete model as the starting values when estimating each of the bootstrapped models. This results in both a decreased computational burden in calculating the bootstrapped estimates, and the bypassing of the label-switching problem, since bootstrap-estimated components will likely share the same label as the component from which their starting values were drawn.

The disadvantage of this approach is that it ignores the possibility of bootstrap-estimated parameters reaching different likelihood maxima as a result of having had random as opposed to fixed starting values. For this reason, this approach underestimates the parameter variance. Moreover, if the first model which one fit returned estimates from a local likelihood maximum instead of a global maximum, subsequent bootstrap estimates may likewise be caught in the same local maximum. Parameter distributions from such a bootstrap might then be heavily biased.

One obvious approach to the problem of label-switching would be to run the ordinary bootstrap with random starting values and to treat the bootstrap-estimated parameters and non-parametric functions as ``observations''. We would then have a set of observations with latent class membership, and moreover, a known number of classes $k$, which is in fact precisely the problem structure which typically motivates a mixture model. Thus, we would run yet another mixture model on these parameters to determine a probabalistic interpretation of the distributions of parameters per component, as well as an estimate of the posterior probability of each parameter set belonging to each component. The implementation of such a hypothetical model may be difficult given that each observed parameter set includes both point-estimates and step functions, meaning that such a model would require the definition of a distance measure between non-parametric functions. We therefore leave this task for future research.

One tempting alternative to the ordinary bootstrap in the context of mixture models would be to take the posterior estimates for each observation from the initial model as resampling weights in the calculation of bootstrapped models. One might suspect that such a method would approximate the ordinary bootstrap with the significant advantages of only having to run the full model once and entirely avoiding the label-switching problem. Unfortunately, such an approach would severely underestimate the variance of the model, as it eschews the uncertainty contained in the initial posterior estimates. Moreover, the uncertainty of the initial posterior estimates cannot be approximated \emph{a priori}, since this is, in part, precisely what is determined by the ordinary bootstrap.

% The conditional bootstrap can thus be thought of as the ordinary bootstrap for the mixture components, conditioned on the weights being equal to the point-estimate posteriors from the initial model. There are two clear advantages to this approach: First, the label-switching problem is entirely avoided, as the component labels are \emph{a priori} known. Second, the bootstrap model refitting is extremely computationally efficient, as the original mixture model fitting process is run only once.
% 
% The disadvantage to this approach is, of course, that the uncertainty in the posterior estimates is entirely ignored in the construction of the final parameter distributions. Moreover, depending on the data, the posterior estimates may themselves be highly unstable. Thus, this approach underestimates the variance of the parameter estimates, and the degree to which it underestimates the variance is unknown. Moreover, if the initial model from which the posterior point-estimates are drawn terminated in a local maximum, the results of the conditional bootstrap may be highly biased and misleading.

% \subsection{Model Selection} TODO include a model selection section?



\subsection{Computational Complexity of Estimator} \label{seccomplex}

A common concern amongst users of this model will be the speed of the estimator, and by extension, the computational complexity of the estimator. It is not possible to specify exact $O(\cdot)$ notation given that the number of iterations within each optimization step of the algorithm is problem-dependent. However, given the generalized nature of the algorithm, we can compare its complexity empirically for different types and numbers of features within the sub-component regression models by running timed applications on pseudo-data. In the benchmarking table below, we compare the run-time of the estimation of 4 models -- with one monotone covariate and no linear effects; with two monotone covariates and no linear effects; with one monotone covariate and four linear effects; with two monotone covariates and three linear effects -- and all with the number of components, 4, known \emph{a priori}.

% \begin{equation} \label{modstruc}
%   Y = 
%   \begin{dcases}
%     3 + X_1 + N(0,3) \text{  with probability $0.25$; } \\
%     X_1^{3} + N(0,4) \text{  with probability $0.25$; }\\
%     \sum_{h=1}^{p} g_{hk} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{jk} X_{j} \ +\ \epsilon_k \text{  with probability $\pi_k$; }\\
%     \sum_{h=1}^{p} g_{hk} (Z_{h}) \ +\  \sum_{j=1}^{q} \beta_{jk} X_{j} \ +\ \epsilon_k \text{  with probability $\pi_k$; }\\
%   \end{dcases}
% \end{equation}

% TIMED COMPLEXITY: here.

\medskip

% \begin{minipage}{0.8\textwidth}
<<complexity_benchmarking, eval=TRUE, echo=FALSE, results='asis', fig.align='left'>>=

# data with 4 latent categories

################
X <- cbind(
  runif(1000, -5, 5),
  runif(1000, -10, 10),
  runif(1000, -100, 100),
  runif(1000, -100, 100),
  runif(1000, -100, 100)
)
################

# print benchmarks (all excluding intercept)
# testtable <- benchmark(
# "Univariate monotone without linear effects" = {
#   Y1 <- (X[1:250,1])+3 + rnorm(250, 0, 3) # component 1
#   Y2 <- (X[251:500,1])^3 + rnorm(250, 0, 4) # component 2
#   Y3 <- 2*((X[501:750,1])+5) + rnorm(250, 0, 3) # component 3
#   Y4 <- 2*((X[751:1000,1])-5) + rnorm(250, 0, 4) # component 4
#   df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
#   names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
#   ###
#   m1 <- flexmix(Y ~ X1 -1, data = df_3, k = 4, 
#                 model = mono_reg(mon_inc_names = "X1"))
# },
# "Bivariate monotone without linear effects" = {
#   Y1 <- (X[1:250,1])+3 + 1.5*X[1:250,2] + 
#     rnorm(250, 0, 3) # component 1
#   Y2 <- (X[251:500,1])^3 + 3*X[251:500,2] + 
#     rnorm(250, 0, 4) # component 2
#   Y3 <- 2*((X[501:750,1])+5) + 5*X[501:750,2] + 
#     rnorm(250, 0, 3) # component 3
#   Y4 <- 2*((X[751:1000,1])-5) + 10*X[751:1000,2] + 
#     rnorm(250, 0, 4) # component 4
#   df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
#   names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
#   ###
#   m2 <- flexmix(Y ~ X1 + X2 -1, data = df_3, k = 4, 
#                 model = mono_reg(mon_inc_names = c("X1", "X2"))) # c("X1", "X2")
# },
# "Univariate monotone with linear effects" = {
#   Y1 <- (X[1:250,1])+3 + 1.5*X[1:250,2] - 1.5*X[1:250,3] -
#     1*X[1:250,4] + X[1:250,5] + rnorm(250, 0, 3) # component 1
#   Y2 <- (X[251:500,1])^3 + 3*X[251:500,2] + 2*X[251:500,3] -
#     2*X[251:500,4] + 2*X[251:500,5] + 
#     rnorm(250, 0, 4) # component 2
#   Y3 <- 2*((X[501:750,1])+5) + 5*X[501:750,2] - 1*X[501:750,3] +
#     2*X[501:750,4] + 4*X[501:750,5] + 
#     rnorm(250, 0, 3) # component 3
#   Y4 <- 2*((X[751:1000,1])-5) + 10*X[751:1000,2] -
#     3*X[751:1000,3] - 3*X[751:1000,4] + 3*X[751:1000,5] + 
#     rnorm(250, 0, 4) # component 4
#   df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
#   names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
#   ###
#   m3 <- flexmix(Y ~ . -1, data = df_3, k = 4, 
#                 model = mono_reg(mon_inc_names = "X1"))
# },
# "Bivariate monotone with linear effects" = {
#   Y1 <- (X[1:250,1])+3 + 1.5*X[1:250,2] - 1.5*X[1:250,3] -
#     1*X[1:250,4] + X[1:250,5] + rnorm(250, 0, 3) # component 1
#   Y2 <- (X[251:500,1])^3 + 3*X[251:500,2] + 2*X[251:500,3] -
#     2*X[251:500,4] + 2*X[251:500,5] + 
#     rnorm(250, 0, 4) # component 2
#   Y3 <- 2*((X[501:750,1])+5) + 5*X[501:750,2] - 1*X[501:750,3] +
#     2*X[501:750,4] + 4*X[501:750,5] + 
#     rnorm(250, 0, 3) # component 3
#   Y4 <- 2*((X[751:1000,1])-5) + 10*X[751:1000,2] - 
#     3*X[751:1000,3] - 3*X[751:1000,4] + 3*X[751:1000,5] + 
#     rnorm(250, 0, 4) # component 4
#   df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
#   names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
#   ###
#   m4 <- flexmix(Y ~ . -1, data = df_3, k = 4, 
#                 model = mono_reg(mon_inc_names = c("X1", "X2"))) # c("X1", "X2")
# },
# replications = 1,
# columns = c("Test", "Replications", "Elapsed", "Relative")
# )

testtable <- data.frame(Test = c("Bivariate monotone with linear effects", "Bivariate monotone without linear effects", "Univariate monotone with linear effects", "Univariate monotone without linear effects"), replications=c(1,1,1,1), elapsed = c(413.444, 656.222, 18.384, 44.245), relative = c(21.475, 34.523, 1.000, 2.320))

t6 <- kable(testtable, format = "latex", booktabs = TRUE, digits = 3)

cat(c("\\begin{table}[!htb]
    
    \\begin{minipage}{1\\linewidth}",
        t6,
    "\\caption{Elapsed time for four different types of model constructions -- with and without linear effects, and with and without multiple monotone terms.} \\label{comptable}
    \\end{minipage}%
\\end{table}"
))  

@
% \captionof{figure}[Abbreviated Caption]{In the table above, we see the average elapsed time for four different types of model constructions -- with and without linear effects, and with and without multiple monotone terms.}
% \end{minipage}

\medskip

One can see that -- for a model with 4 latent components -- adding a second monotone nonparametric effect within each component multiplies the computation time by approximately 50. This is an indication of the heavy cost of increasing even slightly the dimensionality of the non-parametric estimation within each component model.

By comparison, adding linear effects within the component models comes essentially for free. In fact, the estimation of models with univariate monotone effects and 4 linear effects is \emph{faster} than the complementary model without linear effects. 

It should be noted that the difference in speed between fitting models with monovariate and multivariate monotone terms is a function of the maximum number of iterations and the tolerance threshold set on both the backfitting sequence in the larger partial linear model \emph{and} the backfitting sequence in the cyclic PAVA (or CPAV) sub-model (see algorithms [\ref{wplralgo}] and [\ref{cpavalgo}] for reference). All of these parameters are set by the user, and there is as of yet no automated way of selecting parameters for the optimal convergence rate of the algorithm. We leave this problem to future research.

\section{Model Applications}
\subsection{Simulated Data}

In this section, we demonstrate the application of the proposed model by fitting it to randomly generated pseudo-data. We begin by modeling 1000 observations generated from 2 latent categories with the following underlying structure:

\begin{equation}
\begin{aligned}
  Y_{1} &= 50 + X^3 + \epsilon_1 \\
  Y_{2} &= -50 + 0.04 \cdot X^5 + 30 \cdot X + \epsilon_2 \\
\end{aligned}
\end{equation}
where 
\begin{equation}
\begin{aligned}
  \epsilon_1 &\sim N(0,200) \\
  \epsilon_2 &\sim N(0,300) \\
\end{aligned}
\end{equation}
and
\begin{equation}
\begin{aligned}
  \pi_1 &= 0.65 \\
  \pi_2 &= 0.35 \\
\end{aligned}
\end{equation}
and
\begin{equation}
\begin{aligned}
  X &\sim Uniform(-10,10) \\
\end{aligned}
\end{equation}


We proceed to estimate a mixture of univariate regressions (model [\ref{m2}]), with the number of components known \emph{a priori} as 2. The fitted model includes only monotone non-decreasing function of covariate $X$, and no intercept. The regression models are identical for each of the 2 components.

\begin{equation} \label{m2}
  Y = \sum_{k=1}^{2}\pi_k (g_{k} (X) \ +\  \epsilon_k)
\end{equation}

% TODO complete boostrap implementation. Have flexmix bootstrap return model with monotone fit(s) as well as posteriors for a bootstrapped rootogram
\begin{figure}[H]
<<pseudo_mixture_monovar, fig.keep='high', warning=F, echo=FALSE, results='hide', fig.align='center', eval=FALSE, fig.height=3>>=

# data with 2 latent categories
################
pi1 <- 0.65
pi2 <- 0.35
n <- 1000

Xa <- runif(n*pi1,-10,10) #seq(-10,10, length.out=1001)
Xb <- runif(n*pi2,-10,10) #seq(-10,10, length.out=1001)


Y1t <- 50 + Xa^3 
Y1 <- Y1t + rnorm(n*pi1, 0, 200) # component 1
Y2t <- -50 + 0.04*(Xb)^5 + 30*Xb
Y2 <- Y2t + rnorm(n*pi2, 0, 300) # component 2
cat <- c(rep.int(1, n*pi1), rep.int(2,n*pi2))
# plot(X,Y1, type="p", cex=0.05)
# lines(X,Y2,type="p", cex=0.05)

df_2 <- data.frame(c(Y1, Y2), c(Xa,Xb), c(Y1t, Y2t), cat)
names(df_2) <- c("Y", "X", "Yt", "cat")
################


# build model
m2 <- flexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X"))

# m2step <- stepFlexmix(Y ~ X-1, data = df_2, k = 1:5, model = mono_reg(mon_inc_names = "X"))

# set bootstrap reps, and hold variable for all other models
boots <- 10

# build bootstrapped model
m2boot <- boot(initFlexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X")), R=boots, verbose=0, model=TRUE, initialize_solution=TRUE)


# plot fitted model
plot(m2, ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=1)
@
% TODO describe the confidence intervals of the bootstrapped rootogram
\captionof{figure}[Abbrviated Caption]{The rootogram of the two-component mixture model shows the distribution of posterior probabilities with reference to the binary latent variable, for all observations used to fit the model. The model indicates higher confidence in the identification of clusters and the classification of individual observations when the observations accumulate near the limits of the rootogram, at 0 and 1. Conversely, greater mass at the center of the rootogram represents observations that are less confidently classified.}
\end{figure}

\begin{figure}[H]
<<pseudo, fig.keep='high', warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=
plot(m2boot@object, boot_CI=build_CI_trad(m2boot), ylim=c(-1300,1300), palet="Dark2", root_scale="sqrt", subplot=2) + 
  geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
  geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")

# plot(m2boot@object, boot_CI=build_CI_trad(m2boot),  palet="Dark2", root_scale="sqrt", subplot=2) + 
#   geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
#   geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")

@
\captionof{figure}[Abbrviated Caption]{The estimated monotone functions of the two-component mixture model, with overlaid, dotted black lines representing the true functions. The confidence intervals are generated by 1000 iterations of an ordinary (non-parametric) bootstrap. \label{fig:pseudo}}
\end{figure}

% \begin{figure} 
% <pseudo_mixture_monovar_c, fig.keep='high', warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>
% plot(m2, boot_CI=build_CI_cond(m2, R = boots), ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=2) + 
%   geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
%   geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")
% 
% # plot(m2boot@object, boot_CI=build_CI_trad(m2boot),  palet="Dark2", root_scale="sqrt", subplot=2) + 
% #   geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
% #   geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")
% 
% @
% TODO add description/explanation of how the model has difficulty estimating the functions at/after the points where the functions cross one another.
% \captionof{figure}[Abbrviated Caption]{The estimated monotone functions of the two-component mixture model, now with confidence intervals generated via 1000 iterations of the conditional bootstrap.}
% \end{figure}

As can be seen from figure [\ref{fig:pseudo}], the algorithm is more uncertain of the monotone regression shapes where the true data generating functions overlap. 

% Alternately, we can fit mixtures with alternate specifications for each component. In the next demonstration, we model data generated from 2 latent categories with contrary monotonic effects, i.e., one with a monotone non-increasing true function and one with a monotone non-decreasing true function. The data has the following underlying structure:
% 
% \begin{align*}
%   Y_{1} &= X^3 + \epsilon_1 \\
%   Y_{2} &= 100 + 0.02 \cdot X^5 + \epsilon_2 \\
% \end{align*}
% 
% where 
% 
% \begin{align*}
%   \epsilon_1 &\sim N(0,30) \\
%   \epsilon_2 &\sim N(0,20) \\
% \end{align*}
% 
% and
% 
% \begin{align*}
%   \pi_1 &= 0.7 \\
%   \pi_2 &= 0.3 \\
% \end{align*}
% 
% and
% 
% \begin{align*}
%   X &\sim Uniform(-10,10) \\
% \end{align*}
% 
% 
% We proceed to estimate a mixture of univariate regressions (\ref{m2a}), with the number of components known \emph{a priori} as 2. The fitted model includes only monotone non-decreasing function of covariate $X$, and no intercept. The regression models are as follows for each of the 2 components:
% 
% \begin{equation} \label{m2a}
%   Y = 
%   \begin{dcases}
%     \pi_1 (g_{1} (X) \ +\  \epsilon_1) \\
%     \pi_2 (g_{2} (X) \ +\  \epsilon_2)
%   \end{dcases}
% \end{equation}
% 
% where $g_{1}$ is non-decreasing, and $g_{2}$ is non-increasing.
% 
% \begin{minipage}{0.8\textwidth}
% pseudo_mixture_monovar_contrary, fig.keep='high', warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=
% 
% # TODO currently, you cannot specify different models for different components. the larger monoreg mixture object has inc/dec attributes, which it shouldn't -- these should belong to the components.
% 
% 
% # data with 2 latent categories
% ################
% pi1 <- 0.7
% pi2 <- 0.3
% n <- 1000
% 
% Xa <- runif(n*pi1,-10,10) #seq(-10,10, length.out=1001)
% Xb <- runif(n*pi2,-10,10) #seq(-10,10, length.out=1001)
% 
% Y1t <- 50 + -Xa^3 
% Y1 <- Y1t + rnorm(n*pi1, 0, 30) # component 1
% Y2t <- -50 + 0.04*(Xb)^5 + 30*Xb
% Y2 <- Y2t + rnorm(n*pi2, 0, 20) # component 2
% cat <- c(rep.int(1, n*pi1), rep.int(2,n*pi2))
% # plot(X,Y1, type="p", cex=0.05)
% # lines(X,Y2,type="p", cex=0.05)
% 
% df_2 <- data.frame(c(Y1, Y2), c(X,X), c(Y1t, Y2t), cat)
% names(df_2) <- c("Y", "X", "Yt", "cat")
% ################
% 
% 
% # build model
% # TODO mono_reg model is FORCING a linear component here. It should fit a purely nonparametric model when it sees that the monotone indices are all variables minus the intercept.
% m2a <- flexmix( ~ X-1, data = df_2, k = 2, model = list(mono_reg(Y~., mon_inc_names = "X"), mono_reg(Y~., mon_dec_names = "X")) )
% 
% # m2boot <- boot(initFlexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X")), R=10, verbose=1, model=TRUE, initialize_solution=TRUE)
% 
% # plot fitted model
% plot(m2a, ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=1)
% @
% % TODO describe the confidence intervals of the bootstrapped rootogram
% \captionof{figure}[Abbrviated Caption]{The rootogram of the two-component mixture model shows the distribution of posterior probabilities with reference to the binary latent variable, for all observations used to fit the model. The model indicates higher confidence in the identification of clusters and the classification of individual observations when the observations accumulate near the limits of the rootogram, at 0 and 1. Conversely, greater mass at the center of the rootogram represents observations that are less confidently classified.}
% \end{minipage}
% 
% \begin{minipage}{0.8\textwidth}
% pseudo_mixture_monovar_contraryb, fig.keep='high', warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=
% plot(m2a, ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=2) + 
%   geom_line(data = df_2[df_2$cat==1,], aes(X, Yt), linetype="dotted") +
%   geom_line(data = df_2[df_2$cat==2,], aes(X, Yt), linetype="dotted")
% 
% @
% 
% \captionof{figure}[Abbrviated Caption]{The estimated monotone functions of the two-component mixture model, with overlaid, dotted black lines representing the true functions. The confidence intervals are generated by an ordinary (non-parametric) bootstrap.}
% \end{minipage}





We continue the demonstration with the inclusion of linear effects. For the next model, we generate pseudo-data from 4 latent categories with the following underlying structure:

\begin{equation}
\begin{aligned}
  Y_{1} &= 10 + X_1^3 + 1.5\cdot X_2 - 1.5\cdot X_3 - X_4 + X_5 + \epsilon_1 \\
  Y_{2} &= -10 + 40 \cdot X_1 + 3\cdot X_2 + 2\cdot X_3 - 2\cdot X_4 + 2\cdot X_5 + \epsilon_2 \\
  Y_{3} &= -4 + 2 \cdot (X_1^3) - 2\cdot X_2 - X_3 + 2\cdot X_4 + 4\cdot X_5 + \epsilon_3 \\
  Y_{4} &= 4 + 0.1 \cdot (X_1^5) - 3\cdot X_2 - 3\cdot X_3 - 3\cdot X_4 + 3\cdot X_5 + \epsilon_4 \\
\end{aligned}
\end{equation}
where 
\begin{equation}
\begin{aligned}
  \epsilon_1 &\sim N(0,150) \\
  \epsilon_2 &\sim N(0,100) \\
  \epsilon_3 &\sim N(0,70) \\
  \epsilon_4 &\sim N(0,80) \\
\end{aligned}
\qquad
\qquad
\begin{aligned}
  X_1 &\sim Uniform(-5,5) \\
  X_2 &\sim Uniform(-10,10) \\
  X_3 &\sim Uniform(-100,100) \\
  X_4 &\sim Uniform(-100,100) \\
  X_5 &\sim Uniform(-100,100) \\
\end{aligned}
\end{equation}
and
\begin{equation}
\pi_1 = \pi_2 = \pi_3 = \pi_4
\end{equation}


We proceed to estimate a mixture of partial linear regressions (model [\ref{m3}]), with the number of components known \emph{a priori} as 4. The fitted model includes one monotone non-decreasing function of covariate $X_1$, an intercept, and a linear effect for each of $X_2,...,X_5$. The regression models are identical for each of the 4 components.

\begin{equation} \label{m3}
  Y = \sum_{k=1}^{4}\pi_k (g_{k} (X_1) \ +\  \beta_{0,k} \ +\ \beta_{1,k}\cdot X_2 \ +\ \beta_{2,k}\cdot X_3 \ +\ \beta_{3,k}\cdot X_4 \ +\ \beta_{4,k}\cdot X_5 \ +\ \epsilon_k)
\end{equation}

Tables [\ref{t1}] through [\ref{t5}] show 89 \% confidence intervals for the estimates of the linear effects and priors of model [\ref{m3}]. It should be noted that, since these estimates are derived from the nonparametric bootstrap, there are no constraints on the shape of the estimate distributions, meaning they may not be either symmetric or unimodal, and information is therefore lost by summarizing their complete distributions by means and confidence intervals. Moreover, since each bootstrap repetition returns an associated set of estimates, there is known covariance between the estimates. One may represent both full distributions and covariance of up to three linear effects at a time with three-dimensional plots of the covariate space, though with more than three linear effects the interpretation of such plots becomes more difficult. We choose no to report the covariance structure here, though users may find it useful in other applications.

% TODO include chart comparing distributions of parameters and g() for increasing n, i.e., n = 400, 800, 2000,

\begin{figure}[H]
<<pseudo_mixture, warning=F, echo=FALSE, results='hide', fig.align='center', eval=FALSE, fig.height=3>>=

# data with 4 latent categories
################
X <- cbind(
  runif(4000, -5, 5),
  runif(4000, -10, 10),
  runif(4000, -100, 100),
  runif(4000, -100, 100),
  runif(4000, -100, 100)
)


Y1 <- 10 + (X[1:1000,1])^3 + 1.5*X[1:1000,2] - 1.5*X[1:1000,3] - 1*X[1:1000,4] + X[1:1000,5] + rnorm(1000, 0, 150) # component 1
Y2 <- -10 + 40*(X[1001:2000,1]) + 3*X[1001:2000,2] + 2*X[1001:2000,3] - 2*X[1001:2000,4] + 2*X[1001:2000,5] + rnorm(1000, 0, 100) # component 2
Y3 <- -4 + 2*((X[2001:3000,1])^3) - 2*X[2001:3000,2] - 1*X[2001:3000,3] + 2*X[2001:3000,4] + 4*X[2001:3000,5] + rnorm(1000, 0, 70) # component 3
Y4 <- 4 + 0.1*((X[3001:4000,1])^5) - 3*X[3001:4000,2] - 3*X[3001:4000,3] - 3*X[3001:4000,4] + 3*X[3001:4000,5] + rnorm(1000, 0, 80) # component 4

df_3 <- data.frame(c(Y1, Y2, Y3, Y4), X)
names(df_3) <- c("Y", "X1", "X2", "X3", "X4", "X5")
################

X <- seq(-5, 5, length.out=4000)



Y1 <- (X)^3  # component 1
Y2 <- 40*(X)  # component 2
Y3 <- 2*((X)^3)  # component 3
Y4 <- 0.1*((X)^5)  # component 4

df_t <- data.frame(Y1, Y2, Y3, Y4, X)
names(df_t) <- c("Y1","Y2","Y3","Y4", "X1")

###############


# build model
m3 <- flexmix(Y ~ ., data = df_3, k = 4, model = mono_reg(mon_inc_names = "X1"), control = list(minprior = 0.1))

# m3step <- stepFlexmix(Y ~ ., data = df_3, k = 4:6, model = mono_reg(mon_inc_names = "X1"), nrep = 5)

# bootstrap model
m3boot <- boot(initFlexmix(Y ~ ., data = df_3, k = 4, model = mono_reg(mon_inc_names = "X1")), R=boots, initialize_solution=TRUE, verbose=0, model=TRUE)

# plot fitted model
plot(m3, ylim=c(-100,100), palet="Dark2", root_scale="sqrt", subplot=1) 

@
\captionof{figure}[Abbrviated Caption]{The rootogram of the four-component mixture model represented by model [\ref{m3}].}
\end{figure}

\begin{figure}[H]
<<pseudo_mixture_b, warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>>=

plot(m3boot@object, boot_CI=build_CI_trad(m3boot), ylim=c(-300,300), palet="Dark2", root_scale="sqrt", subplot=2) + 
  geom_line(data = df_t, aes(X1, Y1), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y2), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y3), linetype="dotted") +
  geom_line(data = df_t, aes(X1, Y4), linetype="dotted") 

# plot(m2, boot_CI=build_CI_cond(m2, R = 1000), ylim=c(-1000,1000), palet="Dark2", root_scale="sqrt", subplot=2) + 

@
\captionof{figure}[Abbreviated Caption]{The estimated monotone functions of the four-component mixture model, with overlaid, dotted black lines representing the true functions. The confidence intervals are generated by 1000 iterations of an ordinary (non-parametric) bootstrap.}
\end{figure}


% \begin{figure}
% <pseudo_mixture_c, warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3>
% 
% plot(m3, boot_CI=build_CI_cond(m3, R=30, CI = 0.95), ylim=c(-250,250), palet="Dark2", root_scale="sqrt", subplot=2) + 
%   geom_line(data = df_t, aes(X1, Y1), linetype="dotted") +
%   geom_line(data = df_t, aes(X1, Y2), linetype="dotted") +
%   geom_line(data = df_t, aes(X1, Y3), linetype="dotted") +
%   geom_line(data = df_t, aes(X1, Y4), linetype="dotted") 
% 
% @
% \captionof{figure}[Abbrviated Caption]{The estimated monotone functions of the two-component mixture model, now with confidence intervals generated via 1000 iterations of the conditional bootstrap.}
% \end{figure}


<<pseudo_mixture_d, warning=F, echo=FALSE, fig.align='center', eval=FALSE, fig.height=3, results='asis'>>=

# TODO this is terrible and hacky layout. Figure out a better layout of coefficient confidence intervals

holder <- build_CI_trad(m3boot)


t1 <- kable(CI_table(holder)[[1]], format = "latex", booktabs = TRUE, digits = 2)
t2 <- kable(CI_table(holder)[[2]], format = "latex", booktabs = TRUE, digits = 2)

t3 <- kable(CI_table(holder)[[3]], format = "latex", booktabs = TRUE, digits = 2)
t4 <- kable(CI_table(holder)[[4]], format = "latex", booktabs = TRUE, digits = 2)

t5 <- kable(CI_table(holder)[[5]], format = "latex", booktabs = TRUE, digits = 2)

cat(c("\\begin{table}[!htb]
    \\begin{minipage}{.5\\linewidth}
      \\caption{Component 1} \\label{t1}
      \\centering",
        t1,
    "\\end{minipage}%
    \\begin{minipage}{0.7\\linewidth}
      \\centering
        \\caption{Component 2} \\label{t2}",
        t2,
    "\\end{minipage} 
\\end{table}"
))  

cat(c("\\begin{table}[!htb]
    \\begin{minipage}{.5\\linewidth}
      \\caption{Component 3} \\label{t3}
      \\centering",
        t3,
    "\\end{minipage}%
    \\begin{minipage}{0.7\\linewidth}
      \\centering
        \\caption{Component 4} \\label{t4}",
        t4,
    "\\end{minipage} 
\\end{table}"
))  

cat(c("\\begin{table}[!htb]
    \\centering
    \\begin{minipage}{.5\\linewidth}
      \\caption{Priors} \\label{t5}
      \\centering",
        t5,
    "\\end{minipage}%
\\end{table}"
))  

@




% TODO plot the forest-plot of linear-effect estimates and confidence intervals. How do we plot them while accounting for label switching?

\subsection{Algorithm Comparisons with Simulated Data}

Here, we empirically compare the behaviour of our proposed model with that of the Zhang et al. model. We fit both models on four separate datasets, each with two components. The first dataset is generated from a model where the true priors of the mixture components $\pi_k$, as well as the true variance of those components $\sigma_k$, are functions of observed covariates, i.e., $\pi_k(X)$ and $\sigma_k(X)$. The second dataset is generated from a model where the true priors and true variances, $\pi_k$ and $\sigma_k$, are constant. The third dataset is generated from a model with constant true priors and true variances, as well as much ``jagged'' true underlying functions. The fourth dataset, again, is generated from a model with constant true priors and true variances, as well as true underlying functions that twice intersect. Because of the label-switching problem, we choose to plot and compare the resulting fitted models without reference to the component labels. Instead, we overlay component mean estimates sequentially in order to try to compare the general bias and variance behaviours of the two respective model classes. In each pair of comparison plots, the true underlying functions are represented by dotted black lines.

The models resulting from the first dataset, seen in figure [\ref{fig:comparison}], show that the Zhang et al. model produces tight fits around the true functions, with very low variance. Our proposed model also fits tightly around the true functions when its component estimates are well-separated, but it frequently encounters a component-switching problem where the function estimate is in fact a hybrid of the underlying true functions. When the underlying model has constant true priors and variances, as in figure [\ref{fig:comparison_2}], our model is slightly better behaved, although the component-switching problem remains.

In the third dataset, shown in figure [\ref{fig:comparison_3}], our model follows the jagged breaks and changes in the true underlying functions with no visible increase in variance. The Zhang et al. model, on the other hand, become severely biased and confidently estimates an incorrect pair of functions. When the Zhang et al. model does estimate the correct function neighbourhood, it smooths over the sharp edges of the underlying function. In the fourth dataset, shown in figure [\ref{fig:comparison_4}], where the true functions intersect, the Zhang et al. model confidently estimates smooth, non-intersecting functions. Our model, on the other hand, correctly identifies the function intersections.



\pagebreak


%(INCLUDE COMPARISON PLOTS HERE).
\begin{figure}[H]
\centering
    \textbf{Algorithm Comparison for Conditional True Priors and Variances}\par\medskip
<<comparison, echo=FALSE, eval=FALSE, message=FALSE, results='hide', warning=FALSE, fig.align='center', fig.show='hold', out.width='49%', fig.height=3.5, fig.width=3.5>>=

# the R-matlab connection is very finnicky and machine-specific. If you are running this code chunk on a different machine or after a long time, make sure to change your path specification. If the code doesn't run, remove the chunk and move on with our life.

# send r-data to csv
# generate x and y data for mixture of nonparametric regressions:
n <- 1000
boot_rep <- 8
numc <- 2

# x <- runif(n)
# cum_p <- cbind(exp(0.5*x)/(1+exp(0.5*x)), rep(1,n))
# gen_p = rand(n,1)

write.csv(c(n, numc, boot_rep), "./matlabCodes_improved/getme.csv") # set params for matlab script

# run the matlab script as is
run_matlab_script(fname = "./matlabCodes_improved/test.m")

# retrieve the results of the matlab script
comefindme <- read.csv("./matlabCodes_improved/dat.csv") # get the matlab-generated data

bootest <- read.csv("./matlabCodes_improved/bootest.csv") # get the matlab generated model means

# set par
par(mar = c(4, 4, .1, .1))

# build x for plotting true functions:
ex <- seq(0,1, len=1000)

# plot 
mon <- monreg(x = seq(0,1, len=101), y = bootest[boot_rep+1,], hd = 0.08, hr = 0.08)
plot(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02),
     ylim=c(-2.5, 5), ylab="", xlab="Zhang et al. Model")
for(i in (boot_rep+2):(boot_rep*3)){
  mon <- monreg(x = seq(0,1, len=101), y = bootest[i,], hd = 0.08, hr = 0.08)
  if(i > 2*boot_rep){
  lines(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02))
  }
    lines(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02))
}
lines(x = ex, y = 5*(ex^2)-2, lty=3)
lines(x = ex, y = ((ex+0.5)^3)+1, lty=3)


# Fit and plot our version


# build model
ovboot <- boot(initFlexmix(dat2 ~ .-1, data = comefindme, k = 2, model = mono_reg(mon_inc_names = "dat1")), R=boot_rep, initialize_solution=FALSE, verbose=1, model=TRUE)

plot(x = ovboot@parameters[[1]][[1]][[1]][[1]]$mon_obj[,1]$x,
     y = ovboot@parameters[[1]][[1]][[1]][[1]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02),
     ylim=c(-2.5, 5), ylab="", xlab="Authors' Model")
for(i in 2:length(ovboot@parameters)){
  lines(x = ovboot@parameters[[i]][[1]][[1]][[1]]$mon_obj[,1]$x,
     y = ovboot@parameters[[i]][[1]][[1]][[1]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02))
  lines(x = ovboot@parameters[[i]][[1]][[1]][[2]]$mon_obj[,1]$x,
     y = ovboot@parameters[[i]][[1]][[1]][[2]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02))
}
lines(x = ex, y = 5*(ex^2)-2, lty=3)
lines(x = ex, y = ((ex+0.5)^3)+1, lty=3)


#plot(ovboot@object, boot_CI=build_CI_trad(ovboot), ylim=c(-3,5), palet="Dark2", root_scale="sqrt", subplot=2)

# x = seq(0,1, len=101)
# 
# plot(x, y=5*(x^2)-2, type="l", ylim=c(-2,5))
# lines(x, y=(x+0.5)^3+1, type = "l")

# reset par
par(mar = c(5.1, 4.1, 4.1, 2.1))


@
\captionof{figure}[Abbrviated Caption]{The Zhang et al. model and the authors' model, fit to data generated from an underlying model with conditional true priors and variances.} \label{fig:comparison}
\end{figure}



\begin{figure}[H]
\centering
    \textbf{Algorithm Comparison for Separated True Functions}\par\medskip
<<comparison_2, echo=FALSE, eval=FALSE, message=FALSE, results='hide', warning=FALSE, fig.align='center', fig.show='hold', out.width='49%', fig.height=3.5, fig.width=3.5>>=

# data with 2 latent categories, generated with constant prior and constant sigma
################
pi1 <- 0.65
pi2 <- 0.35
n <- 1000

Xa <- sort(runif(n*pi1,0,1)) #seq(-10,10, length.out=1001)
Xb <- sort(runif(n*pi2,0,1)) #seq(-10,10, length.out=1001)


Y1t <- 7*(Xa-0.5)^3 + 0.1
Y1 <- Y1t + rnorm(n*pi1, 0, 0.3) # component 1
Y2t <- 1/(1+1000000^(-Xb+0.5)) + 0.6
Y2 <- Y2t + rnorm(n*pi2, 0, 0.2) # component 2
cat <- c(rep.int(1, n*pi1), rep.int(2,n*pi2))
# plot(X,Y1, type="p", cex=0.05)
# lines(X,Y2,type="p", cex=0.05)

df_2 <- data.frame(c(Y1, Y2), c(Xa,Xb), c(Y1t, Y2t), cat)
names(df_2) <- c("Y", "X", "Yt", "cat")
################
# plot(Xa, Y1t, type="l", lty=3, ylim=c(-1, 3))
# lines(Xb, Y2t, lty=3)

# pass data to matlab:
write.csv(df_2, "./matlabCodes_improved/rdat.csv") # set params for matlab script

# run matlab script:
run_matlab_script(fname = "./matlabCodes_improved/mod2.m")

# retrieve the results of the matlab script
bootest <- read.csv("./matlabCodes_improved/bootest.csv") # get the matlab generated model means

# set par
par(mar = c(4, 4, .1, .1))

# plot bootest
mon <- monreg(x = seq(0,1, len=101), y = bootest[boot_rep+1,], hd = 0.08, hr = 0.08)
plot(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02),
     ylim=c(-1, 3), ylab="", xlab="Zhang et al. Model")
for(i in (boot_rep+2):(boot_rep*3)){
  mon <- monreg(x = seq(0,1, len=101), y = bootest[i,], hd = 0.08, hr = 0.08)
  if(i > 2*boot_rep){
  lines(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02))
  }
    lines(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02))
}
lines(Xa, Y1t, type="l", lty=3)
lines(Xb, Y2t, lty=3)

# build bootstrapped model
m2boot <- boot(initFlexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X")), R=boot_rep, verbose=1, model=TRUE, initialize_solution=FALSE)


plot(x = m2boot@parameters[[1]][[1]][[1]][[1]]$mon_obj[,1]$x,
     y = m2boot@parameters[[1]][[1]][[1]][[1]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02), 
     ylim=c(-1, 3), ylab="", xlab="Authors' Model")
for(i in 2:length(m2boot@parameters)){
  lines(x = m2boot@parameters[[i]][[1]][[1]][[1]]$mon_obj[,1]$x,
     y = m2boot@parameters[[i]][[1]][[1]][[1]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02))
  lines(x = m2boot@parameters[[i]][[1]][[1]][[2]]$mon_obj[,1]$x,
     y = m2boot@parameters[[i]][[1]][[1]][[2]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02))
}
lines(Xa, Y1t, type="l", lty=3)
lines(Xb, Y2t, lty=3)


# reset par
par(mar = c(5.1, 4.1, 4.1, 2.1))

@
\captionof{figure}[Abbrviated Caption]{The Zhang et al. model and the authors' model, fit to data generated from an underlying model with constant true priors and variances.} \label{fig:comparison_2}
\end{figure}

\pagebreak

\begin{figure}[H]
\centering
    \textbf{Algorithm Comparison for Jagged True Functions}\par\medskip
<<comparison_3, echo=FALSE, eval=FALSE, message=FALSE, results='hide', warning=FALSE, fig.align='center', fig.show='hold', out.width='49%', fig.height=3.5, fig.width=3.5>>=

# data with 2 latent categories, generated with constant prior and constant sigma
################
pi1 <- 0.6
pi2 <- 0.4
n <- 1000

Xa <- sort(runif(n*pi1,0,1)) #seq(-10,10, length.out=1001)
Xb <- sort(runif(n*pi2,0,1)) #seq(-10,10, length.out=1001)


Y1t <- ifelse(Xa < 0.6, 7*(Xa-0.3)^3 - 0.3, log(5*Xa)-0.4 )
Y1 <- Y1t + rnorm(n*pi1, 0, 0.3) # component 1
Y2t <- ifelse(Xb < 0.4, 1/(1+1000000^(-Xb+0.5)) + 0.3, ifelse(Xb < 0.45, 10*Xb-3, 0.5*Xb+1.5) )
Y2 <- Y2t + rnorm(n*pi2, 0, 0.2) # component 2
cat <- c(rep.int(1, n*pi1), rep.int(2,n*pi2))
# plot(X,Y1, type="p", cex=0.05)
# lines(X,Y2,type="p", cex=0.05)

df_2 <- data.frame(c(Y1, Y2), c(Xa,Xb), c(Y1t, Y2t), cat)
names(df_2) <- c("Y", "X", "Yt", "cat")
################
# plot(Xa, Y1t, type="l", lty=3, ylim=c(-1, 3))
# lines(Xb, Y2t, lty=3)

# pass data to matlab:
write.csv(df_2, "./matlabCodes_improved/rdat.csv") # set params for matlab script

# run matlab script:
run_matlab_script(fname = "./matlabCodes_improved/mod2.m")

# retrieve the results of the matlab script
bootest <- read.csv("./matlabCodes_improved/bootest.csv") # get the matlab generated model means

# set par
par(mar = c(4, 4, .1, .1))

# plot bootest
mon <- monreg(x = seq(0,1, len=101), y = bootest[boot_rep+1,], hd = 0.08, hr = 0.08)
plot(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02),
     ylim=c(-1, 3), ylab="", xlab="Zhang et al. Model")
for(i in (boot_rep+2):(boot_rep*3)){
  mon <- monreg(x = seq(0,1, len=101), y = bootest[i,], hd = 0.08, hr = 0.08)
  if(i > 2*boot_rep){
  lines(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02))
  }
    lines(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02))
}
lines(Xa, Y1t, type="l", lty=3)
lines(Xb, Y2t, lty=3)

# build bootstrapped model
m2boot <- boot(initFlexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X")), R=boot_rep, verbose=1, model=TRUE, initialize_solution=FALSE)


plot(x = m2boot@parameters[[1]][[1]][[1]][[1]]$mon_obj[,1]$x,
     y = m2boot@parameters[[1]][[1]][[1]][[1]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02), 
     ylim=c(-1, 3), ylab="", xlab="Authors' Model")
for(i in 2:length(m2boot@parameters)){
  lines(x = m2boot@parameters[[i]][[1]][[1]][[1]]$mon_obj[,1]$x,
     y = m2boot@parameters[[i]][[1]][[1]][[1]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02))
  lines(x = m2boot@parameters[[i]][[1]][[1]][[2]]$mon_obj[,1]$x,
     y = m2boot@parameters[[i]][[1]][[1]][[2]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02))
}
lines(Xa, Y1t, type="l", lty=3)
lines(Xb, Y2t, lty=3)


# reset par
par(mar = c(5.1, 4.1, 4.1, 2.1))

@
\captionof{figure}[Abbrviated Caption]{The Zhang et al. model and the authors' model, fit to data generated from an underlying model with constant true priors and variances, as well as ``jagged'' underlying true functions.} \label{fig:comparison_3}
\end{figure}

\begin{figure}[H]
\centering
    \textbf{Algorithm Comparison for Overlapping True Functions}\par\medskip
<<comparison_4, echo=FALSE, eval=FALSE, message=FALSE, results='hide', fig.align='center',warning=FALSE, fig.show='hold', out.width='49%', fig.height=3.5, fig.width=3.5>>=


# data with 2 latent categories, generated with constant prior and constant sigma
################
pi1 <- 0.6
pi2 <- 0.4
n <- 1000

Xa <- sort(runif(n*pi1,0,1)) #seq(-10,10, length.out=1001)
Xb <- sort(runif(n*pi2,0,1)) #seq(-10,10, length.out=1001)


Y1t <- 10*(Xa-0.6)^3 + 1.1
Y1 <- Y1t + rnorm(n*pi1, 0, 0.2) # component 1
Y2t <- 2.5/(1+1000000^(-Xb+0.5)) - 0.3
Y2 <- Y2t + rnorm(n*pi2, 0, 0.2) # component 2
cat <- c(rep.int(1, n*pi1), rep.int(2,n*pi2))
# plot(X,Y1, type="p", cex=0.05)
# lines(X,Y2,type="p", cex=0.05)

df_2 <- data.frame(c(Y1, Y2), c(Xa,Xb), c(Y1t, Y2t), cat)
names(df_2) <- c("Y", "X", "Yt", "cat")
################
# plot(Xa, Y1, type="l", lty=3, ylim=c(-1.5, 3))
# lines(Xb, Y2, lty=3)

# pass data to matlab:
write.csv(df_2, "./matlabCodes_improved/rdat.csv") # set params for matlab script

# run matlab script:
run_matlab_script(fname = "./matlabCodes_improved/mod2.m")

# retrieve the results of the matlab script
bootest <- read.csv("./matlabCodes_improved/bootest.csv") # get the matlab generated model means

# set par
par(mar = c(4, 4, .1, .1))

# plot bootest
mon <- monreg(x = seq(0,1, len=101), y = bootest[boot_rep+1,], hd = 0.08, hr = 0.08)
plot(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02),
     ylim=c(-1.5, 3), ylab="", xlab="Zhang et al. Model")
for(i in (boot_rep+2):(boot_rep*3)){
  mon <- monreg(x = seq(0,1, len=101), y = bootest[i,], hd = 0.08, hr = 0.08)
  if(i > 2*boot_rep){
  lines(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02))
  }
    lines(x = mon$t, y = mon$estimation, type="l", col=alpha("blue", 0.02))
}
lines(Xa, Y1t, type="l", lty=3)
lines(Xb, Y2t, lty=3)

# build bootstrapped model
m2boot <- boot(initFlexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X")), R=boot_rep, verbose=1, model=TRUE, initialize_solution=FALSE)


plot(x = m2boot@parameters[[1]][[1]][[1]][[1]]$mon_obj[,1]$x,
     y = m2boot@parameters[[1]][[1]][[1]][[1]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02), 
     ylim=c(-1.5, 3), yaxt="n", ylab="", xlab="Authors' Model")
for(i in 2:length(m2boot@parameters)){
  lines(x = m2boot@parameters[[i]][[1]][[1]][[1]]$mon_obj[,1]$x,
     y = m2boot@parameters[[i]][[1]][[1]][[1]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02))
  lines(x = m2boot@parameters[[i]][[1]][[1]][[2]]$mon_obj[,1]$x,
     y = m2boot@parameters[[i]][[1]][[1]][[2]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.02))
}
lines(Xa, Y1t, type="l", lty=3)
lines(Xb, Y2t, lty=3)


# reset par
par(mar = c(5.1, 4.1, 4.1, 2.1))


## TEST ## ## TEST ## ## TEST ## ## TEST ## ## TEST ## ## TEST ## ## TEST ## 

# pi1 <- 0.6
# pi2 <- 0.4
# n <- 1000
# 
# Xa <- sort(runif(n*pi1,-10,10)) #seq(-10,10, length.out=1001)
# Xb <- sort(runif(n*pi2,-10,10)) #seq(-10,10, length.out=1001)
# 
# 
# Y1t <- 0.7*(Xa-0.6)^3 + 1.1
# Y1 <- Y1t + rnorm(n*pi1, 0, 70) # component 1
# Y2t <- 1000/(1+2^(-Xb)) - 550
# Y2 <- Y2t + rnorm(n*pi2, 0, 70) # component 2
# cat <- c(rep.int(1, n*pi1), rep.int(2,n*pi2))
# # plot(X,Y1, type="p", cex=0.05)
# # lines(X,Y2,type="p", cex=0.05)
# 
# df_2 <- data.frame(c(Y1, Y2), c(Xa,Xb), c(Y1t, Y2t), cat)
# names(df_2) <- c("Y", "X", "Yt", "cat")
# ################
# plot(Xa, Y1, type="l", lty=3)
# lines(Xb, Y2, lty=3)
# 
# 
# m2boot_B <- boot(initFlexmix(Y ~ X-1, data = df_2, k = 2, model = mono_reg(mon_inc_names = "X")), R=boot_rep, verbose=1, control = list(iter.max = 400), model=TRUE, initialize_solution=FALSE)
# 
# 
# plot(x = m2boot_B@parameters[[1]][[1]][[1]][[1]]$mon_obj[,1]$x,
#      y = m2boot_B@parameters[[1]][[1]][[1]][[1]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.1), 
#      ylim=c(-1000, 1000))
# for(i in 2:length(m2boot_B@parameters)){
#   lines(x = m2boot_B@parameters[[i]][[1]][[1]][[1]]$mon_obj[,1]$x,
#      y = m2boot_B@parameters[[i]][[1]][[1]][[1]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.1))
#   lines(x = m2boot_B@parameters[[i]][[1]][[1]][[2]]$mon_obj[,1]$x,
#      y = m2boot_B@parameters[[i]][[1]][[1]][[2]]$mon_obj[,1]$yf, type="l", col=alpha("blue", 0.1))
# }
# lines(Xa, Y1t, type="l", lty=3)
# lines(Xb, Y2t, lty=3)

@
\captionof{figure}[Abbrviated Caption]{The Zhang et al. model and the authors' model, fit to data generated from an underlying model with constant true priors and variances, as well as intersecting underlying true functions.} \label{fig:comparison_4}
\end{figure}

<<comparison2, echo=FALSE, eval=FALSE>>=

# the R-matlab connection is very finnicky and machine-specific. If you are running this code chun on a different machine, make sure to change your path specification. If the code doesn't run, remove the chunk and move on with our life.

# open matlab connection
# options(matlab="/Applications/MATLAB_R2021a.app/bin/matlab") # set path for starting server
# Matlab$startServer() # star server
# 
# matlab <- Matlab() # store matlab server connection object
# isOpen <- open(matlab) # check if is open?
# 
# setFunction(matlab, " \
# function[x,y] = gen_nmix2(n,numc) \
# 
# % this function generate data for a 2-component nonparametric mixture of  \
# % regression models with some specification.  \
# 
# x = rand(n,1); \
# 
# true_p = [exp(0.5*x)./(1+exp(0.5*x)),1-exp(0.5*x)./(1+exp(0.5*x))]; \
# cum_p = [exp(0.5*x)./(1+exp(0.5*x)),ones(n,1)]; \
# 
# gen_p = rand(n,1); \
# bernulli = zeros(n,numc); \
# 
# bernulli(:,1) = (gen_p<cum_p(:,1)); \
# 
# for i = 2:numc \
# bernulli(:,i) = (cum_p(:,i-1)<gen_p).*(gen_p<cum_p(:,i)); \
# end; \
# 
# e1 = 0.75*exp(0.5*x).*randn(n,1); \
# e2 = 0.65*exp(-0.2*x).*randn(n,1); \
# 
# e1 = 0.60*exp(0.5*x).*randn(n,1); \
# e2 = 0.50*exp(-0.2*x).*randn(n,1); \
# 
# func1 = 3-sin(2*pi*x)+e1; \
# func2 = cos(3*pi*x)+e2; \
# 
# 
# func = [func1,func2]; \
# y = sum(func.*bernulli,2); \
# 
# end \
# ")
# 
# setFunction(matlab, " \
# function [win, aver] = dice(B) \
# %Play the dice game B times \
# gains = [-1, 2, -3, 4, -5, 6]; \
# plays = unidrnd(6, B, 1); \
# win = sum(gains(plays)); \
# aver = win/B; \
# ")
# 
# run_matlab_code(
#   c("x = 5; \ disp(['The value of x is ', num2str(x)])")
#   )
# 
# code <- c(
#   " 
# 
#   function[x,y] = gen_nmix2(n,numc) \
#   
#   % this function generate data for a 2-component nonparametric mixture of  \
#   % regression models with some specification.  \
#   
#   x = rand(n,1); \
#   
#   true_p = [exp(0.5*x)./(1+exp(0.5*x)),1-exp(0.5*x)./(1+exp(0.5*x))]; \
#   cum_p = [exp(0.5*x)./(1+exp(0.5*x)),ones(n,1)]; \
#   
#   gen_p = rand(n,1); \
#   bernulli = zeros(n,numc); \
#   
#   bernulli(:,1) = (gen_p<cum_p(:,1)); \
#   
#   for i = 2:numc \
#   bernulli(:,i) = (cum_p(:,i-1)<gen_p).*(gen_p<cum_p(:,i)); \
#   end; \
#   
#   e1 = 0.75*exp(0.5*x).*randn(n,1); \
#   e2 = 0.65*exp(-0.2*x).*randn(n,1); \
#   
#   e1 = 0.60*exp(0.5*x).*randn(n,1); \
#   e2 = 0.50*exp(-0.2*x).*randn(n,1); \
#   
#   func1 = 3-sin(2*pi*x)+e1; \
#   func2 = cos(3*pi*x)+e2; \
#   
#   
#   func = [func1,func2]; \
#   y = sum(func.*bernulli,2); \
#   
#   end 
#   
#   "
# )
# write.csv(c(3, 2), "./matlabCodes_improved/getme.csv")
# hello <- run_matlab_code("dat = gen_nmix2(10,2); writematrix(dat,'dat1.csv'); ")

# rvec_to_matlab(diag(1, 10), matname="isthison")

# run_matlab_script(fname = "./matlabCodes_improved/test.m")
# 
# 
# comefindme <- read.csv("./matlabCodes_improved/dat1.csv")
# print(comefindme)
# when finished, close matlab server:
# close(matlab) 
@

\subsection{Real Data: Global Life Expectancy }

In this section, we apply the mixture of monotone regressions to global life expectancy data. Consider data on 'GDP per person' and 'Life expectancy' for all countries between the years 1960 and 2018, drawn from the free online resources of the World Bank (\cite{worldbank}). Specifically, the data consists of $n$ observations $(y_1, \vec{x_1}),...,(y_n,\vec{x_n})$, where $Y$ represents Life Expectancy and the vector $\vec{X}$ represents both GDP and Year. 

Moreover, this data has two properties that are very common in real world data:
\begin{enumerate} %[noitemsep] 
  \item Missing Data: Not all countries have data for all years, and several have gaps due to years of conflict in which data was not collected.
  \item \emph{A priori} Groupings: This data contains multiple observations per country, but we expect that our model will constrain countries to be clustered together. 
\end{enumerate}

On a first pass visualization of this data, we find a mostly linear relationship between Life Expectancy \& Year (figure [\ref{lexy}]), and a mostly logarithmic relationship between Life Expectancy \& GDP (figure [\ref{lexgdp}]). 

\begin{figure}[H]
<<motivation, echo=F, fig.align='center', fig.height=3, message=FALSE>>=


# exploratory vizualization

continent %>%
  filter(Region != "") %>%
  group_by(Region, Year) %>%
  summarise(mean_LE = mean(LifeExpectancy)) %>%
ggplot(mapping = aes(x = Year, y = mean_LE, color = as.factor(Region))) + 
  geom_line() +
  theme_minimal() +
  labs(title = "Life Expectancy by Global Region",
       color="Regions") +
  ylab("Life Expectancy (years)") +
  xlab("Year")
@

\captionof{figure}[Abbrviated Caption]{The relationships between Life Expectancy and time are largely linear across all global regions. Sub-saharan Africa uniquely demonstrates what appears to be a cubic relationship over time.} \label{lexy}
\end{figure}

\begin{figure}[H]
<<motivation_b, echo=F, fig.align='center', fig.height=3, message=FALSE>>=
continent %>%
  filter(Region != "") %>%
  group_by(Region, Year) %>%
  summarise(mean_GDP = log(mean(GDP)), mean_LE = mean(LifeExpectancy)) %>%
ggplot(mapping = aes(x = mean_GDP, y = mean_LE, color = as.factor(Region))) + 
  geom_line() +
  theme_minimal() +
  labs(title = "GDP per Capita by Global Region (log scale)",
       color="Regions") +
  ylab("Life Expectancy (years)") +
  xlab("GDP per Capita (current USD, log scale)")
  
  
@
\captionof{figure}[Abbrviated Caption]{The relationships between Life Expectancy and GDP per capita are largely log-linear across global regions. Nearly all regions feature brief noisy sections surrounded by notably larger smooth sections.}  \label{lexgdp}
\end{figure}

For the purposes of demonstration, we choose to model this data without log-transforming the GDP data in order to preserve its highly non-linear relationship with Life Expectancy. We proceed by building two step models: with and without an intercept, and each with `GDP` as the monotone covariate. Each model is in fact a series of 30 mixture models, 3 for each of $k = 1,...,10$. These models have the form of equations [\ref{m6}] and [\ref{m7}] respectively.

\begin{equation} \label{m6}
  Y = \sum_{k=1}^{K}\pi_k (g_{k} (GDP) \ +\  \beta_{k}\cdot Year \ +\ \epsilon_k)
\end{equation}

\begin{equation} \label{m7}
  Y = \sum_{k=1}^{K}\pi_k (g_{k} (GDP) \ +\  \beta_{0,k} \ +\ \beta_{1,k}\cdot Year \ +\ \epsilon_k)
\end{equation}

For each series, we plot the AIC and BIC per $k$, the rootogram of the model with the lowest AIC, and the fitted monotone functions for the model with the lowest AIC.


% [PLOT MODELS AIC/BIC AND MONOTONE COMPS HERE]

<<le_mod6, eval=FALSE, echo=FALSE, results=F>>=

m6 <- stepFlexmix(LifeExpectancy ~ .-1-Country.Name|Country.Name, data = le, k = 1:4, model = mono_reg(mon_inc_names = "GDP"), nrep = 1) # step model | no intercept | grouped | monotone GDP

@

<<le_mod7, eval=FALSE, echo=FALSE, results=F>>=

m7 <- stepFlexmix(LifeExpectancy ~ .-Country.Name|Country.Name, data = le, k = 1:4, model = mono_reg(mon_inc_names = "GDP"), nrep = 1) # step model | with intercept | grouped | monotone GDP

@





\begin{figure}[H]
<<le_mod_plot1, eval=FALSE, echo=F, message=F, fig.align='center', fig.height=4>>=

# m6
plot(m6, main="AIC / BIC / ICL in Model 6 by Number of Components")

@
\captionof{figure}[Abbreviated Caption]{Here we observe the various model selection metrics -- AIC, BIC and ICL -- for models represented by equation [\ref{m6}], constructed with each of $k$ components.}  
\end{figure}

\begin{figure}[H]
<<le_mod_plot1_a, eval=FALSE, echo=F, message=F, fig.align='center', fig.height=4>>=

# TODO select ACTUAL lowest AIC
num <- which.min(AIC(m6))

plot(m6@models[[num]], palet="Dark2", root_scale="sqrt", subplot=1) # rootogram

@
\captionof{figure}[Abbreviated Caption]{Here we observe the rootogram of model [\ref{m6}] with the lowest AIC. The high proportion of observations classed near the outer limits of 0 and 1 indicate a well-separated model.}  
\end{figure}

\begin{figure}[H]
<<le_mod_plot1_b, eval=FALSE, echo=F, message=F, fig.align='center', fig.height=4>>=

 # format data for adding fitted Life Expectancy curves by country
plot_data <- le
plot_data$Cluster <- factor(m6@models[[num]]@cluster)
plot_data$fitle <- plot_data$LifeExpectancy - (sapply(plot_data$Cluster, function(x) m6@models[[num]]@components[[x]][[1]]@parameters$coef)*plot_data$Year)
plot(m6@models[[num]], log=T, subplot=2) + # overlay monotone plots with plots of individual countries
  geom_line(data = plot_data, mapping = aes(GDP, fitle, group=Country.Name, color=Cluster), alpha=0.1) +
  scale_color_brewer(palette="Dark2") +
  theme_bw() + 
  scale_x_log10() # set to log scale
# TODO 3-D plot?

@
\captionof{figure}[Abbreviated Caption]{Here we observe the monotone functions for all components in the fitted model [\ref{m6}] with lowest AIC. Observed values are overlaid in the colour of the component to which each country is assigned. Since the Y-value only represents the additive contribution of the monotone term, its range spans across 0 and into negative real numbers.}  
\end{figure}

\begin{figure}[H]
<<le_mod_plot2, eval=FALSE, echo=F, message=F, fig.align='center', fig.height=4>>=

############
# m7
plot(m7, main="AIC / BIC / ICL in Model 7 by Number of Components")

@
\captionof{figure}[Abbreviated Caption]{Here we observe the various model selection metrics -- AIC, BIC and ICL -- for models represented by equation [\ref{m7}], constructed with each of $k$ components. }
\end{figure}

\begin{figure}[H]
<<le_mod_plot2_a, eval=FALSE, echo=F, message=F, fig.align='center', fig.height=4>>=

# TODO select ACTUAL lowest AIC
num <- which.min(AIC(m7)) 

plot(m7@models[[num]], palet="Dark2", root_scale="sqrt", subplot=1) # rootogram

@
\captionof{figure}[Abbreviated Caption]{Here we observe the rootogram of model [\ref{m7}] with the lowest AIC. Again, the high proportion of observations classed near the outer limits of 0 and 1 indicate a well-separated model.}
\end{figure}

\begin{figure}[H]
<<le_mod_plot2_b, eval=FALSE, echo=F, message=F, fig.align='center', fig.height=4>>=

 # format data for adding fitted Life Expectancy curves by country
plot_data <- le
plot_data$Cluster <- factor(m7@models[[num]]@cluster)
plot_data$fitle <- plot_data$LifeExpectancy - apply(t(sapply(plot_data$Cluster, function(x) m7@models[[num]]@components[[x]][[1]]@parameters$coef))*cbind(rep.int(1, dim(plot_data)[2]), plot_data$Year), 1, sum) # jesus... subtract from life expectancy the SUM of the intercept and the GDP coeffecient (of the correct cluster) x the correct country-year GDP
plot(m7@models[[num]], log=T, subplot=2) + # overlay monotone plots with plots of individual countries
  geom_line(data = plot_data, mapping = aes(GDP, fitle, group=Country.Name, color=Cluster), alpha=0.1) +
  scale_color_brewer(palette="Dark2") +
  theme_bw() +
  scale_x_log10() # set to log scale
@
\captionof{figure}[Abbreviated Caption]{Here we observe the monotone functions for all components in the fitted model [\ref{m7}] with lowest AIC. Observed values are overlaid in the colour of the component to which each country is assigned. Again, since the Y-value only represents the additive contribution of the monotone term, its range spans across 0 and into negative real numbers.}
\end{figure}
 

Next, we would typically plot the distribution of clusters within the lowest-AIC model of each step-model series, projected onto a world map. For demonstration purposes, we instead plot the 4-component model from each series. In these world map plots, the colors of each cluster span a spectrum from white to full-color, representing the strength of the posterior and the confidence of the model in placing a given country within a given cluster. The world-maps indicate what the rootograms had previously indicated, namely that the resulting models are extremely confident about the clustering of nearly all countries.

\begin{figure}[H]
<<worldmaps1, eval=FALSE, echo=F, message=F, fig.align='center', fig.height=4>>=

world <- ne_countries(scale = "medium", returnclass = "sf")

mod6 <- m6@models[[4]]


maxpost <- apply(mod6@posterior$scaled, 1, function(x) max(x))
clustdat <- tibble(le$Country.Name, mod6@cluster, maxpost)
names(clustdat) <- c("name", "cluster", "posterior")
plot_data <- merge(world, clustdat, by = "name", all.x = TRUE)

gg <- ggplot()
pal <- brewer.pal(length(mod6@components), "Dark2")

for(j in 1:length(mod6@components)){
  gg <- gg + geom_sf(data = plot_data[plot_data$cluster==j,], aes(fill = posterior)) + 
  scale_fill_gradient2(paste("Cluster", j), limits=c(0,1), low = "white", high = pal[j]) +
  theme(legend.direction = "horizontal") +
 
  new_scale("fill")    
}

gg <- gg + theme_minimal() + 
  theme(legend.direction = "horizontal") + 
  ggtitle("Map of Country Clusters in m6")


# TODO extract legend grob, edit, and replace with title. If this doesnt work, we resort to adding a text annotation
g <- ggplotGrob(gg) # Get the ggplot grob
leg <- g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] # Get the legend
title_grob <- textGrob("Posterior", gp = gpar(fontsize = 12))
table_grob <- gtable_add_rows(leg, heights = grobHeight(title_grob) + unit(5,'mm'), pos = 0)
table_grob <- gtable_add_grob(table_grob, title_grob, 1, 1, 1, ncol(table_grob), clip = "off")
g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] <- table_grob # replace original grob with edited grob

ggpubr::as_ggplot(g) + theme_minimal() # cast back to ggplot, and print


@
\captionof{figure}[Abbreviated Caption]{Here we observe the world map with colour code corresponding to the results of model [\ref{m6}].}
\end{figure}

\begin{figure}[H]
<<worldmaps2, eval=FALSE, echo=F, message=F, fig.align='center', fig.height=4>>=

##################
world <- ne_countries(scale = "medium", returnclass = "sf")

mod7 <- m7@models[[4]]


maxpost <- apply(mod7@posterior$scaled, 1, function(x) max(x))
clustdat <- tibble(le$Country.Name, mod7@cluster, maxpost)
names(clustdat) <- c("name", "cluster", "posterior")
plot_data <- merge(world, clustdat, by = "name", all.x = TRUE)

gg <- ggplot()
pal <- brewer.pal(length(mod7@components), "Dark2")

for(j in 1:length(mod7@components)){
  gg <- gg + geom_sf(data = plot_data[plot_data$cluster==j,], aes(fill = posterior)) +
  scale_fill_gradient2(paste("Cluster", j), limits=c(0,1), low = "white", high = pal[j]) +
  theme(legend.direction = "horizontal") +

  new_scale("fill")
}

gg <- gg + theme_minimal() +
  theme(legend.direction = "horizontal") +
  ggtitle("Map of Country Clusters in m7")

# TODO extract legend grob, edit, and replace with title. If this doesnt work, we resort to adding a text annotation
g <- ggplotGrob(gg) # Get the ggplot grob
leg <- g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] # Get the legend
title_grob <- textGrob("Posterior", gp = gpar(fontsize = 12))
table_grob <- gtable_add_rows(leg, heights = grobHeight(title_grob) + unit(5,'mm'), pos = 0)
table_grob <- gtable_add_grob(table_grob, title_grob, 1, 1, 1, ncol(table_grob), clip = "off")
g$grobs[[which(g$layout$name == "guide-box")]]$grobs[[1]] <- table_grob # replace original grob with edited grob

ggpubr::as_ggplot(g) + theme_minimal() # cast back to ggplot, and print


@
\captionof{figure}[Abbreviated Caption]{Here we observe the world map with colour code corresponding to the results of model [\ref{m7}].}
\end{figure}


Finally, we iteratively refit model [\ref{m6}] while excluding all observations from a series of countries -- Argentina, Canada, Iran, Senegal, Switzerland, Thailand, chosen to represent a variety of countries from different regions -- in order to demonstrate the predictive capacities of the mixture model. As stated previously, in section [\ref{prediction}], there are two possible contexts in which one might use the mixture model predictively. One may have a complete observation or set of observations, e.g., the \texttt{Year}, \texttt{GDP} and \texttt{Life Expectancy} data for a given country over several years, in which case one could use the mixture model to generate a set of posterior probabilities representing the probability of that country belonging to each of the model clusters. Alternately, one may have an incomplete observation or set of observations, e.g., the \texttt{Year} and \texttt{GDP} data (but not \texttt{Life Expectancy}) for a country over several years, in which case one could use the mixture model to generate a conditional marginal distribution of \texttt{Life Expectancy} for the given country.


\begin{figure}[H]
<<predictions, eval=FALSE, echo=F, message=F, fig.align='center', out.width='4cm', out.height='4cm', fig.show='hold'>>=
# TODO fig alignemnt option: fig.align='center'
# TODO demonstrate predictions of new observed X
# TODO predictions should give: a posterior distribution of cluster belonging, and
# a marginal distribution of Y given X


country_list <- c("Canada", "Iran", "Paraguay", "Senegal", "Switzerland", "Thailand")

mod6 <- m6@models[[which.min(AIC(m6))]] # testing running marginal plots on lowest AIC mod

# par(mfrow=c(3,2))
for(name in country_list){
    exdat <- le[le$Country.Name == name,]
    
    # refit mod6
    # refit <- flexmix(LifeExpectancy ~ .-1 -Country.Name|Country.Name, data = le[le$Country.Name != name,], k = num, model = mono_reg(mon_inc_names = "GDP")) # num-component model | with intercept | grouped | monotone GDP

    # TODO predict cluster membership for complete covariate set (y, x)
    # predict_cluster(refit, exdat[, c(2,3,4)])

    # TODO change mod6 to refit
    retval <- predict_marginal(mod6, exdat[,c(2,4)]) # get marginal distribution parameters for each observation in exdat
    x <- exdat$Year 
    y <- seq(40, 90, length.out = 1000) # set grid points for drawing densities from the marginal distributions
    z <- t(matrix(sapply(retval, function(i) dnormm(y, p = i$prior, mu = i$mus, sigma = i$sigmas)), ncol = length(x))) # get density at grid points
    
    # a hacky way of assigning labels...
    if(name == "Canada"){
      #Heatmap
    image(x,y,z,main=name, ylab="Life Expectancy", xlab="", cex.main=3, cex.lab=3, cex.axis=2) # print heat map of distribution
    lines(x = exdat$Year, y = exdat$LifeExpectancy) # add observed life expectancy
    }
    
    if(name == "Senegal"){
      #Heatmap
    image(x,y,z,main=name, xlab="Year", ylab="Life Expectancy", cex.main=3, cex.lab=3, cex.axis=2) # print heat map of distribution
    lines(x = exdat$Year, y = exdat$LifeExpectancy) # add observed life expectancy
    }
    
    if(name == "Switzerland" | name == "Thailand"){
      #Heatmap
    image(x,y,z,main=name, ylab="", xlab="Year", cex.main=3, cex.lab=3, cex.axis=2) # print heat map of distribution
    lines(x = exdat$Year, y = exdat$LifeExpectancy) # add observed life expectancy
    }
    
    if(name == "Iran" | name == "Paraguay"){
      #Heatmap
    image(x,y,z,main=name, ylab="", xlab="", cex.main=3, cex.lab=3, cex.axis=2) # print heat map of distribution
    lines(x = exdat$Year, y = exdat$LifeExpectancy) # add observed life expectancy
    }
}

@
\captionof{figure}[Abbreviated Caption]{Marginal distributions of Life Expectancy over time for each country, corresponding to the results of model [\ref{m6}]. Observed Life Expectancy values are superimposed in black.}
\end{figure}


The marginal distributions produced by model [\ref{m6}] illustrate the flexibility of the model, and at the same time highlight some of its crucial assumptions. First, the \emph{must-link} constraints, which form the groupings of the model and ensure that all observations from a single country are clustered together, assume rather minimally that observations from a single country are \emph{i.i.d.} and drawn from some common underlying distribution. In this case, that is an untenable assumption since observations were in fact drawn sequentially, and one should therefore assume autocorrelation among the observations, as with any time-series data. 

Second, as discussed in section [\ref{fmrs}], the model is constructed such that the posterior probabilities of an observation ($\boldsymbol{\Lambda}_i$ for some $i$, to keep with earlier notation) depend on covariates $\boldsymbol{X}_i$ only through dependent variable $Y_i$. The simplicity of this dependence structure is advantageous, but for the predictive task of determining a marginal distribution of $Y$ for an observed $\boldsymbol{X} = \boldsymbol{x}$, the resulting distribution is a mixture proportionate only to the uninformed priors. Thus, although in model [\ref{m6}] countries are clearly being clustered with respect to economic development and therefore, in some way, with respect to GDP per capita, the marginal distributions of Life Expectancy for a wealthy country such as Switzerland and a poorer country such as Senegal will have the same mixing proportions regardless of observed GDP per capita. A clear solution to this would be to model the priors $\boldsymbol{\pi}$ of the mixture model as additionally dependent on covariates $\boldsymbol{X}$, as is typically done in mixture-of-expert models (c.f., section [\ref{fmrs}]).



\section{Discussion}

The model presented in this paper provides a general and robust option for representing mixtures of regressions where some, or all, of the regression terms are non-parametric, monotone functions. There are several advantages to this implementation which make it simpler and more versatile. First, this model generalizes univariate mixtures of monotone regressions to the multivariate case, where any finite number of both linear and monotone terms can be specified within each mixture component. Second, this model is free of any tuning parameters in the component regression functions, which allows the overall model to be computationally simpler and leaves fewer modelling decision to the user. Finally, this model fits each monotone term in its mixture components in a single step, thereby avoiding the bias inherent in a two-step process, and giving the model flexibility in fitting data for which the true underlying functions are jagged and/or intersect. This flexibility is especially important for modelling in contexts where assuming that the model components are well separated or divisible by a hyperplane is unrealistic.


\subsection{Weaknesses}

Although the model presented in this paper makes some substantial improvements on its predecessor, the Zhang et al. model, these improvements are conditional upon the data being modelled. The Zhang et al. model still has lower variance in contexts where the true underlying components have sufficiently smooth monotone terms and are sufficiently well-separated. When these are reasonable assumptions to make, the Zhang et al. model is clearly preferable. Moreover, given that users may have little reason to believe that these assumptions are or are not reasonable, deciding which model to use may be difficult.

Another, conext-specific weakness of our model is that the monotone component of each regression is not a smooth function but rather a step-wise function. For cases in which a smooth function is required, this is prohibitive. However, where smooth monotone functions are not required, the advantages of our model may outweigh this cost.

\subsection{Future Developments}

There are three clear directions for future development with respect to the model presented in this paper. First, the model could be generalized to accept alternate types of shape constraints -- for example, convex or \emph{k}-monotone constraints. This would greatly extend the model's applicability, especially in cases where model components might combine nonparametric terms with different types of shape constraint.

Second, the model could be generalized to fit regressions with non-normal conditional errors. Since mixtures of generalized linear models have previously been implemented (see, for example, \cite{mixglms}) and the MLE isotonic estimator has already been generalized to non-normal errors (see \cite{genpava}), we expect that this would be a simple practical extension. The clearest advantage to such an extension would be the ability to model mixtures of partially monotone logistic regressions. Such models would have ready applications in, for example, epidemiological studies like \cite{morton} where the outcomes are binary disease states instead of continuous disease severities. 

Finally, the model could be generalized to fit mixtures with coefficient-dependent priors, as do the model of Zhang et al. and other variations of the mixture-of-experts model. Although such models do not necessarily have interpretable prior parameters, they are much better suited to applications that place high importance on prediction.




\pagebreak
\bibliography{mono}

\pagebreak
\begin{appendices}

\section{Asymptotic Behaviour of Partially Linear Models with Monotone Constraints} \label{abplmmc}

As discussed in section [\ref{plmmc}], the asymptotic behaviour of partially linear models with monotone constraints described by \cite{guangcheng} requires that the assumptions below hold.

Let the estimators $\hat{\beta}_n, \hat{h}_1, ..., \hat{h}_J$ be defined as the minimizer of 

\begin{equation} \label{guangassum}
   S_n(\beta, h_1, ..., h_j) \ =\ n^{-1} \sum_{i=1}^{n}(Y_i - X'_i\beta - \sum_{j=1}^{J}h_j(W_{ij}))^2
\end{equation}

where each $h_j$ is a monotone function with bounded derivative and $\int h_j(w_j)dw_j = 0$. Then, the following are regularity conditions for $1 \leq j \leq J$:

\begin{enumerate}
  \item The function $h_j$ satisfies the condition that
  \begin{equation}
   \text{inf}_{|w_j - w'_j|\geq \delta} \ |h_j(w_j) - h_j(w'_j)| \ \geq \ C_1 \delta^{\gamma}
  \end{equation}
for any $\delta > 0$ and some constants $C_1, \gamma > 0$.

  \item The density for $W_j$, denoted as $p_{w_j}$, is assumed to be bounded away from zero and infinity, and fulfills the below Lipschitz condition
  \begin{equation}
   \text{sup}_{-1 \leq w_j, w'_j \leq 1} \ |p_{w_j}(w_j) - p_{w_j}(w'_j)| \ \leq \ M |w_j - w'_j|^{\rho}
  \end{equation}
for some constants $M, \rho > 0$.

  \item The function $\zeta_j(w) \equiv E(X|W_j = w)$ satisfies the condition
  \begin{equation}
   || \zeta_j(w_j) - \zeta_j(w'_j) || \leq \ C_2 |w_j - w'_j|
  \end{equation}
  for some constant $C_2 > 0$.
  
  \item $E(X - \sum_{j=1}^{J}E(X|W_j))^{\otimes 2}$ is strictly positive.
\end{enumerate}

\section{Flexmix Extension}


All results in this paper were produced by an extension of Flexmix, a flexible implementation of generalized finite mixture models created by Bettina Grun and Friedrich Leisch (\cite{flexmix}). The package provides a framework for implementing specific types of mixture models based on a central, universal framework. It is thus intended to allow users to elaborate a specific estimator or ``driver'' for the modeling of mixture components, while the more abstract behaviours are managed by the Flexmix backend. More specifically, Flexmix implements:

\begin{enumerate}
  \item The universal functions of the EM algorithm for assigning prior values $\pi_k$ to each of the mixture components and posterior values $\Lambda_i$ to each of the observations upon each iteration of the EM algorithm;
  \item Threshold constants for the convergence of the EM algorithm;
  \item Restrictions on the model estimate, e.g., restricting components to have an estimated prior above a certain threshhold;
  \item Ordinary- and parametric-bootstrap methods for the estimation of confidence intervals with respect to each component.
\end{enumerate}

Conversely, the user must provide a model estimator and a model object with a log-likelihood method, \texttt{logLik()}, which returns the density of an observation given a component's parameters, and a prediction function, \texttt{predict()}, which gives an expected value of an observation's dependent variable given the observed independent variables.

To implement the Flexmix extension, one must call \texttt{flexmix()} from the Flexmix package with \texttt{mono\_reg()} as the model argument. E.g., the following call to \texttt{flexmix()} produces a model with 6 components, each of which is a partial linear model regressing \texttt{Y} on all other 
variables of \texttt{df}, excluding an intercept. The \texttt{mon\_inc\_index} argument to \texttt{mono\_reg()} instructs the function to estimate an isotonic function on the second independent variable in \texttt{df}.

<<dem1, eval=FALSE>>=
mod <- flexmix(Y ~ .-1, data = df, k = 6, model = mono_reg(mon_inc_index = 2))
@

It should be noted that the indexing arguments passed to \texttt{mono\_reg()} are indices of the design matrix constructed by the formula passed to \texttt{flexmix()}, so they change based on the design matrix. For example, without an intercept, \texttt{mono\_inc\_index = 2} refers to the 2nd independent variable of the data frame; when an intercept \emph{is} included, \texttt{mono\_inc\_index = 2} refers to the 1st independent variable of the data frame \texttt{df}. It is therefore safer and recommended that the user specify the monotone terms by name using the \texttt{mono\_inc\_names} or \texttt{mono\_dec\_names} arguments, as below.

<<dem2, eval=FALSE>>=
mod <- flexmix(Y ~ ., data = df, k = 6, model = mono_reg(mon_inc_names = "Var_2"))
@

The extension allows the user to include any number of isotonic and/or antitonic (monotone-non-decreasing, monotone-non-increasing) variables in the call to \texttt{mono\_reg}, as demonstrated below.

<<dem3, eval=FALSE>>=
mod <- flexmix(Y ~ ., data = df, k = 6, model = mono_reg(mon_inc_names = 
                c("Var_2", "Var_4"), mon_dec_names = c("Var_3", "Var_6")))
@

Flexmix also allows the construction of multiple mixture models within a single object. The following call to \texttt{stepFlexmix()} builds 25 mixture models. For each of $k$ equal to $1$ through $5$ components, \texttt{nrep} specifies the construction of 5 models. Each model contains 2 monotone components -- a monotone-non-decreasing, or isotonic, relationship between \texttt{Y} and \texttt{Var\_2}, and a monotone-non-increasing, or antitonic, relationship between \texttt{Y} and \texttt{Var\_3}.

<<dem4, eval=FALSE>>=
m2 <- stepFlexmix(Y ~ ., data = df, model = mono_reg(mon_inc_names = 
                "Var_2", mon_dec_names = "Var_3"), k = 1:5, nrep = 5)
@

A notable weakness in the Flexmix architecture is that the structure of all components must be specified identically within a given model. Thus, whereas in theory one could fit a mixture of monotone regressions where different components have different monotone directions -- e.g., the first component is monotone non-decreasing and the second component is monotone non-increasing -- this is not possible with the current instantiation of Flexmix. 

For further discussion of the use of the Flexmix package, see the guides at \url{https://ro.uow.edu.au/cgi/viewcontent.cgi?article=3410&context=commpapers} and \url{https://cran.rapporter.net/web/packages/flexmix/vignettes/mixture-regressions.pdf}, or the following blog post (\url{https://www.r-bloggers.com/2013/06/estimating-finite-mixture-models-with-flexmix-package/}).


% WHAT ELSE DOES FLEXMIX DO FOR US?

The complete code for the extension of Flexmix for modelling mixtures of partially-linear monotone regressions is included below. The first code block implements the partial linear model with monotone shape constraints; the second code block integrates the partial linear model with the Flexmix framework; the third code block provides additional functions for the visualization of results.

% initialize solution (CITE Finite Mixture Model Diagnostics Using Resampling Methods)

\subsection{Code for the Estimation of Partial Linear Models}

<<part_fit, eval=FALSE>>=
# define estimator for partial linear model with arbitrary monoto-
# ne-constrained component

cpav <- function(x_mat, y, weights, inc_index=NULL, dec_index=NULL
                 , max_iters_cpav=NULL, max_delta_cpav=NULL){
  
  joint_ind <- c(inc_index, dec_index)
  
  if(!is.matrix(x_mat)) stop("x_mat is not of class matrix, and 
                             will be rejected by lm.wfit")
  if(any(weights == 0)) stop("monoreg(), and therefore cpav(), 
                             cannot take weights of 0!")
  if(length(y) != length(weights) | length(y) != 
    dim(x_mat)[1]) stop("The dimension of the inputs is not 
                                        equal to the dimension 
                                        of the weights")
  
  # if there is only 1 monotone component, apply ordinary 
  # monotone regression
  if(length(joint_ind) == 1){
    if(length(inc_index) == 1){ # the component is monotone 
      # increasing
      return( # cast the monoreg object as a matrix, with all 
        #attributes as rows in the first column
        matrix(suppressWarnings(monoreg(x = x_mat[,inc_index], 
                                        y = y, w = weights)), 
               dimnames = list(c("x", "y", "w", "yf", "type", 
                                 "call")))
      )}
    else{ # the component is monotone decreasing
      return( # cast the monoreg object as a matrix, with all 
        #attributes as rows in the first column
        
        matrix(suppressWarnings(monoreg(x = x_mat[,dec_index], 
                                        y = y, w = weights, type = 
                                          "antitonic")), 
               dimnames = list(c("x", "y", "w", "yf", "type", 
                                 "call")))
      ) } }
  
  else{ # the monotone components are multiple, so continue with 
    # cyclic algorithm
    
    # fit ordinary lm on x_mat and y
    start_betas <- coef(lm.wfit(x=x_mat[,joint_ind], y=y, 
                                w=weights))
    
    # set initial monotone reg estimates by calling each monoreg() 
    #against y - lm.predict(all other vars)
    
    mr_fits <- sapply(1:length(joint_ind), function(i) 
      if(joint_ind[i] %in% inc_index){
        # i apologize to anyone trying to read this line, 
        # but think: the columns of the x_matrix
        # indicated by join_ind, except the value of joint_ind at 
        # the ith place in joint_ind
        suppressWarnings(monoreg(x = x_mat[,joint_ind[i]], 
                y = (y - (as.matrix(x_mat[,joint_ind[-i]]) %*% 
                            start_betas[-i]) ), w = weights, 
                type = "isotonic"))
        }
      else if(joint_ind[i] %in% dec_index){
        suppressWarnings(monoreg(x = x_mat[,joint_ind[i]], 
                y = (y - (as.matrix(x_mat[,joint_ind[-i]]) %*% 
                            start_betas[-i]) ), w = weights, 
                type = "antitonic"))
      })
    
    # iterate through mr_fits. each column of mr_fits (e.g., 
    # mr_fits[,1]) is a monoreg fitted object,
    # and its attributes can be called (e.g., mr_fits[,1]$yf)
    iters <- 0
    delta <- 0.5
    if(is.null(max_iters_cpav)){
      max_iters_cpav <- 100
    }
    if(is.null(max_delta_cpav)){
      max_delta_cpav <- 0.00001}
    
    while(abs(delta) > max_delta_cpav & iters < max_iters_cpav){
      old_SS <- mean((y - get_pred(mr_fits, x_mat[,joint_ind]))^2)
      
      for(i in 1:length(joint_ind)){
        if(joint_ind[i] %in% inc_index){
          # i apologize to anyone trying to read this line, but 
          # think: the columns of the x_matrix
          # indicated by join_ind, except the value of joint_ind 
          # at the ith place in joint_ind
          mr_fits[,i] <- suppressWarnings(monoreg(x = 
                                        x_mat[,joint_ind[i]],
                                 y = (y - get_pred(mr_fits[,-i], 
                                        x_mat[,joint_ind[-i]])), 
                                 w = weights, type = "isotonic"))}
        else if(joint_ind[i] %in% dec_index){
          mr_fits[,i] <- suppressWarnings(monoreg(x = 
                                        x_mat[,joint_ind[i]],
                                 y = (y - get_pred(mr_fits[,-i], 
                                        x_mat[,joint_ind[-i]])), 
                                 w = weights, type = "antitonic"))}}

      new_SS <- mean((y - get_pred(mr_fits, x_mat[,joint_ind]))^2)
      delta <- (old_SS - new_SS)/old_SS
      
      iters <- iters + 1 }
    
    return(mr_fits)
  }}

# first, define function for obtaining f(x_new) for monotone 
# regression f()
# get_pred returns a vector of length = nrows(xvals), ie, a value 
# for each observation of xvals
get_pred <- function(mr_obj, xvals){
  xvals <- as.matrix(xvals)
  mr_obj <- as.matrix(mr_obj)
  
  if(dim(mr_obj)[2] != dim(xvals)[2]) stop("get_pred() must take 
  an X-matrix with as many columns
                                            as monoreg() objects")
  if(dim(xvals)[1] == 1){ # if xvals is a single observation
    sapply(1:ncol(xvals), function(j)
      mr_obj[,j]$yf[sapply(xvals[,j], function(z)
        ifelse( z < mr_obj[,j]$x[1], 1,
                ifelse(z >= tail(mr_obj[,j]$x, n=1), 
                       length(mr_obj[,j]$x), 
                       which.min(mr_obj[,j]$x <= z)-1 )))])}
  else{
    apply(sapply(1:ncol(xvals), function(j)
         mr_obj[,j]$yf[sapply(xvals[,j], function(z)
           ifelse( z < mr_obj[,j]$x[1], 1,
            ifelse(z >= tail(mr_obj[,j]$x, n=1), 
                   length(mr_obj[,j]$x), 
                   which.min(mr_obj[,j]$x <= z)-1 )))]
         ), 1, function(h) sum(h))}}

# define partial linear regression of y on x with weights w
# inputs are: x, y, wates, mon_inc_index, mon_dec_index, max_iter
part_fit <- function(x, y, wates = NULL, mon_inc_index=NULL, 
                     mon_dec_index=NULL, max_iter=NULL, 
                     component = NULL, na.rm=T, 
                     mon_inc_names = NULL, 
                     mon_dec_names = NULL, start_fit = NULL, ...){

  # cast x to matrix
  x <- as.matrix(x)
  
  # set default weights
  if(is.null(wates)) wates <- rep(1, length(y))
  
  # remove incomplete cases
  if(T){ # for now, there is no alternative to na.rm=T.  All 
    # incomplete cases are removed.
    cc <- complete.cases(y) & complete.cases(x) & 
      complete.cases(wates)
    y <- y[cc]
    x <- x[cc,, drop=FALSE]
    wates <- wates[cc]
    cc <- NULL}
  
  x <- as.matrix(x) # cast again. hacky but apparently necessary
  
  # make sure y and wates is not multivariate
  if(length(y) != dim(x)[1] | length(y) != length(wates)) stop(
      "Inputs are not of the same dimension!")
  
  # take monotone indices of previous component
  if(!is.null(component)){
    inc_ind <- component$mon_inc_index
    dec_ind <- component$mon_dec_index}
  else{
    if(is.null(mon_inc_index) & is.null(mon_dec_index) & 
       is.null(mon_inc_names) & is.null(mon_dec_names)){
      stop("Some monotone index or name must be specified")
    }
    if(!is.null(mon_inc_names)){ # add names to index list
      mon_inc_index <- unique(c(mon_inc_index, which(colnames(x) 
                                          %in% mon_inc_names)))
    }
    if(!is.null(mon_dec_names)){
      mon_dec_index <- unique(c(mon_dec_index, which(colnames(x) 
                                          %in% mon_dec_names)))
    }
    # transfer inc_names to inc_index
      inc_ind <- mon_inc_index
      dec_ind <- mon_dec_index}
 
  # throw warning if there are duplicates in inc_ind or dec_ind, 
  # and then remove
  if(anyDuplicated(inc_ind) | anyDuplicated(dec_ind)){
    warning("There are duplicate index instructions; Duplicates 
            are being removed.")
    inc_ind <- unique(inc_ind)
    dec_ind <- unique(dec_ind)}
  
  # throw error if indices overlap
  if(length(intersect(inc_ind, dec_ind)) > 0) stop("At least one 
  variable was marked as BOTH
                                               monotone increasing 
  and monotone decreasing.")
  
  # throw error if indices are not integers
  if(!is.null(inc_ind)){
    if(any(inc_ind != as.integer(inc_ind))) stop("Monotone 
                      increasing indices are not integers.")
  }
  if(!is.null(dec_ind)){
    if(any(dec_ind != as.integer(dec_ind))) stop("Monotone 
                      decreasing indices are not integers.")}
  
  # throw error if indices are not positive 
  if(any(c(inc_ind, dec_ind) < 1)) stop("all monotone component 
                                        indices must be positive")
  
  # throw error if the number of indices exceeds columns of x
  if(length(c(inc_ind, dec_ind)) > ncol(x)) stop("Number of 
      proposed monotonic relationships exceeds columns of x.")
  
  # If there is an intercept but no other linear effects, stop
  if((length(c(inc_ind, dec_ind))+1) == ncol(x) & "(Intercept)" 
     %in% colnames(x)){
    stop("For identifiability purposes, you cannot build a 
      part_fit with only an intercept as a linear component.")}

  # If start_fit is specified, make sure it has only part_fit  
  # elements with the appropriate dimensions
  if(!is.null(start_fit)){
    
    if(!is(start_fit, "list")) stop("start_fit must be a list of 
                                    part_fit attributes")
    if("coef" %in% names(start_fit)){
      if(length(start_fit$coef) != (dim(x)[2] - length(c(inc_ind, 
                                                  dec_ind))) ){
        stop("Not the right number of coefficients in 
             starting values")
      }
      if(!all(names(start_fit$coef) %in% colnames(x)[-c(inc_ind, 
                                                  dec_ind)]) ){
        stop("Some coefficient(s) in starting values have 
             incorrect names")
      }
      if(!all(colnames(x)[-c(inc_ind, dec_ind)] %in% 
              names(start_fit$coef)) ){
        stop("Some coefficient(s) in starting values are missing")
      }
      if(!all(names(start_fit$coef) == colnames(x)[-c(inc_ind, 
                                                  dec_ind)])){
        start_fit$coef <- start_fit$coef[
          match(colnames(x)[-c(inc_ind, 
                  dec_ind)],start_fit$coef)] }}}
  
  # option for fit with no linear independent components and one 
  # or multiple monotone components:
  if(length(c(inc_ind, dec_ind)) == ncol(x)){
    
    yhat <- cpav(x_mat = as.matrix(x[wates != 0,]), y = 
                   y[wates != 0],
                 weights = wates[wates != 0], 
                 inc_index = inc_ind, dec_index = dec_ind)

    # get residuals of model
    resids <- y - get_pred(yhat, x[,c(inc_ind, dec_ind)])
    
    # mod must have: coef attribute, sigma attribute, cov 
    # attribute, df attribute, ..., and 
    # may have mon_inc_index and mon_dec_index attributes
    mod <- list(coef = NULL, fitted_pava = NULL, sigma = NULL, 
                df = NULL, mon_inc_index = NULL, 
                mon_dec_index = NULL, 
                iterations = NULL, 
                mon_inc_names = NULL, mon_dec_names = NULL)
  
    mod$coef <- NULL
    mod$fitted_pava <- yhat
    mod$mon_inc_index <- inc_ind
    mod$mon_dec_index <- dec_ind
    mod$sigma <- sqrt(sum(wates * (resids)^2 /
                              mean(wates))/ (nrow(x)-qr(x)$rank))
    mod$df <- ncol(x)+1
    class(mod) <- "part_fit"
    
    return(mod)
  }
  else{
    # for starting values, fit a regular lm or use starting values
    if(!is.null(start_fit) && "coef" %in% names(start_fit)){
      betas <- start_fit$coef
    }
    else{
      fit <- lm.wfit(x=x, y=y, w=wates)
      betas <- coef(fit)[-c(inc_ind, dec_ind)]}
  
    # set maximum iterations for convergence
    if(!is.null(max_iter) & !is.list(max_iter)){
      if(max_iter < 1) stop("max_iter must be positive")
      maxiter <- max_iter
    }
    else{ 
      maxiter <- 200 }
      
    # set while loop initial values
    iter <- 0
    delta <- 10
    # iterate between pava and linear model
    # set while loop condition(s). 
    # delta works well enough with delta > 1e-12.
    while(delta > 1e-6 & iter < maxiter){ 
  
      yhat <- cpav(x_mat = as.matrix(x[wates != 0,]), y = 
                     (y[wates != 0] - (as.matrix(x[wates != 0,
                              -c(inc_ind, dec_ind)]) %*% betas)), 
                   weights = wates[wates != 0], inc_index = 
                     inc_ind, dec_index = dec_ind)
      
      # save old betas for distance calculation
      old_betas <- betas    
      # to retrieve old ordering of y for fitted values, we use 
      # y[match(x, sorted_x)]
      betas <- coef(lm.wfit(x=as.matrix(x[,-c(inc_ind, dec_ind)]), 
                        y= (y - get_pred(yhat, 
                           x[,c(inc_ind, dec_ind)]) ), w=wates))

      # quantify change in yhat vals and beta vals
      # get euclidian distance between betas 
      # transformed into unit vectors
      delta <- dist(rbind( as.vector(betas)/norm(as.vector(betas), 
                                            type ="2"), 
                  as.vector(old_betas)/norm(as.vector(old_betas), 
                                            type="2")
                           ))
  
      iter <- iter + 1    # iterate maxiter
    }
  }
  
  # get residuals of model
  resids <- y - (get_pred(yhat, x[,c(inc_ind, dec_ind)]) + 
                (as.matrix(x[,-c(inc_ind, dec_ind)]) %*% betas))

  # mod must have: coef attribute, sigma attribute, cov attribute, 
  # df attribute, ..., and may have mon_inc_index and 
  # mon_dec_index attributes
  mod <- list(coef = NULL, fitted_pava = NULL, sigma = NULL, 
              df = NULL, mon_inc_index = NULL, mon_dec_index = 
                NULL, iterations = NULL, mon_inc_names = NULL, 
              mon_dec_names = NULL)
  
  mod$coef <- betas
  mod$fitted_pava <- yhat
  mod$iterations <- iter
  mod$mon_inc_index <- inc_ind
  mod$mon_dec_index <- dec_ind
  mod$sigma <- sqrt(sum(wates * (resids)^2 / 
                          mean(wates))/ (nrow(x)-qr(x)$rank))
  mod$df <- ncol(x)+1

  class(mod) <- "part_fit"
  
  return(mod)
}


# write plot method for objects returned from part_fit()
append_suffix <- function(num){
  suff <- case_when(num %in% c(11,12,13) ~ "th",
                    num %% 10 == 1 ~ 'st',
                    num %% 10 == 2 ~ 'nd',
                    num %% 10 == 3 ~'rd',
                    TRUE ~ "th")
  paste0(num, suff)
}

plot.part_fit <- function(z){
  if(dim(as.matrix(z$fitted_pava))[2] > 1){
    temp <- list()
    for(i in 1:dim(as.matrix(z$fitted_pava))[2]){
      temp[[i]] <- ggplotGrob(ggplot() +
        geom_line(aes(x = z$fitted_pava[,i]$x, y = 
                        z$fitted_pava[,i]$yf)) + 
        theme_bw() +
        labs(title = paste(append_suffix(i), 
                           " Monotone Regression"),
             x = "X",
             y = "Y"))
    }
    return(grid.arrange(grobs=temp, ncol=1))
  }
  else{
    temp <- ggplot() +
      geom_line(aes(x = z$fitted_pava[,1]$x, 
                    y = z$fitted_pava[,1]$yf)) + 
      theme_bw() +
      labs(title = "Monotone Regression",
           x = "X",
           y = "Y")
    return(temp)
  }
}

@


\subsection{Code for the M-step Driver}

<<M_driver, eval=FALSE>>=

# allow slots defined for numeric to accept NULL
setClassUnion("numericOrNULL",members=c("numeric", "NULL"))
setClassUnion("characterOrNULL", 
              members = c("character", "NULL"))
setOldClass("monoreg")
setClassUnion("matrixOrMonoreg", 
              members = c("matrix", "monoreg"))

# Define new classes
setClass(
  "FLX_monoreg_component",
  contains="FLXcomponent",
  # allow mon_index to take either numeric or NULL
  slots=c(mon_inc_index="numericOrNULL", 
          mon_dec_index="numericOrNULL",
          mon_obj="matrix",
          mon_inc_names="characterOrNULL",
          mon_dec_names="characterOrNULL"
          )) 

# Define FLXM_monoreg 
setClass("FLXM_monoreg",
         contains = "FLXM",
         slots = c(mon_inc_index="numericOrNULL", 
                   mon_dec_index="numericOrNULL",
                   mon_inc_names="characterOrNULL",
                   mon_dec_names="characterOrNULL"))

# definition of monotone regression model.
mono_reg <- function (formula = .~., mon_inc_names = NULL, 
                      mon_dec_names = NULL, mon_inc_index=NULL, 
                      mon_dec_index=NULL, ...) {

  # only names or indices can be indicated, not both
  if((!is.null(mon_inc_names)|!is.null(mon_dec_names)) &
     (!is.null(mon_inc_index)|!is.null(mon_dec_index))) stop(
     "mono_reg() can accept either monotone 
     names or indices can be chosen, but not both.")
  
  retval <- new("FLXM_monoreg", weighted = TRUE,
                formula = formula,
                name = "partially linear monotonic regression",
                mon_inc_index= sort(mon_inc_index),
                mon_dec_index= sort(mon_dec_index), 
                mon_inc_names= mon_inc_names,
                mon_dec_names= mon_dec_names) 
  
  # @defineComponent: Expression or function constructing the 
  # object of class FLXcomponent fit must have attributes: 
  # coef, sigma, cov, df, ..., and 
  # may have mon_inc_index and mon_dec_index attributes
  # ... all must be defined by fit() function
  retval@defineComponent <- function(fit, ...) {
    # @logLik: A function(x,y) returning the 
    # log-likelihood for observations in matrices x and y
                  logLik <- function(x, y) { 
                    dnorm(y, mean=predict(x, ...), sd=fit$sigma, 
                          log=TRUE)}
                  # @predict: A function(x) predicting y given x. 
                  predict <- function(x) {
                    x <- as.matrix(x)
                    inc_ind <- fit$mon_inc_index
                    dec_ind <- fit$mon_dec_index
                    
                    p <-  get_pred(fit$fitted_pava, x[,c(inc_ind, 
                                                    dec_ind)])
                    if(!is.null(fit$coef)){
                      p <- p + (as.matrix(x[,-c(inc_ind, 
                                        dec_ind)]) %*% fit$coef)
                    }
                    p
                  }
                  # return new FLX_monoreg_component object
                  new("FLX_monoreg_component", parameters =
                        list(coef = fit$coef, sigma = fit$sigma, 
                             mon_obj = fit$fitted_pava),
                      df = fit$df, logLik = logLik, predict = 
                        predict, mon_inc_index = 
                        fit$mon_inc_index, mon_dec_index = 
                        fit$mon_dec_index,
                      # mon_obj = fit$fitted_pava,
                      mon_inc_names = fit$mon_inc_names,
                      mon_dec_names = fit$mon_dec_names)
  }
  
  # @fit: A function(x,y,w) returning an object of 
  # class "FLXcomponent"
  retval@fit <- function(x, y, w, component, mon_inc_index = 
                           retval@mon_inc_index, 
                         mon_dec_index = retval@mon_dec_index, 
                         mon_inc_names = retval@mon_inc_names,
                         mon_dec_names = retval@mon_dec_names, 
                         ...) {
    
                  if(is.null(mon_inc_index) & 
                     is.null(mon_dec_index)){
                    
                # if not all monotone names are in the design 
                # matrix, stop & print the name that is missing
                    if(!all(c(mon_inc_names, mon_dec_names) %in%
                            colnames(x))){
                      stop(paste(setdiff(c(mon_inc_names, 
                                    mon_dec_names), colnames(x)),
                                 "could not be found in the model 
                                 matrix. Check your spelling."))
                    } 
                    # Discover correct monotone indices
                    if(any(colnames(x) %in% mon_inc_names)){
                      mon_inc_index <- which(colnames(x) %in% 
                                               mon_inc_names)
                    }
                    if(any(colnames(x) %in% mon_dec_names)){
                      mon_dec_index <- which(colnames(x) 
                                            %in% mon_dec_names)
                    }
                  }
                  if(is.null(mon_inc_names) & 
                     is.null(mon_dec_names)){
                    # Discover correct monotone names
                    mon_inc_names <- colnames(x)[
                      sort(mon_inc_index)]
                    mon_dec_names <- colnames(x)[
                      sort(mon_dec_index)]
                  }

                  fit <- part_fit(x, y, w, component, 
                              mon_inc_index=mon_inc_index, 
                              mon_dec_index=mon_dec_index, ...)
                  
                  retval@defineComponent(fit, ...)
                  }
  retval 
  }

@

\subsection{Code for Wrapper Functions}

<<flexmix_wrappers, eval=FALSE>>=

# Wrapper functions for Flexmix objects with monoreg components

# import libraries
library(ggplot2)
library(grid)
library(gridExtra)
library(RColorBrewer)


# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}


####

# overwrite method for plot.flexmix
setMethod('plot',  signature(x="flexmix", y="missing"),
          function(x, mark=NULL, markcol=NULL, col=NULL, 
                   eps=1e-4, root=TRUE, ylim=NULL, xlim=NULL, main=NULL, xlab=NULL, ylab=NULL,
                   as.table = TRUE, endpoints = c(-0.04, 1.04), rootogram=F, palet = NULL, 
                   root_scale = "unscaled", subplot=NULL, ...) {
            
            if(is.null(palet)){
              palet <- "Accent"
            }
            
           
  if(is(x@components[[1]][[1]], "FLX_monoreg_component")){ # check that this is a mixture of part_fits
    # assign appropriate names for graph labelling
    if(is.null( c(x@model[[1]]@mon_inc_names, x@model[[1]]@mon_dec_names) )){
      xnames <- sapply(1:dim(x@components[[1]][[1]]@mon_obj)[2], function(x) paste0("X", x))
      mono_names <- c("Y", xnames)
    }
    else{
      mono_names <- c(x@formula[[2]], c(x@model[[1]]@mon_inc_names, x@model[[1]]@mon_dec_names))
    }
    
            # get dimension of monotone components by reading columns of fitted_pava object
            if(dim( x@components[[1]][[1]]@mon_obj )[2] > 1){ 
              np <- list()
              for(i in 1:dim( x@components[[1]][[1]]@mon_obj )[2]){
                holder <- ggplot()
                
                if(length(x@components) == 1){
                    holder <- holder + 
                      geom_line(aes(x = x@components[[1]][[1]]@mon_obj[,i]$x,
                                y = x@components[[1]][[1]]@mon_obj[,i]$yf)) +
                      theme_bw() +
                      labs(title = paste(append_suffix(i), " Monotone Regression"),
                       x = mono_names[i+1],
                       y = mono_names[1])
                    }
                
                if(length(x@components) > 1){
                  
                  monlist <- list()
                  for(b in 1:length(x@components)){
                    monlist[[b]] <- data.frame(x = x@components[[b]][[1]]@mon_obj[,i]$x, 
                                               yf = x@components[[b]][[1]]@mon_obj[,i]$yf)
                  }
                  
                  mondf   <- cbind(Cluster=rep(1:length(x@components), 
                                               sapply(monlist,nrow)),do.call(rbind,monlist))
                  mondf$Cluster <- as.factor(mondf$Cluster)
                  
                  holder <- holder + geom_line(mondf, mapping = aes(x,yf, color=Cluster)) + 
                    scale_color_brewer(palette=palet) +
                    theme_bw() +
                    labs(title = paste(append_suffix(i), " Monotone Regression"),
                         x = mono_names[i+1],
                         y = mono_names[1])
                }
                
                if(!is.null(ylim)){
                  if(length(ylim) != dim( x@components[[1]][[1]]@mon_obj )[2] |
                     length(ylim[[1]]) != 2 ){
                    stop("If you pass a ylim argument, it must have as many element pairs 
                         as the model has monotone components. Try formulating the argument
                         as: ylim = list(c(i,j), c(i,j), ...)")}
                  holder <- holder + ylim(ylim[[i]])
                }
                if(!is.null(xlim)){
                  if(length(xlim) != dim( x@components[[1]][[1]]@mon_obj )[2] |
                     length(xlim[[1]]) != 2 ){
                    stop("If you pass a xlim argument, it must have as many element pairs 
                         as the model has monotone components. Try formulating the argument
                         as: xlim = list(c(i,j), c(i,j), ...)")}
                  holder <- holder + xlim(xlim[[i]])
                }
                if(!is.null(ylab)){
                  if(length(ylab) != dim( x@components[[1]][[1]]@mon_obj )[2]){
                    stop("If you pass a ylab argument, it must have as many elements 
                         as the model has monotone components. Try formulating the argument
                         as: ylab = c(\"first\",\"second\",...)")}
                  holder <- holder + ylab(ylab[[i]])
                }
                if(!is.null(xlab)){
                  if(length(xlab) != dim( x@components[[1]][[1]]@mon_obj )[2]){
                    stop("If you pass a xlab argument, it must have as many elements 
                         as the model has monotone components. Try formulating the argument
                         as: xlab = c(\"first\",\"second\",...)")}
                  holder <- holder + xlab(xlab[[i]])
                }
                if(!is.null(main)){
                  if(length(main) != dim( x@components[[1]][[1]]@mon_obj )[2]){
                    stop("If you pass a main argument, it must have as many elements 
                         as the model has monotone components. Try formulating the argument
                         as: main = c(\"first\",\"second\",...)")}
                  holder <- holder + ggtitle(main[[i]])
                }
                
                
                
                np[[i]] <- holder
              }
              # return(grid.arrange(grobs=np, ncol=1))
              # return(grid.arrange(grobs=np, ncol=1))
            }
            else{
              np <- ggplot()
              
              if(length(x@components) == 1){
                np <- np + geom_line(aes(x = x@components[[1]][[1]]@mon_obj[,1]$x, y = 
                                           x@components[[1]][[1]]@mon_obj[,1]$yf)) + 
                  theme_bw() +
                  labs(title = "Monotone Component",
                     x = mono_names[2],
                     y = mono_names[1]) 
                }
              
              if(length(x@components) > 1){
                
                monlist <- list()
                for(b in 1:length(x@components)){
                  monlist[[b]] <- data.frame(x = x@components[[b]][[1]]@mon_obj[,1]$x, 
                                                   yf = x@components[[b]][[1]]@mon_obj[,1]$yf)
                }
                
                mondf   <- cbind(Cluster=rep(1:length(x@components), 
                                             sapply(monlist,nrow)), do.call(rbind, monlist))
                mondf$Cluster <- as.factor(mondf$Cluster)
                
                np <- np + geom_line(mondf, mapping = aes(x,yf, color=Cluster)) + 
                  scale_color_brewer(palette=palet) +
                  theme_bw() +
                  labs(title = "Monotone Component",
                       x = mono_names[2],
                       y = mono_names[1])
              }
              
              
              if(!is.null(ylim)){
                np <- np + ylim(ylim)
              }
              if(!is.null(xlim)){
                np <- np + xlim(xlim)
              }
              if(!is.null(ylab)){
                np <- np + ylab(ylab)
              }
              if(!is.null(xlab)){
                np <- np + xlab(xlab)
              }
              if(!is.null(main)){
                np <- np + ggtitle(main)
              }
              
              
              
              
              # return(np)
            }
  }
            # plot and append rootogram
            post <- data.frame(x@posterior$scaled) # collect posteriors
            names(post) <- 1:dim(post)[2] # change columns of posteriors to cluster numbers
            post <- melt(setDT(post), measure.vars = c(1:dim(post)[2]), variable.name = "Cluster")
            rg <- ggplot(post, aes(x=value, fill=Cluster)) + # plot rootogram, with color indicating cluster
              geom_histogram(binwidth = 0.05) + 
              scale_fill_brewer(palette=palet) +
              theme_bw() +
              labs(title = "Rootogram",
                   x = "Posteriors",
                   y = "Count")
            
            if(root_scale == "sqrt"){rg <- rg + 
              scale_y_sqrt() +
              labs(title = "Rootogram (square root scale)",
                   x = "Posteriors",
                   y = "Count (square root)")}
            if(root_scale == "log"){rg <- rg + 
              scale_y_log10() + 
              labs(title = "Rootogram (log scale)",
                   x = "Posteriors",
                   y = "Count (log)")}
            
            if(!is.null(subplot)){
              return(list(rg, np)[[subplot[1]]])
            }
            else{
              multiplot(rg, np)
            }
          }          
)

            
            


          

@

\end{appendices}



\end{document}